[{"title": "Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach", "abstract": "In Federated Learning, we aim to train models across multiple computing units (users), while users can only communicate with a common central server, without exchanging their data samples. This mechanism exploits the computational power of all users and allows users to obtain a richer model as their models are trained over a larger set of data points. However, this scheme only develops a common output for all the users, and, therefore, it does not adapt the model to each user. This is an important missing feature, especially given the heterogeneity of the underlying data distribution for various users. In this paper, we study a personalized variant of the federated learning in which our goal is to find an initial shared model that current or new users can easily adapt to their local dataset by performing one or a few steps of gradient descent with respect to their own data. This approach keeps all the benefits of the federated learning architecture, and, by structure, leads to a more personalized model for each user. We show this problem can be studied within the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection, we study a personalized variant of the well-known Federated Averaging algorithm and evaluate its performance in terms of gradient norm for non-convex loss functions. Further, we characterize how this performance is affected by the closeness of underlying distributions of user data, measured in terms of distribution distances such as Total Variation and 1-Wasserstein metric.", "year": 2020, "venue": "", "authors": "Alireza Fallah and Aryan Mokhtari and Asuman Ozdaglar"}, {"title": "The computational limits of deep learning", "abstract": "Deep learning\u2019s recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image classification, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article catalogs the extent of this dependency, showing that progress across a wide variety of applications is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationally-efficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods.", "year": 2020, "venue": "", "authors": "Neil C Thompson and Kristjan Greenewald and Keeheon Lee and Gabriel F Manso"}, {"title": "Personalized federated learning: A meta-learning approach", "abstract": "In Federated Learning, we aim to train models across multiple computing units (users), while users can only communicate with a common central server, without exchanging their data samples. This mechanism exploits the computational power of all users and allows users to obtain a richer model as their models are trained over a larger set of data points. However, this scheme only develops a common output for all the users, and, therefore, it does not adapt the model to each user. This is an important missing feature, especially given the heterogeneity of the underlying data distribution for various users. In this paper, we study a personalized variant of the federated learning in which our goal is to find an initial shared model that current or new users can easily adapt to their local dataset by performing one or a few steps of gradient descent with respect to their own data. This approach keeps all the benefits of the federated learning architecture, and, by structure, leads to a more personalized model for each user. We show this problem can be studied within the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection, we study a personalized variant of the well-known Federated Averaging algorithm and evaluate its performance in terms of gradient norm for non-convex loss functions. Further, we characterize how this performance is affected by the closeness of underlying distributions of user data, measured in terms of distribution distances such as Total Variation and 1-Wasserstein metric.", "year": 2020, "venue": "", "authors": "Alireza Fallah and Aryan Mokhtari and Asuman Ozdaglar"}, {"title": "Is conditional generative modeling all you need for decision-making?", "abstract": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.", "year": 2022, "venue": "", "authors": "Anurag Ajay and Yilun Du and Abhi Gupta and Joshua Tenenbaum and Tommi Jaakkola and Pulkit Agrawal"}, {"title": "Relative uncertainty learning for facial expression recognition", "abstract": "In facial expression recognition (FER), the uncertainties introduced by inherent noises like ambiguous facial expressions and inconsistent labels raise concerns about the credibility of recognition results. To quantify these uncertainties and achieve good performance under noisy data, we regard uncertainty as a relative concept and propose an innovative uncertainty learning method called Relative Uncertainty Learning (RUL). Rather than assuming Gaussian uncertainty distributions for all datasets, RUL builds an extra branch to learn uncertainty from the relative difficulty of samples by feature mixup. Specifically, we use uncertainties as weights to mix facial features and design an add-up loss to encourage uncertainty learning. It is easy to implement and adds little or no extra computation overhead. Extensive experiments show that RUL outperforms state-of-the-art FER uncertainty learning methods in both real-world and synthetic noisy FER datasets. Besides, RUL also works well on other datasets such as CIFAR and Tiny ImageNet. The code is available at https://github. com/zyh-uaiaaaa/Relative-Uncertainty-Learning.", "year": 2021, "venue": "", "authors": "Yuhang Zhang and Chengrui Wang and Weihong Deng"}, {"title": "On the convergence theory of gradient-based model-agnostic meta-learning algorithms", "abstract": "We study the convergence of a class of gradient-based Model-Agnostic Meta-Learning (MAML) methods and characterize their overall complexity as well as their best achievable accuracy in terms of gradient norm for nonconvex loss functions. We start with the MAML method and its first-order approximation (FO-MAML) and highlight the challenges that emerge in their analysis. By overcoming these challenges not only we provide the first theoretical guarantees for MAML and FO-MAML in nonconvex settings, but also we answer some of the unanswered questions for the implementation of these algorithms including how to choose their learning rate and the batch size for both tasks and datasets corresponding to tasks. In particular, we show that MAML can find an?-first-order stationary point (?-FOSP) for any positive? after at most O (1/?^ 2) iterations at the expense of requiring second-order information. We also show that FO-MAML which ignores the second-order information required in the update of MAML cannot achieve any small desired level of accuracy, ie, FO-MAML cannot find an?-FOSP for any?> 0. We further propose a new variant of the MAML algorithm called Hessian-free MAML which preserves all theoretical guarantees of MAML, without requiring access to second-order information.", "year": 2020, "venue": "", "authors": "Alireza Fallah and Aryan Mokhtari and Asuman Ozdaglar"}, {"title": "Bao: Making learned query optimization practical", "abstract": "Recent efforts applying machine learning techniques to query optimization have shown few practical gains due to substantive training overhead, inability to adapt to changes, and poor tail performance. Motivated by these difficulties, we introduce Bao (the \\underlineBa ndit \\underlineo ptimizer). Bao takes advantage of the wisdom built into existing query optimizers by providing per-query optimization hints. Bao combines modern tree convolutional neural networks with Thompson sampling, a well-studied reinforcement learning algorithm. As a result, Bao automatically learns from its mistakes and adapts to changes in query workloads, data, and schema. Experimentally, we demonstrate that Bao can quickly learn strategies that improve end-to-end query execution performance, including tail latency, for several workloads containing long-running queries. In cloud environments, we show that Bao can offer both \u2026", "year": 2021, "venue": "", "authors": "Ryan Marcus and Parimarjan Negi and Hongzi Mao and Nesime Tatbul and Mohammad Alizadeh and Tim Kraska"}, {"title": "Neural circuit policies enabling auditable autonomy", "abstract": "A central goal of artificial intelligence in high-stakes decision-making applications is to design a single algorithm that simultaneously expresses generalizability by learning coherent representations of their world and interpretable explanations of its dynamics. Here, we combine brain-inspired neural computation principles and scalable deep learning architectures to design compact neural controllers for task-specific compartments of a full-stack autonomous vehicle control system. We discover that a single algorithm with 19 control neurons, connecting 32 encapsulated input features to outputs by 253 synapses, learns to map high-dimensional inputs into steering commands. This system shows superior generalizability, interpretability and robustness compared with orders-of-magnitude larger black-box learning systems. The obtained neural agents enable high-fidelity autonomy for task-specific parts of a complex \u2026", "year": 2020, "venue": "", "authors": "Mathias Lechner and Ramin Hasani and Alexander Amini and Thomas A Henzinger and Daniela Rus and Radu Grosu"}, {"title": "Rapid locomotion via reinforcement learning", "abstract": "Agile maneuvers such as sprinting and high-speed turning in the wild are challenging for legged robots. We present an end-to-end learned controller that achieves record agility for the MIT Mini Cheetah, sustaining speeds up to 3.9 m/s. This system runs and turns fast on natural terrains like grass, ice, and gravel and responds robustly to disturbances. Our controller is a neural network trained in simulation via reinforcement learning and transferred to the real world. The two key components are (i) an adaptive curriculum on velocity commands and (ii) an online system identification strategy for sim-to-real transfer. Videos of the robot\u2019s behaviors are available at https://agility.csail.mit.edu/.", "year": 2024, "venue": "", "authors": "Gabriel B Margolis and Ge Yang and Kartik Paigwar and Tao Chen and Pulkit Agrawal"}, {"title": "Integration of neural network-based symbolic regression in deep learning for scientific discovery", "abstract": "Symbolic regression is a powerful technique to discover analytic equations that describe data, which can lead to explainable models and the ability to predict unseen data. In contrast, neural networks have achieved amazing levels of accuracy on image recognition and natural language processing tasks, but they are often seen as black-box models that are difficult to interpret and typically extrapolate poorly. In this article, we use a neural network-based architecture for symbolic regression called the equation learner (EQL) network and integrate it with other deep learning architectures such that the whole system can be trained end-to-end through backpropagation. To demonstrate the power of such systems, we study their performance on several substantially different tasks. First, we show that the neural network can perform symbolic regression and learn the form of several functions. Next, we present an MNIST \u2026", "year": 2020, "venue": "", "authors": "Samuel Kim and Peter Y Lu and Srijon Mukherjee and Michael Gilbert and Li Jing and Vladimir \u010ceperi\u0107 and Marin Solja\u010di\u0107"}, {"title": "Hydra: A real-time spatial perception system for 3D scene graph construction and optimization", "abstract": "3D scene graphs have recently emerged as a powerful high-level representation of 3D environments. A 3D scene graph describes the environment as a layered graph where nodes represent spatial concepts at multiple levels of abstraction and edges represent relations between concepts. While 3D scene graphs can serve as an advanced \"mental model\" for robots, how to build such a rich representation in real-time is still uncharted territory. This paper describes a real-time Spatial Perception System, a suite of algorithms to build a 3D scene graph from sensor data in real-time. Our first contribution is to develop real-time algorithms to incrementally construct the layers of a scene graph as the robot explores the environment; these algorithms build a local Euclidean Signed Distance Function (ESDF) around the current robot location, extract a topological map of places from the ESDF, and then segment the places into rooms using an approach inspired by community-detection techniques. Our second contribution is to investigate loop closure detection and optimization in 3D scene graphs. We show that 3D scene graphs allow defining hierarchical descriptors for loop closure detection; our descriptors capture statistics across layers in the scene graph, ranging from low-level visual appearance to summary statistics about objects and places. We then propose the first algorithm to optimize a 3D scene graph in response to loop closures; our approach relies on embedded deformation graphs to simultaneously correct all layers of the scene graph. We implement the proposed Spatial Perception System into a architecture named Hydra, that combines fast \u2026", "year": 2022, "venue": "", "authors": "Nathan Hughes and Yun Chang and Luca Carlone"}, {"title": "Breeds: Benchmarks for subpopulation shift", "abstract": "We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines for them. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of off-the-shelf train-time robustness interventions. Code and data available at https://github.com/MadryLab/BREEDS-Benchmarks .", "year": 2020, "venue": "", "authors": "Shibani Santurkar and Dimitris Tsipras and Aleksander Madry"}, {"title": "From imagenet to image classification: Contextualizing progress on benchmarks", "abstract": "Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset\u2014including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignment into account.", "year": 2020, "venue": "", "authors": "Dimitris Tsipras and Shibani Santurkar and Logan Engstrom and Andrew Ilyas and Aleksander Madry"}, {"title": "Tsunami: A learned multi-dimensional index for correlated data and skewed workloads", "abstract": "Filtering data based on predicates is one of the most fundamental operations for any modern data warehouse. Techniques to accelerate the execution of filter expressions include clustered indexes, specialized sort orders (e.g., Z-order), multi-dimensional indexes, and, for high selectivity queries, secondary indexes. However, these schemes are hard to tune and their performance is inconsistent. Recent work on learned multi-dimensional indexes has introduced the idea of automatically optimizing an index for a particular dataset and workload. However, the performance of that work suffers in the presence of correlated data and skewed query workloads, both of which are common in real applications. In this paper, we introduce Tsunami, which addresses these limitations to achieve up to 6X faster query performance and up to 8X smaller index size than existing learned multi-dimensional indexes, in addition to up to 11X faster query performance and 170X smaller index size than optimally-tuned traditional indexes.", "year": 2020, "venue": "", "authors": "Jialin Ding and Vikram Nathan and Mohammad Alizadeh and Tim Kraska"}, {"title": "Walk these ways: Tuning robot control for generalization with multiplicity of behavior", "abstract": "Learned locomotion policies can rapidly adapt to diverse environments similar to those experienced during training but lack a mechanism for fast tuning when they fail in an out-of-distribution test environment. This necessitates a slow and iterative cycle of reward and environment redesign to achieve good performance on a new task. As an alternative, we propose learning a single policy that encodes a structured family of locomotion strategies that solve training tasks in different ways, resulting in Multiplicity of Behavior (MoB). Different strategies generalize differently and can be chosen in real-time for new tasks or environments, bypassing the need for time-consuming retraining. We release a fast, robust open-source MoB locomotion controller, Walk These Ways, that can execute diverse gaits with variable footswing, posture, and speed, unlocking diverse downstream tasks: crouching, hopping, high-speed running, stair traversal, bracing against shoves, rhythmic dance, and more. Video and code release: https://gmargo11. github. io/walk-these-ways", "year": 2023, "venue": "", "authors": "Gabriel B Margolis and Pulkit Agrawal"}, {"title": "From words to watts: Benchmarking the energy costs of large language model inference", "abstract": "Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs-despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies. In this paper, we describe experiments conducted to study the computational and energy utilization of \u2026", "year": 2023, "venue": "", "authors": "Siddharth Samsi and Dan Zhao and Joseph McDonald and Baolin Li and Adam Michaleas and Michael Jones and William Bergeron and Jeremy Kepner and Devesh Tiwari and Vijay Gadepally"}, {"title": "Trak: Attributing model behavior at scale", "abstract": "The goal of data attribution is to trace model predictions back to training data. Despite a long line of work towards this goal, existing approaches to data attribution tend to force users to choose between computational tractability and efficacy. That is, computationally tractable methods can struggle with accurately attributing model predictions in non-convex settings (e.g., in the context of deep neural networks), while methods that are effective in such regimes require training thousands of models, which makes them impractical for large models or datasets. In this work, we introduce TRAK (Tracing with the Randomly-projected After Kernel), a data attribution method that is both effective and computationally tractable for large-scale, differentiable models. In particular, by leveraging only a handful of trained models, TRAK can match the performance of attribution methods that require training thousands of models. We demonstrate the utility of TRAK across various modalities and scales: image classifiers trained on ImageNet, vision-language models (CLIP), and language models (BERT and mT5). We provide code for using TRAK (and reproducing our work) at https://github.com/MadryLab/trak .", "year": 2023, "venue": "", "authors": "Sung Min Park and Kristian Georgiev and Andrew Ilyas and Guillaume Leclerc and Aleksander Madry"}, {"title": "Generative models as a data source for multiview representation learning", "abstract": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \"views\" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival or even outperform those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \"model zoos\" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is available on our project page https://ali-design.github.io/GenRep/.", "year": 2021, "venue": "", "authors": "Ali Jahanian and Xavier Puig and Yonglong Tian and Phillip Isola"}, {"title": "Beyond expertise and roles: A framework to characterize the stakeholders of interpretable machine learning and their needs", "abstract": " To ensure accountability and mitigate harm, it is critical that diverse stakeholders can interrogate black-box automated systems and find information that is understandable, relevant, and useful to them. In this paper, we eschew prior expertise- and role-based categorizations of interpretability stakeholders in favor of a more granular framework that decouples stakeholders\u2019 knowledge from their interpretability needs. We characterize stakeholders by their formal, instrumental, and personal knowledge and how it manifests in the contexts of machine learning, the data domain, and the general milieu. We additionally distill a hierarchical typology of stakeholder needs that distinguishes higher-level domain goals from lower-level interpretability tasks. In assessing the descriptive, evaluative, and generative powers of our framework, we find our more nuanced treatment of stakeholders reveals gaps and opportunities in the \u2026", "year": 2021, "venue": "", "authors": "Harini Suresh and Steven R Gomez and Kevin K Nam and Arvind Satyanarayan"}, {"title": "Sevir: A storm event imagery dataset for deep learning applications in radar and satellite meteorology", "abstract": "Modern deep learning approaches have shown promising results in meteorological applications like precipitation nowcasting, synthetic radar generation, front detection and several others. In order to effectively train and validate these complex algorithms, large and diverse datasets containing high-resolution imagery are required. Petabytes of weather data, such as from the Geostationary Environmental Satellite System (GOES) and the Next-Generation Radar (NEXRAD) system, are available to the public; however, the size and complexity of these datasets is a hindrance to developing and training deep models. To help address this problem, we introduce the Storm EVent ImagRy (SEVIR) dataset-a single, rich dataset that combines spatially and temporally aligned data from multiple sensors, along with baseline implementations of deep learning models and evaluation metrics, to accelerate new algorithmic innovations. SEVIR is an annotated, curated and spatio-temporally aligned dataset containing over 10,000 weather events that each consist of 384 km x 384 km image sequences spanning 4 hours of time. Images in SEVIR were sampled and aligned across five different data types: three channels (C02, C09, C13) from the GOES-16 advanced baseline imager, NEXRAD vertically integrated liquid mosaics, and GOES-16 Geostationary Lightning Mapper (GLM) flashes. Many events in SEVIR were selected and matched to the NOAA Storm Events database so that additional descriptive information such as storm impacts and storm descriptions can be linked to the rich imagery provided by the sensors. We describe the data collection methodology \u2026", "year": 2020, "venue": "", "authors": "Mark Veillette and Siddharth Samsi and Chris Mattioli"}, {"title": "Equivariant contrastive learning", "abstract": "In state-of-the-art self-supervised learning (SSL) pre-training produces semantically good representations by encouraging them to be invariant under meaningful transformations prescribed from human knowledge. In fact, the property of invariance is a trivial instance of a broader class called equivariance, which can be intuitively understood as the property that representations transform according to the way the inputs transform. Here, we show that rather than using only invariance, pre-training that encourages non-trivial equivariance to some transformations, while maintaining invariance to other transformations, can be used to improve the semantic quality of representations. Specifically, we extend popular SSL methods to a more general framework which we name Equivariant Self-Supervised Learning (E-SSL). In E-SSL, a simple additional pre-training objective encourages equivariance by predicting the transformations applied to the input. We demonstrate E-SSL's effectiveness empirically on several popular computer vision benchmarks, e.g. improving SimCLR to 72.5% linear probe accuracy on ImageNet. Furthermore, we demonstrate usefulness of E-SSL for applications beyond computer vision; in particular, we show its utility on regression problems in photonics science. Our code, datasets and pre-trained models are available at https://github.com/rdangovs/essl to aid further research in E-SSL.", "year": 2021, "venue": "", "authors": "Rumen Dangovski and Li Jing and Charlotte Loh and Seungwook Han and Akash Srivastava and Brian Cheung and Pulkit Agrawal and Marin Solja\u010di\u0107"}, {"title": "The low-rank simplicity bias in deep networks", "abstract": "Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear models can be used to induce low-rank bias, improving generalization performance on CIFAR and ImageNet without changing the modeling capacity.", "year": 2021, "venue": "", "authors": "Minyoung Huh and Hossein Mobahi and Richard Zhang and Brian Cheung and Pulkit Agrawal and Phillip Isola"}, {"title": "Do GANs always have Nash equilibria?", "abstract": "Generative adversarial networks (GANs) represent a zero-sum game between two machine players, a generator and a discriminator, designed to learn the distribution of data. While GANs have achieved state-of-the-art performance in several benchmark learning tasks, GAN minimax optimization still poses great theoretical and empirical challenges. GANs trained using first-order optimization methods commonly fail to converge to a stable solution where the players cannot improve their objective, ie, the Nash equilibrium of the underlying game. Such issues raise the question of the existence of Nash equilibria in GAN zero-sum games. In this work, we show through theoretical and numerical results that indeed GAN zero-sum games may have no Nash equilibria. To characterize an equilibrium notion applicable to GANs, we consider the equilibrium of a new zero-sum game with an objective function given by a proximal operator applied to the original objective, a solution we call the proximal equilibrium. Unlike the Nash equilibrium, the proximal equilibrium captures the sequential nature of GANs, in which the generator moves first followed by the discriminator. We prove that the optimal generative model in Wasserstein GAN problems provides a proximal equilibrium. Inspired by these results, we propose a new approach, which we call proximal training, for solving GAN problems. We perform several numerical experiments indicating the existence of proximal equilibria in GANs.", "year": 2020, "venue": "", "authors": "Farzan Farnia and Asuman Ozdaglar"}, {"title": "Use of neural networks for stable, accurate and physically consistent parameterization of subgrid atmospheric processes with good performance at reduced precision", "abstract": "A promising approach to improve climate\u2010model simulations is to replace traditional subgrid parameterizations based on simplified physical models by machine learning algorithms that are data\u2010driven. However, neural networks (NNs) often lead to instabilities and climate drift when coupled to an atmospheric model. Here, we learn an NN parameterization from a high\u2010resolution atmospheric simulation in an idealized domain by accurately calculating subgrid terms through coarse graining. The NN parameterization has a structure that ensures physical constraints are respected, such as by predicting subgrid fluxes instead of tendencies. The NN parameterization leads to stable simulations that replicate the climate of the high\u2010resolution simulation with similar accuracy to a successful random\u2010forest parameterization while needing far less memory. We find that the simulations are stable for different horizontal \u2026", "year": 2021, "venue": "", "authors": "Janni Yuval and Paul A O'Gorman and Chris N Hill"}, {"title": "Modelingtoolkit: A composable graph transformation system for equation-based modeling", "abstract": "Getting good performance out of numerical equation solvers requires that the user has provided stable and efficient functions representing their model. However, users should not be trusted to write good code. In this manuscript we describe ModelingToolkit (MTK), a symbolic equation-based modeling system which allows for composable transformations to generate stable, efficient, and parallelized model implementations. MTK blurs the lines of traditional symbolic computing by acting directly on a user's numerical code. We show the ability to apply graph algorithms for automatically parallelizing and performing index reduction on code written for differential-algebraic equation (DAE) solvers, \"fixing\" the performance and stability of the model without requiring any changes to on the user's part. We demonstrate how composable model transformations can be combined with automated data-driven surrogate generation techniques, allowing machine learning methods to generate accelerated approximate models within an acausal modeling framework. These reduced models are shown to outperform the Dymola Modelica compiler on an HVAC model by 590x at 3\\% error. Together, this demonstrates MTK as a system for bringing the latest research in graph transformations directly to modeling applications.", "year": 2021, "venue": "", "authors": "Yingbo Ma and Shashi Gowda and Ranjan Anantharaman and Chris Laughman and Viral Shah and Chris Rackauckas"}, {"title": "Neural scaling of deep chemical models", "abstract": "Massive scale, in terms of both data availability and computation, enables important breakthroughs in key application areas of deep learning such as natural language processing and computer vision. There is emerging evidence that scale may be a key ingredient in scientific deep learning, but the importance of physical priors in scientific domains makes the strategies and benefits of scaling uncertain. Here we investigate neural-scaling behaviour in large chemical models by varying model and dataset sizes over many orders of magnitude, studying models with over one billion parameters, pre-trained on datasets of up to ten million datapoints. We consider large language models for generative chemistry and graph neural networks for machine-learned interatomic potentials. We investigate the interplay between physical priors and scale and discover empirical neural-scaling relations for language models in \u2026", "year": 2023, "venue": "", "authors": "Nathan C Frey and Ryan Soklaski and Simon Axelrod and Siddharth Samsi and Rafael Gomez-Bombarelli and Connor W Coley and Vijay Gadepally"}, {"title": "Closed-form continuous-time neural networks", "abstract": "Continuous-time neural networks are a class of machine learning systems that can tackle representation learning on spatiotemporal decision-making tasks. These models are typically represented by continuous differential equations. However, their expressive power when they are deployed on computers is bottlenecked by numerical differential equation solvers. This limitation has notably slowed down the scaling and understanding of numerous natural physical phenomena such as the dynamics of nervous systems. Ideally, we would circumvent this bottleneck by solving the given dynamical system in closed form. This is known to be intractable in general. Here, we show that it is possible to closely approximate the interaction between neurons and synapses\u2014the building blocks of natural and artificial neural networks\u2014constructed by liquid time-constant networks efficiently in closed form. To this end, we compute \u2026", "year": 2022, "venue": "", "authors": "Ramin Hasani and Mathias Lechner and Alexander Amini and Lucas Liebenwein and Aaron Ray and Max Tschaikowski and Gerald Teschl and Daniela Rus"}, {"title": "Barriernet: Differentiable control barrier functions for learning of safe robot control", "abstract": "Many safety-critical applications of neural networks, such as robotic control, require safety guarantees. This article introduces a method for ensuring the safety of learned models for control using differentiable control barrier functions (dCBFs). dCBFs are end-to-end trainable and guarantee safety. They improve over classical control barrier functions (CBFs), which are usually overly conservative. Our dCBF solution relaxes the CBF definitions by: 1) using environmental dependencies; 2) embedding them into differentiable quadratic programs. These novel safety layers are called a BarrierNet. They can be used in conjunction with any neural network-based controller. They are trained by gradient descent. With BarrierNet, the safety constraints of a neural controller become adaptable to changing environments. We evaluate BarrierNet on the following several problems: 1) robot traffic merging; 2) robot navigation in 2-D \u2026", "year": 2023, "venue": "", "authors": "Wei Xiao and Tsun-Hsuan Wang and Ramin Hasani and Makram Chahine and Alexander Amini and Xiao Li and Daniela Rus"}, {"title": "Predictive and generative machine learning models for photonic crystals", "abstract": "The prediction and design of photonic features have traditionally been guided by theory-driven computational methods, spanning a wide range of direct solvers and optimization techniques. Motivated by enormous advances in the field of machine learning, there has recently been a growing interest in developing complementary data-driven methods for photonics. Here, we demonstrate several predictive and generative data-driven approaches for the characterization and inverse design of photonic crystals. Concretely, we built a data set of 20,000 two-dimensional photonic crystal unit cells and their associated band structures, enabling the training of supervised learning models. Using these data set, we demonstrate a high-accuracy convolutional neural network for band structure prediction, with orders-of-magnitude speedup compared to conventional theory-driven solvers. Separately, we demonstrate an \u2026", "year": 2020, "venue": "", "authors": "Thomas Christensen and Charlotte Loh and Stjepan Picek and Domagoj Jakobovi\u0107 and Li Jing and Sophie Fisher and Vladimir Ceperic and John D Joannopoulos and Marin Solja\u010di\u0107"}, {"title": "Editing a classifier by rewriting its prediction rules", "abstract": "We propose a methodology for modifying the behavior of a classifier by directly rewriting its prediction rules. Our method requires virtually no additional data collection and can be applied to a variety of settings, including adapting a model to new environments, and modifying it to ignore spurious features.", "year": 2021, "venue": "", "authors": "Shibani Santurkar and Dimitris Tsipras and Mahalaxmi Elango and David Bau and Antonio Torralba and Aleksander Madry"}, {"title": "Leveraging sparse linear layers for debuggable deep networks", "abstract": "We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantitatively and via human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks.", "year": 2021, "venue": "", "authors": "Eric Wong and Shibani Santurkar and Aleksander Madry"}, {"title": "Distilling model failures as directions in latent space", "abstract": "Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes. Code available at https://github.com/MadryLab/failure-directions", "year": 2022, "venue": "", "authors": "Saachi Jain and Hannah Lawrence and Ankur Moitra and Aleksander Madry"}, {"title": "Bayesian neural ordinary differential equations", "abstract": "Recently, Neural Ordinary Differential Equations has emerged as a powerful framework for modeling physical simulations without explicitly defining the ODEs governing the system, but instead learning them via machine learning. However, the question: \"Can Bayesian learning frameworks be integrated with Neural ODE's to robustly quantify the uncertainty in the weights of a Neural ODE?\" remains unanswered. In an effort to address this question, we primarily evaluate the following categories of inference methods: (a) The No-U-Turn MCMC sampler (NUTS), (b) Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) and (c) Stochastic Langevin Gradient Descent (SGLD). We demonstrate the successful integration of Neural ODEs with the above Bayesian inference frameworks on classical physical systems, as well as on standard machine learning datasets like MNIST, using GPU acceleration. On the MNIST dataset, we achieve a posterior sample accuracy of 98.5% on the test ensemble of 10,000 images. Subsequently, for the first time, we demonstrate the successful integration of variational inference with normalizing flows and Neural ODEs, leading to a powerful Bayesian Neural ODE object. Finally, considering a predator-prey model and an epidemiological system, we demonstrate the probabilistic identification of model specification in partially-described dynamical systems using universal ordinary differential equations. Together, this gives a scientific machine learning tool for probabilistic estimation of epistemic uncertainties.", "year": 2020, "venue": "", "authors": "Raj Dandekar and Karen Chung and Vaibhav Dixit and Mohamed Tarek and Aslan Garcia-Valadez and Krishna Vishal Vemula and Chris Rackauckas"}, {"title": "Reverse-mode automatic differentiation and optimization of GPU kernels via Enzyme", "abstract": "Computing derivatives is key to many algorithms in scientific computing and machine learning such as optimization, uncertainty quantification, and stability analysis. Enzyme is a LLVM compiler plugin that performs reverse-mode automatic differentiation (AD) and thus generates high performance gradients of programs in languages including C/C++, Fortran, Julia, and Rust. Prior to this work, Enzyme and other AD tools were not capable of generating gradients of GPU kernels. Our paper presents a combination of novel techniques that make Enzyme the first fully automatic reversemode AD tool to generate gradients of GPU kernels. Since unlike other tools Enzyme performs automatic differentiation within a general-purpose compiler, we are able to introduce several novel GPU and AD-specific optimizations. To show the generality and efficiency of our approach, we compute gradients of five GPU-based HPC \u2026", "year": 2021, "venue": "", "authors": "William S Moses and Valentin Churavy and Ludger Paehler and Jan H\u00fcckelheim and Sri Hari Krishna Narayanan and Michel Schanen and Johannes Doerfert"}, {"title": "Polygeist: Raising C to polyhedral MLIR", "abstract": "We present Polygeist, a new compilation flow that connects the MLIR compiler infrastructure to cutting edge polyhedral optimization tools. It consists of a C and C++ frontend capable of converting a broad range of existing codes into MLIR suitable for polyhedral transformation and a bi-directional conversion between MLIR and OpenScop exchange format. The Polygeist/MLIR intermediate representation featuring high-level (affine) loop constructs and n-D arrays embedded into a single static assignment (SSA) substrate enables an unprecedented combination of SSA-based and polyhedral optimizations. We illustrate this by proposing and implementing two extra transformations: statement splitting and reduction parallelization. Our evaluation demonstrates that Polygeist outperforms on average both an LLVM IR-level optimizer (Polly) and a source-to-source state-of-the-art polyhedral compiler (Pluto) when exercised \u2026", "year": 2021, "venue": "", "authors": "William S Moses and Lorenzo Chelini and Ruizhe Zhao and Oleksandr Zinenko"}, {"title": "Vistext: A benchmark for semantically rich chart captioning", "abstract": "Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts' construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph -- a hierarchical representation of a chart's visual elements akin to a web page's Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. Our models generate coherent, semantically rich captions and perform on par with state-of-the-art chart captioning models across machine translation and text generation metrics. Through qualitative analysis, we identify six broad categories of errors that our models make that can inform future work.", "year": 2023, "venue": "", "authors": "Benny J Tang and Angie Boggust and Arvind Satyanarayan"}, {"title": "Measuring social biases in grounded vision and language embeddings", "abstract": "We generalize the notion of social biases from language embeddings to grounded vision and language embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting extending standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society.", "year": 2020, "venue": "", "authors": "Candace Ross and Boris Katz and Andrei Barbu"}, {"title": "Certified patch robustness via smoothed vision transformers", "abstract": "Certified patch defenses can guarantee robustness of an image classifier to arbitrary changes within a bounded contiguous region. But, currently, this robustness comes at a cost of degraded standard accuracies and slower inference times. We demonstrate how using vision transformers enables significantly better certified patch robustness that is also more computationally efficient and does not incur a substantial drop in standard accuracy. These improvements stem from the inherent ability of the vision transformer to gracefully handle largely masked images.", "year": 2022, "venue": "", "authors": "Hadi Salman and Saachi Jain and Eric Wong and Aleksander Madry"}, {"title": "Embedding comparator: Visualizing differences in global structure and local neighborhoods via small multiples", "abstract": " Embeddings mapping high-dimensional discrete input to lower-dimensional continuous vector spaces have been widely adopted in machine learning applications as a way to capture domain semantics. Interviewing 13 embedding users across disciplines, we find comparing embeddings is a key task for deployment or downstream analysis but unfolds in a tedious fashion that poorly supports systematic exploration. In response, we present the Embedding Comparator, an interactive system that presents a global comparison of embedding spaces alongside fine-grained inspection of local neighborhoods. It systematically surfaces points of comparison by computing the similarity of the k-nearest neighbors of every embedded object between a pair of spaces. Through case studies across multiple modalities, we demonstrate our system rapidly reveals insights, such as semantic changes following fine-tuning, language \u2026", "year": 2022, "venue": "", "authors": "Angie Boggust and Brandon Carter and Arvind Satyanarayan"}, {"title": "Accelerating training and inference of graph neural networks with fast sampling and pipelining", "abstract": "Improving the training and inference performance of graph neural networks (GNNs) is faced with a challenge uncommon in general neural networks: creating mini-batches requires a lot of computation and data movement due to the exponential growth of multi-hop graph neighborhoods along network layers. Such a unique challenge gives rise to a diverse set of system design choices. We argue in favor of performing mini-batch training with neighborhood sampling in a distributed multi-GPU environment, under which we identify major performance bottlenecks hitherto under-explored by developers: mini-batch preparation and transfer. We present a sequence of improvements to mitigate these bottlenecks, including a performance-engineered neighborhood sampler, a shared-memory parallelization strategy, and the pipelining of batch transfer with GPU computation. We also conduct an empirical analysis that supports the use of sampling for inference, showing that test accuracies are not materially compromised. Such an observation unifies training and inference, simplifying model implementation. We report comprehensive experimental results with several benchmark data sets and GNN architectures, including a demonstration that, for the ogbn-papers100M data set, our system SALIENT achieves a speedup of 3x over a standard PyTorch-Geometric implementation with a single GPU and a further 8x parallel speedup with 16 GPUs. Therein, training a 3-layer GraphSAGE model with sampling fanout (15, 10, 5) takes 2.0 seconds per epoch and inference with fanout (20, 20, 20) takes 2.4 seconds, attaining test accuracy 64.58%.", "year": 2022, "venue": "", "authors": "Tim Kaler and Nickolas Stathas and Anne Ouyang and Alexandros-Stavros Iliopoulos and Tao Schardl and Charles E Leiserson and Jie Chen"}, {"title": "Curiosity-driven red-teaming for large language models", "abstract": "Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \\textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at \\url{https://github.com/Improbable-AI/curiosity_redteam}", "year": 2024, "venue": "", "authors": "Zhang-Wei Hong and Idan Shenfeld and Tsun-Hsuan Wang and Yung-Sung Chuang and Aldo Pareja and James Glass and Akash Srivastava and Pulkit Agrawal"}, {"title": "Miso: exploiting multi-instance gpu capability on multi-tenant gpu clusters", "abstract": "GPU technology has been improving at an expedited pace in terms of size and performance, empowering HPC and AI/ML researchers to advance the scientific discovery process. However, this also leads to inefficient resource usage, as most GPU workloads, including complicated AI/ML models, are not able to utilize the GPU resources to their fullest extent - encouraging support for GPU multi-tenancy. We propose MISO, a technique to exploit the Multi-Instance GPU (MIG) capability on the latest NVIDIA datacenter GPUs (e.g., A100, H100) to dynamically partition GPU resources among co-located jobs. MISO's key insight is to use the lightweight, more flexible Multi-Process Service (MPS) capability to predict the best MIG partition allocation for different jobs, without incurring the overhead of implementing them during exploration. Due to its ability to utilize GPU resources more efficiently, MISO achieves 49% and 16 \u2026", "year": 2022, "venue": "", "authors": "Baolin Li and Tirthak Patel and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "Generalization of model-agnostic meta-learning algorithms: Recurring and unseen tasks", "abstract": "In this paper, we study the generalization properties of Model-Agnostic Meta-Learning (MAML) algorithms for supervised learning problems. We focus on the setting in which we train the MAML model over  tasks, each with  data points, and characterize its generalization error from two points of view: First, we assume the new task at test time is one of the training tasks, and we show that, for strongly convex objective functions, the expected excess population loss is bounded by . Second, we consider the MAML algorithm's generalization to an unseen task and show that the resulting generalization error depends on the total variation distance between the underlying distributions of the new task and the tasks observed during the training process. Our proof techniques rely on the connections between algorithmic stability and generalization bounds of algorithms. In particular, we propose a new definition of stability for meta-learning algorithms, which allows us to capture the role of both the number of tasks  and number of samples per task  on the generalization error of MAML.", "year": 2021, "venue": "", "authors": "Alireza Fallah and Aryan Mokhtari and Asuman Ozdaglar"}, {"title": "Great power, great responsibility: Recommendations for reducing energy for training language models", "abstract": "The energy requirements of current natural language processing models continue to grow at a rapid, unsustainable pace. Recent works highlighting this problem conclude there is an urgent need for methods that reduce the energy needs of NLP and machine learning more broadly. In this article, we investigate techniques that can be used to reduce the energy consumption of common NLP applications. In particular, we focus on techniques to measure energy usage and different hardware and datacenter-oriented settings that can be tuned to reduce energy consumption for training and inference for language models. We characterize the impact of these settings on metrics such as computational performance and energy consumption through experiments conducted on a high performance computing system as well as popular cloud computing platforms. These techniques can lead to significant reduction in energy consumption when training language models or their use for inference. For example, power-capping, which limits the maximum power a GPU can consume, can enable a 15\\% decrease in energy usage with marginal increase in overall computation time when training a transformer-based language model.", "year": 2022, "venue": "", "authors": "Joseph McDonald and Baolin Li and Nathan Frey and Devesh Tiwari and Vijay Gadepally and Siddharth Samsi"}, {"title": "Identifying statistical bias in dataset replication", "abstract": "Dataset replication is a useful tool for assessing whether improvements in test accuracy on a specific benchmark correspond to improvements in models\u2019 ability to generalize reliably. In this work, we present unintuitive yet significant ways in which standard approaches to dataset replication introduce statistical bias, skewing the resulting observations. We study ImageNet-v2, a replication of the ImageNet dataset on which models exhibit a significant (11-14%) drop in accuracy, even after controlling for selection frequency, a human-in-the-loop measure of data quality. We show that after remeasuring selection frequencies and correcting for statistical bias, only an estimated 3.6% of the original 11.7% accuracy drop remains unaccounted for. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication. Code for our study is publicly available: https://git. io/data-rep-analysis.", "year": 2020, "venue": "", "authors": "Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Jacob Steinhardt and Aleksander Madry"}, {"title": "Bliss: auto-tuning complex applications using a pool of diverse lightweight learning models", "abstract": "As parallel applications become more complex, auto-tuning becomes more desirable, challenging, and time-consuming. We propose, Bliss, a novel solution for auto-tuning parallel applications without requiring apriori information about applications, domain-specific knowledge, or instrumentation. Bliss demonstrates how to leverage a pool of Bayesian Optimization models to find the near-optimal parameter setting 1.64\u00d7 faster than the state-of-the-art approaches.", "year": 2021, "venue": "", "authors": "Rohan Basu Roy and Tirthak Patel and Vijay Gadepally and Devesh Tiwari"}, {"title": "Terrace: A hierarchical graph container for skewed dynamic graphs", "abstract": "Various applications model problems as streaming graphs, which need to quickly apply a stream of updates and run algorithms on the updated graph. Furthermore, many dynamic real-world graphs, such as social networks, follow a skewed distribution of vertex degrees, where there are a few high-degree vertices and many low-degree vertices.Existing static graph-processing systems optimized for graph skewness achieve high performance and low space usage by preprocessing a cache-efficient graph partitioning based on vertex degree. In the streaming setting, the whole graph is not available upfront, however, so finding an optimal partitioning is not feasible in the presence of updates. As a result, existing streaming graph-processing systems take a \"one-size-fits-all\" approach, leaving performance on the table.We present Terrace, a system for streaming graphs that uses a hierarchical data structure design to \u2026", "year": 2021, "venue": "", "authors": "Prashant Pandey and Brian Wheatman and Helen Xu and Aydin Buluc"}, {"title": "Unadversarial examples: Designing objects for robust vision", "abstract": "We study a class of computer vision settings wherein one can modify the design of the objects being recognized. We develop a framework that leverages this capability---and deep networks' unusual sensitivity to input perturbations---to design``robust objects,''ie, objects that are explicitly optimized to be confidently classified. Our framework yields improved performance on standard benchmarks, a simulated robotics environment, and physical-world experiments.", "year": 2021, "venue": "", "authors": "Hadi Salman and Andrew Ilyas and Logan Engstrom and Sai Vemprala and Aleksander Madry and Ashish Kapoor"}, {"title": "Dribblebot: Dynamic legged manipulation in the wild", "abstract": "DribbleBot (Dexterous Ball Manipulation with a Legged Robot) is a legged robotic system that can dribble a soccer ball under the same real-world conditions as humans. We identify key challenges of in-the-wild soccer ball manipulation, including variable ball motion dynamics and perception using body-mounted cameras. To overcome these challenges, we propose a domain and task specification for learning viable soccer dribbling behaviors in simulation that transfer to real fields. Our system provides promising evidence that current legged robots are physically capable and adequately sensorized for varied and dynamic real-world soccer play. Video is available at https://gmargoll.github.io/dribblebot.", "year": 2023, "venue": "", "authors": "Yandong Ji and Gabriel B Margolis and Pulkit Agrawal"}, {"title": "Using trio: juniper networks' programmable chipset-for emerging in-network applications", "abstract": "This paper describes Trio, a programmable chipset used in Juniper Networks' MX-series routers and switches. Trio's architecture is based on a multi-threaded programmable packet processing engine and a hierarchy of high-capacity memory systems, making it fundamentally different from pipeline-based architectures. Trio gracefully handles non-homogeneous packet processing rates for a wide range of networking use cases and protocols, making it an ideal platform for emerging in-network applications. We begin by describing the Trio chipset's fundamental building blocks, including its multi-threaded Packet Forwarding and Packet Processing Engines. We then discuss Trio's programming language, called Microcode. To showcase Trio's flexible Microcode-based programming environment, we describe two use cases. First, we demonstrate Trio's ability to perform in-network aggregation for distributed machine \u2026", "year": 2022, "venue": "", "authors": "Mingran Yang and Alex Baban and Valery Kugel and Jeff Libby and Scott Mackie and Swamy Sadashivaiah Renu Kananda and Chang-Hong Wu and Manya Ghobadi"}, {"title": "Mashup: making serverless computing useful for hpc workflows via hybrid execution", "abstract": "This work introduces Mashup, a novel strategy to leverage serverless computing model for executing scientific workflows in a hybrid fashion by taking advantage of both the traditional VM-based cloud computing platform and the emerging serverless platform. Mashup outperforms the state-of-the-art workflow execution engines by an average of 34% and 43% in terms of execution time reduction and cost reduction, respectively, for widely-used HPC workflows on the Amazon Cloud platform (EC2 and Lambda).", "year": 2022, "venue": "", "authors": "Rohan Basu Roy and Tirthak Patel and Vijay Gadepally and Devesh Tiwari"}, {"title": "Train simultaneously, generalize better: Stability of gradient-based minimax learners", "abstract": "The success of minimax learning problems of generative adversarial networks (GANs) has been observed to depend on the minimax optimization algorithm used for their training. This dependence is commonly attributed to the convergence speed and robustness properties of the underlying optimization algorithm. In this paper, we show that the optimization algorithm also plays a key role in the generalization performance of the trained minimax model. To this end, we analyze the generalization properties of standard gradient descent ascent (GDA) and proximal point method (PPM) algorithms through the lens of algorithmic stability as defined by Bousquet & Elisseeff, 2002 under both convex-concave and nonconvex-nonconcave minimax settings. While the GDA algorithm is not guaranteed to have a vanishing excess risk in convex-concave problems, we show the PPM algorithm enjoys a bounded excess risk in the same setup. For nonconvex-nonconcave problems, we compare the generalization performance of stochastic GDA and GDmax algorithms where the latter fully solves the maximization subproblem at every iteration. Our generalization analysis suggests the superiority of GDA provided that the minimization and maximization subproblems are solved simultaneously with similar learning rates. We discuss several numerical results indicating the role of optimization algorithms in the generalization of learned minimax models.", "year": 2021, "venue": "", "authors": "Farzan Farnia and Asuman Ozdaglar"}, {"title": "Causal navigation by continuous-time neural networks", "abstract": "Imitation learning enables high-fidelity, vision-based learning of policies within rich, photorealistic environments. However, such techniques often rely on traditional discrete-time neural models and face difficulties in generalizing to domain shifts by failing to account for the causal relationships between the agent and the environment. In this paper, we propose a theoretical and experimental framework for learning causal representations using continuous-time neural networks, specifically over their discrete-time counterparts. We evaluate our method in the context of visual-control learning of drones over a series of complex tasks, ranging from short-and long-term navigation, to chasing static and dynamic objects through photorealistic environments. Our results demonstrate that causal continuous-time deep models can perform robust navigation tasks, where advanced recurrent models fail. These models learn complex causal control representations directly from raw visual inputs and scale to solve a variety of tasks using imitation learning.", "year": 2021, "venue": "", "authors": "Charles Vorbach and Ramin Hasani and Alexander Amini and Mathias Lechner and Daniela Rus"}, {"title": "Robust flight navigation out of distribution with liquid neural networks", "abstract": "Autonomous robots can learn to perform visual navigation tasks from offline human demonstrations and generalize well to online and unseen scenarios within the same environment they have been trained on. It is challenging for these agents to take a step further and robustly generalize to new environments with drastic scenery changes that they have never encountered. Here, we present a method to create robust flight navigation agents that successfully perform vision-based fly-to-target tasks beyond their training environment under drastic distribution shifts. To this end, we designed an imitation learning framework using liquid neural networks, a brain-inspired class of continuous-time neural models that are causal and adapt to changing conditions. We observed that liquid agents learn to distill the task they are given from visual inputs and drop irrelevant features. Thus, their learned navigation skills transferred to \u2026", "year": 2023, "venue": "", "authors": "Makram Chahine and Ramin Hasani and Patrick Kao and Aaron Ray and Ryan Shubert and Mathias Lechner and Alexander Amini and Daniela Rus"}, {"title": "Automatic differentiation of programs with discrete randomness", "abstract": "Automatic differentiation (AD), a technique for constructing new programs which compute the derivative of an original program, has become ubiquitous throughout scientific computing and deep learning due to the improved performance afforded by gradient-based optimization. However, AD systems have been restricted to the subset of programs that have a continuous dependence on parameters. Programs that have discrete stochastic behaviors governed by distribution parameters, such as flipping a coin with probability  of being heads, pose a challenge to these systems because the connection between the result (heads vs tails) and the parameters () is fundamentally discrete. In this paper we develop a new reparameterization-based methodology that allows for generating programs whose expectation is the derivative of the expectation of the original program. We showcase how this method gives an unbiased and low-variance estimator which is as automated as traditional AD mechanisms. We demonstrate unbiased forward-mode AD of discrete-time Markov chains, agent-based models such as Conway's Game of Life, and unbiased reverse-mode AD of a particle filter. Our code package is available at https://github. com/gaurav-arya/StochasticAD. jl.", "year": 2022, "venue": "", "authors": "Gaurav Arya and Moritz Schauer and Frank Sch\u00e4fer and Christopher Rackauckas"}, {"title": "Compressing neural networks: Towards determining the optimal layer-wise decomposition", "abstract": "We present a novel global compression framework for deep neural networks that automatically analyzes each layer to identify the optimal per-layer compression ratio, while simultaneously achieving the desired overall compression. Our algorithm hinges on the idea of compressing each convolutional (or fully-connected) layer by slicing its channels into multiple groups and decomposing each group via low-rank decomposition. At the core of our algorithm is the derivation of layer-wise error bounds from the Eckart\u2013Young\u2013Mirsky theorem. We then leverage these bounds to frame the compression problem as an optimization problem where we wish to minimize the maximum compression error across layers and propose an efficient algorithm towards a solution. Our experiments indicate that our method outperforms existing low-rank compression approaches across a wide range of networks and data sets. We believe that our results open up new avenues for future research into the global performance-size trade-offs of modern neural networks.", "year": 2021, "venue": "", "authors": "Lucas Liebenwein and Alaa Maalouf and Dan Feldman and Daniela Rus"}, {"title": "Foundations of spatial perception for robotics: Hierarchical representations and real-time systems", "abstract": "3D spatial perception is the problem of building and maintaining an actionable and persistent representation of the environment in real-time using sensor data and prior knowledge. Despite the fast-paced progress in robot perception, most existing methods either build purely geometric maps (as in traditional SLAM) or \u201cflat\u201d metric-semantic maps that do not scale to large environments or large dictionaries of semantic labels. The first part of this paper is concerned with representations: we show that scalable representations for spatial perception need to be hierarchical in nature. Hierarchical representations are efficient to store, and lead to layered graphs with small treewidth, which enable provably efficient inference. We then introduce an example of hierarchical representation for indoor environments, namely a 3D scene graph, and discuss its structure and properties. The second part of the paper focuses on \u2026", "year": 2024, "venue": "", "authors": "Nathan Hughes and Yun Chang and Siyi Hu and Rajat Talak and Rumaia Abdulhai and Jared Strader and Luca Carlone"}, {"title": "3db: A framework for debugging computer vision models", "abstract": "We introduce 3DB: an extendable, unified framework for testing and debugging vision models using photorealistic simulation. We demonstrate, through a wide range of use cases, that 3DB allows users to discover vulnerabilities in computer vision systems and gain insights into how models make decisions. 3DB captures and generalizes many robustness analyses from prior work, and enables one to study their interplay. Finally, we find that the insights generated by the system transfer to the physical world. 3DB will be released as a library alongside a set of examples and documentation. We attach 3DB to the submission.", "year": 2022, "venue": "", "authors": "Guillaume Leclerc and Hadi Salman and Andrew Ilyas and Sai Vemprala and Logan Engstrom and Vibhav Vineet and Kai Xiao and Pengchuan Zhang and Shibani Santurkar and Greg Yang and Ashish Kapoor and Aleksander Madry"}, {"title": "Video action understanding", "abstract": "Many believe that the successes of deep learning on image understanding problems can be replicated in the realm of video understanding. However, due to the scale and temporal nature of video, the span of video understanding problems and the set of proposed deep learning solutions is arguably wider and more diverse than those of their 2D image siblings. Finding, identifying, and predicting actions are a few of the most salient tasks in this emerging and rapidly evolving field. With a pedagogical emphasis, this tutorial introduces and systematizes fundamental topics, basic concepts, and notable examples in supervised video action understanding. Specifically, we clarify a taxonomy of action problems, catalog and highlight video datasets, describe common video data preparation methods, present the building blocks of state-of-the-art deep learning model architectures, and formalize domain-specific metrics to \u2026", "year": 2021, "venue": "", "authors": "Matthew S Hutchinson and Vijay N Gadepally"}, {"title": "A data-based perspective on transfer learning", "abstract": "It is commonly believed that more pre-training data leads to better transfer learning performance. However, recent evidence suggests that removing data from the source dataset can actually help too. In this work, we present a framework for probing the impact of the source dataset's composition on transfer learning performance. Our framework facilitates new capabilities such as identifying transfer learning brittleness and detecting pathologies such as data-leakage and the presence of misleading examples in the source dataset. In particular, we demonstrate that removing detrimental datapoints identified by our framework improves transfer performance from ImageNet on a variety of transfer tasks.", "year": 2023, "venue": "", "authors": "Saachi Jain and Hadi Salman and Alaa Khaddaj and Eric Wong and Sung Min Park and Aleksander M\u0105dry"}, {"title": "Shared interest: Measuring human-ai alignment to identify recurring patterns in model behavior", "abstract": "Saliency methods \u2014 techniques to identify the importance of input features on a model\u2019s output \u2014 are a common step in understanding neural network behavior. However, interpreting saliency requires tedious manual inspection to identify and aggregate patterns in model behavior, resulting in ad hoc or cherry-picked analysis. To address these concerns, we present Shared Interest: metrics for comparing model reasoning (via saliency) to human reasoning (via ground truth annotations). By providing quantitative descriptors, Shared Interest enables ranking, sorting, and aggregating inputs, thereby facilitating large-scale systematic analysis of model behavior. We use Shared Interest to identify eight recurring patterns in model behavior, such as cases where contextual features or a subset of ground truth features are most important to the model. Working with representative real-world users, we show how Shared \u2026", "year": 2022, "venue": "", "authors": "Angie Boggust and Benjamin Hoover and Arvind Satyanarayan and Hendrik Strobelt"}, {"title": "Toward sustainable hpc: Carbon footprint estimation and environmental implications of hpc systems", "abstract": "The rapid growth in demand for HPC systems has led to a rise in carbon footprint, which requires urgent intervention. In this work, we present a comprehensive analysis of the carbon footprint of highperformance computing (HPC) systems, considering the carbon footprint during both the hardware manufacturing and system operational stages. Our work employs HPC hardware component carbon footprint modeling, regional carbon intensity analysis, and experimental characterization of the system life cycle to highlight the importance of quantifying the carbon footprint of HPC systems.", "year": 2023, "venue": "", "authors": "Baolin Li and Rohan Basu Roy and Daniel Wang and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "Dataset distillation with convexified implicit gradients", "abstract": "We propose a new dataset distillation algorithm using reparameterization and convexification of implicit gradients (RCIG), that substantially improves the state-of-the-art. To this end, we first formulate dataset distillation as a bi-level optimization problem. Then, we show how implicit gradients can be effectively used to compute meta-gradient updates. We further equip the algorithm with a convexified approximation that corresponds to learning on top of a frozen finite-width neural tangent kernel. Finally, we improve bias in implicit gradients by parameterizing the neural network to enable analytical computation of final-layer parameters given the body parameters. RCIG establishes the new state-of-the-art on a diverse series of dataset distillation tasks. Notably, with one image per class, on resized ImageNet, RCIG sees on average a 108% improvement over the previous state-of-the-art distillation algorithm. Similarly, we observed a 66% gain over SOTA on Tiny-ImageNet and 37% on CIFAR-100.", "year": 2023, "venue": "", "authors": "Noel Loo and Ramin Hasani and Mathias Lechner and Daniela Rus"}, {"title": "Featup: A model-agnostic framework for features at any resolution", "abstract": "Deep features are a cornerstone of computer vision research, capturing image semantics and enabling the community to solve downstream tasks even in the zero- or few-shot regime. However, these features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. In this work, we introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-training. We show that FeatUp significantly outperforms other feature upsampling and image super-resolution approaches in class activation map generation, transfer learning for segmentation and depth prediction, and end-to-end training for semantic segmentation.", "year": 2024, "venue": "", "authors": "Stephanie Fu and Mark Hamilton and Laura Brandt and Axel Feldman and Zhoutong Zhang and William T Freeman"}, {"title": "Experimental evaluation of nisq quantum computers: Error measurement, characterization, and implications", "abstract": "Noisy Intermediate-Scale Quantum (NISQ) computers are being increasingly used for executing early-stage quantum programs to establish the practical realizability of existing quantum algorithms. These quantum programs have uses cases in the realm of high-performance computing ranging from molecular chemistry and physics simulations to addressing NP-complete optimization problems. However, NISQ devices are prone to multiple types of errors, which affect the fidelity and reproducibility of the program execution. As the technology is still primitive, our understanding of these quantum machines and their error characteristics is limited. To bridge that understanding gap, this is the first work to provide a systematic and rich experimental evaluation of IBM Quantum Experience (QX) quantum computers of different scales and topologies. Our experimental evaluation uncovers multiple important and interesting \u2026", "year": 2020, "venue": "", "authors": "Tirthak Patel and Abhay Potharaju and Baolin Li and Rohan Basu Roy and Devesh Tiwari"}, {"title": "Learning a natural-language to LTL executable semantic parser for grounded robotics", "abstract": "Children acquire their native language with apparent ease by observing how language is used in context and attempting to use it themselves. They do so without laborious annotations, negative examples, or even direct corrections. We take a step toward robots that can do the same by training a grounded semantic parser, which discovers latent linguistic representations that can be used for the execution of natural-language commands. In particular, we focus on the difficult domain of commands with a temporal aspect, whose semantics we capture with Linear Temporal Logic, LTL. Our parser is trained with pairs of sentences and executions as well as an executor. At training time, the parser hypothesizes a meaning representation for the input as a formula in LTL. Three competing pressures allow the parser to discover meaning from language. First, any hypothesized meaning for a sentence must be permissive enough to reflect all the annotated execution trajectories. Second, the executor\u2014a pretrained end-to-end LTL planner\u2014must find that the observed trajectories are likely executions of the meaning. Finally, a generator, which reconstructs the original input, encourages the model to find representations that conserve knowledge about the command. Together these ensure that the meaning is neither too general nor too specific. Our model generalizes well, being able to parse and execute both machine-generated and human-generated commands, with near-equal accuracy, despite the fact that the human-generated sentences are much more varied and complex with an open lexicon. The approach presented here is not specific to LTL: it can \u2026", "year": 2021, "venue": "", "authors": "Christopher Wang and Candace Ross and Yen-Ling Kuo and Boris Katz and Andrei Barbu"}, {"title": "Tuplex: Data science in python at native code speed", "abstract": "Today's data science pipelines often rely on user-defined functions (UDFs) written in Python. But interpreted Python code is slow, and Python UDFs cannot be compiled to machine code easily.We present Tuplex, a new data analytics framework that just in-time compiles developers' natural Python UDFs into efficient, end-to-end optimized native code. Tuplex introduces a novel dual-mode execution model that compiles an optimized fast path for the common case, and falls back on slower exception code paths for data that fail to match the fast path's assumptions. Dual-mode execution is crucial to making end-to-end optimizing compilation tractable: by focusing on the common case, Tuplex keeps the code simple enough to apply aggressive optimizations. Thanks to dual-mode execution, Tuplex pipelines always complete even if exceptions occur, and Tuplex's post-facto exception handling simplifies debugging \u2026", "year": 2021, "venue": "", "authors": "Leonhard Spiegelberg and Rahul Yesantharao and Malte Schwarzkopf and Tim Kraska"}, {"title": "Dataset interfaces: Diagnosing model failures using controllable counterfactual generation", "abstract": "Distribution shift is a major source of failure for machine learning models. However, evaluating model reliability under distribution shift can be challenging, especially since it may be difficult to acquire counterfactual examples that exhibit a specified shift. In this work, we introduce the notion of a dataset interface: a framework that, given an input dataset and a user-specified shift, returns instances from that input distribution that exhibit the desired shift. We study a number of natural implementations for such an interface, and find that they often introduce confounding shifts that complicate model evaluation. Motivated by this, we propose a dataset interface implementation that leverages Textual Inversion to tailor generation to the input distribution. We then demonstrate how applying this dataset interface to the ImageNet dataset enables studying model behavior across a diverse array of distribution shifts, including variations in background, lighting, and attributes of the objects. Code available at https://github.com/MadryLab/dataset-interfaces.", "year": 2023, "venue": "", "authors": "Joshua Vendrow and Saachi Jain and Logan Engstrom and Aleksander Madry"}, {"title": "When does bias transfer in transfer learning?", "abstract": "Using transfer learning to adapt a pre-trained \"source model\" to a downstream \"target task\" can dramatically increase performance with seemingly no downside. In this work, we demonstrate that there can exist a downside after all: bias transfer, or the tendency for biases of the source model to persist even after adapting the model to the target class. Through a combination of synthetic and natural experiments, we show that bias transfer both (a) arises in realistic settings (such as when pre-training on ImageNet or other standard datasets) and (b) can occur even when the target dataset is explicitly de-biased. As transfer-learned models are increasingly deployed in the real world, our work highlights the importance of understanding the limitations of pre-trained source models. Code is available at https://github.com/MadryLab/bias-transfer", "year": 2022, "venue": "", "authors": "Hadi Salman and Saachi Jain and Andrew Ilyas and Logan Engstrom and Eric Wong and Aleksander Madry"}, {"title": "Exsample: Efficient searches on video repositories through adaptive sampling", "abstract": "Capturing and processing video is increasingly common as cameras become cheaper to deploy. At the same time, rich video-understanding methods have progressed greatly in the last decade. As a result, many organizations now have massive repositories of video data, with applications in mapping, navigation, autonomous driving, and other areas. Because state-of-the-art object-detection methods are slow and expensive, our ability to process even simple ad-hoc object search queries (\u201cfind 100 traffic lights in dashcam video\u201d) over this accumulated data lags far behind our ability to collect the data. Processing video at reduced sampling rates is a reasonable default strategy for these types of queries; however, the ideal sampling rate is both data and query dependent. We introduce ExSample, a low cost framework for object search over un-indexed video that quickly processes search queries by adapting the \u2026", "year": 2022, "venue": "", "authors": "Oscar Moll and Favyen Bastani and Sam Madden and Mike Stonebraker and Vijay Gadepally and Tim Kraska"}, {"title": "Provably convergent policy gradient methods for model-agnostic meta-reinforcement learning", "abstract": "We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement Learning (RL) problems where the goal is to find a policy (using data from several tasks represented by Markov Decision Processes (MDPs)) that can be updated by one step of stochastic policy gradient for the realized MDP. In particular, using stochastic gradients in MAML update step is crucial for RL problems since computation of exact gradients requires access to a large number of possible trajectories. For this formulation, we propose a variant of the MAML method, named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and study its convergence properties. We derive the iteration and sample complexity of SG-MRL to find an efirst-order stationary point, which, to the best of our knowledge, provides the first convergence guarantee for model-agnostic meta-reinforcement learning algorithms. We further show how our results extend to the case where more than one step of stochastic policy gradient method is used in the update during the test time.", "year": 2020, "venue": "", "authors": "Alireza Fallah and Aryan Mokhtari and Asuman Ozdaglar"}, {"title": "BrainBERT: Self-supervised representation learning for intracranial recordings", "abstract": "We create a reusable Transformer, BrainBERT, for intracranial recordings bringing modern representation learning approaches to neuroscience. Much like in NLP and speech recognition, this Transformer enables classifying complex concepts, i.e., decoding neural data, with higher accuracy and with much less data by being pretrained in an unsupervised manner on a large corpus of unannotated neural recordings. Our approach generalizes to new subjects with electrodes in new positions and to unrelated tasks showing that the representations robustly disentangle the neural signal. Just like in NLP where one can study language by investigating what a language model learns, this approach opens the door to investigating the brain by what a model of the brain learns. As a first step along this path, we demonstrate a new analysis of the intrinsic dimensionality of the computations in different areas of the brain. To construct these representations, we combine a technique for producing super-resolution spectrograms of neural data with an approach designed for generating contextual representations of audio by masking. In the future, far more concepts will be decodable from neural recordings by using representation learning, potentially unlocking the brain like language models unlocked language.", "year": 2023, "venue": "", "authors": "Christopher Wang and Vighnesh Subramaniam and Adam Uri Yaari and Gabriel Kreiman and Boris Katz and Ignacio Cases and Andrei Barbu"}, {"title": "GlobalSensitivity. jl: Performant and Parallel GlobalSensitivity Analysis with Julia", "abstract": "Global Sensitivity Analysis (GSA) methods are used to quantify the uncertainty in the output of a model with respect to the parameters. These methods allow practitioners to measure both parameters\u2019 individual contributions and the contribution of their interactions to the output uncertainty. GlobalSensitivity. jl is a Julia (Bezanson et al., 2017) package containing implementations of some of the most popular GSA methods. Currently it supports Delta Moment-Independent (Borgonovo, 2007; Plischke et al., 2013), DGSM (Sobol\u2019& Kucherenko, 2009), EASI (Plischke, 2010, 2012), eFAST (A. Saltelli et al., 1999; Andrea Saltelli & Bolado, 1998), Morris (Campolongo et al., 2007; Morris, 1991), Fractional Factorial (Andrea Saltelli et al., 2008), RBD-FAST (Tarantola et al., 2006), Sobol (Andrea Saltelli, 2002; Andrea Saltelli et al., 2008; Sobol\u2019, 2001) and regression-based sensitivity (Ridolfi & Mooij, 2016) methods.", "year": 2022, "venue": "", "authors": "Vaibhav Kumar Dixit and Christopher Rackauckas"}, {"title": "Ai-enabling workloads on large-scale gpu-accelerated system: characterization, opportunities, and implications", "abstract": "Production high-performance computing (HPC) systems are adopting and integrating GPUs into their design to accommodate artificial intelligence (AI), machine learning, and data visualization workloads. To aid with the design and operations of new and existing GPU-based large-scale systems, we provide a detailed characterization of system operations, job characteristics, user behavior, and trends on a contemporary GPU-accelerated production HPC system. Our insights indicate that the pre-mature phases in modern AI workflow take up significant GPU hours while underutilizing GPUs, which opens up the opportunity for a multi-tier system. Finally, we provide various potential recommendations and areas for future investment for system architects, operators, and users.", "year": 2022, "venue": "", "authors": "Baolin Li and Rohin Arora and Siddharth Samsi and Tirthak Patel and William Arcand and David Bestor and Chansup Byun and Rohan Basu Roy and Bill Bergeron and John Holodnak and Michael Houle and Matthew Hubbell and Michael Jones and Jeremy Kepner and Anna Klein and Peter Michaleas and Joseph McDonald and Lauren Milechin and Julie Mullen and Andrew Prout and Benjamin Price and Albert Reuther and Antonio Rosa and Matthew Weiss and Charles Yee and Daniel Edelman and Allan Vanterpool and Anson Cheng and Vijay Gadepally and Devesh Tiwari"}, {"title": "Clover: Toward sustainable ai with carbon-aware machine learning inference service", "abstract": "This paper presents a solution to the challenge of mitigating carbon emissions from hosting large-scale machine learning (ML) inference services. ML inference is critical to modern technology products, but it is also a significant contributor to carbon footprint. We introduce, Clover, a carbon-friendly ML inference service runtime system that balances performance, accuracy, and carbon emissions through mixed-quality models and GPU resource partitioning. Our experimental results demonstrate that Clover is effective in substantially reducing carbon emissions while maintaining high accuracy and meeting service level agreement (SLA) targets.", "year": 2023, "venue": "", "authors": "Baolin Li and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "Redeeming intrinsic rewards via constrained optimization", "abstract": "State-of-the-art reinforcement learning (RL) algorithms typically use random sampling (eg, -greedy) for exploration, but this method fails on hard exploration tasks like Montezuma's Revenge. To address the challenge of exploration, prior works incentivize exploration by rewarding the agent when it visits novel states. Such intrinsic rewards (also called exploration bonus or curiosity) often lead to excellent performance on hard exploration tasks. However, on easy exploration tasks, the agent gets distracted by intrinsic rewards and performs unnecessary exploration even when sufficient task (also called extrinsic) reward is available. Consequently, such an overly curious agent performs worse than an agent trained with only task reward. Such inconsistency in performance across tasks prevents the widespread use of intrinsic rewards with RL algorithms. We propose a principled constrained optimization procedure called Extrinsic-Intrinsic Policy Optimization (EIPO) that automatically tunes the importance of the intrinsic reward: it suppresses the intrinsic reward when exploration is unnecessary and increases it when exploration is required. The results is superior exploration that does not require manual tuning in balancing the intrinsic reward against the task reward. Consistent performance gains across sixty-one ATARI games validate our claim. The code is available at https://github. com/Improbable-AI/eipo.", "year": 2022, "venue": "", "authors": "Eric Chen and Zhang-Wei Hong and Joni Pajarinen and Pulkit Agrawal"}, {"title": "Phase: Physically-grounded abstract social events for machine social perception", "abstract": "The ability to perceive and reason about social interactions in the context of physical environments is core to human social intelligence and human-machine cooperation. However, no prior dataset or benchmark has systematically evaluated physically grounded perception of complex social interactions that go beyond short actions, such as high-fiving, or simple group activities, such as gathering. In this work, we create a dataset of physically-grounded abstract social events, PHASE, that resemble a wide range of real-life social interactions by including social concepts such as helping another agent. PHASE consists of 2D animations of pairs of agents moving in a continuous space generated procedurally using a physics engine and a hierarchical planner. Agents have a limited field of view, and can interact with multiple objects, in an environment that has multiple landmarks and obstacles. Using PHASE, we design a social recognition task and a social prediction task. PHASE is validated with human experiments demonstrating that humans perceive rich interactions in the social events, and that the simulated agents behave similarly to humans. As a baseline model, we introduce a Bayesian inverse planning approach, SIMPLE (SIMulation, Planning and Local Estimation), which outperforms state-of-the-art feed-forward neural networks. We hope that PHASE can serve as a difficult new challenge for developing new models that can recognize complex social interactions.", "year": 2021, "venue": "", "authors": "Aviv Netanyahu and Tianmin Shu and Boris Katz and Andrei Barbu and Joshua B Tenenbaum"}, {"title": "Towards instance-optimized data systems", "abstract": "In recent years, we have seen increased interest in applying machine learning to system problems. For example, there has been work on applying machine learning to improve query optimization, indexing, storage layouts, scheduling, log-structured merge trees, sorting, compression, and sketches, among many other data management tasks. Arguably, the ideas behind these techniques are similar: machine learning is used to model the data and/or workload in order to derive a more efficient algorithm or data structure. Ultimately, these techniques will allow us to build \"instance-optimized\" systems: that is, systems that self-adjust to a given workload and data distribution to provide unprecedented performance without the need for tuning by an administrator. While many of these techniques promise orders-of-magnitude better performance in lab settings, there is still general skepticism about how practical the current techniques really are. The following is intended as a progress report on ML for Systems and its readiness for real-world deployments, with a focus on our projects done as part of the Data Systems and AI Lab (DSAIL) at MIT By no means is it a comprehensive overview of all existing work, which has been steadily growing over the past several years not only in the database community but also in the systems, networking, theory, PL, and many other adjacent communities.", "year": 2021, "venue": "", "authors": "Tim Kraska"}, {"title": "Discovering sparse interpretable dynamics from partial observations", "abstract": "Identifying the governing equations of a nonlinear dynamical system is key to both understanding the physical features of the system and constructing an accurate model of the dynamics that generalizes well beyond the available data. Achieving this kind of interpretable system identification is even more difficult for partially observed systems. We propose a machine learning framework for discovering the governing equations of a dynamical system using only partial observations, combining an encoder for state reconstruction with a sparse symbolic model. The entire architecture is trained end-to-end by matching the higher-order symbolic time derivatives of the sparse symbolic model with finite difference estimates from the data. Our tests show that this method can successfully reconstruct the full system state and identify the equations of motion governing the underlying dynamics for a variety of ordinary differential \u2026", "year": 2022, "venue": "", "authors": "Peter Y Lu and Joan Ari\u00f1o Bernad and Marin Solja\u010di\u0107"}, {"title": "Replacing neural networks by optimal analytical predictors for the detection of phase transitions", "abstract": "Identifying phase transitions and classifying phases of matter is central to understanding the properties and behavior of a broad range of material systems. In recent years, machine-learning (ML) techniques have been successfully applied to perform such tasks in a data-driven manner. However, the success of this approach notwithstanding, we still lack a clear understanding of ML methods for detecting phase transitions, particularly of those that utilize neural networks (NNs). In this work, we derive analytical expressions for the optimal output of three widely used NN-based methods for detecting phase transitions. These optimal predictions correspond to the results obtained in the limit of high model capacity. Therefore, in practice, they can, for example, be recovered using sufficiently large, well-trained NNs. The inner workings of the considered methods are revealed through the explicit dependence of the optimal \u2026", "year": 2022, "venue": "", "authors": "Julian Arnold and Frank Sch\u00e4fer"}, {"title": "Finding and optimizing certified, collision-free regions in configuration space for robot manipulators", "abstract": "Configuration space (C-space) has played a central role in collision-free motion planning, particularly for robot manipulators. While it is possible to check for collisions at a point using standard algorithms, to date no practical method exists for computing collision-free C-space regions with rigorous certificates due to the complexities of mapping task-space obstacles through the kinematics. In this work, we present the first to our knowledge method for generating such regions and certificates through convex optimization. Our method, called C-Iris (C-space Iterative Regional Inflation by Semidefinite programming), generates large, convex polytopes in a rational parametrization of the configuration space which are guaranteed to be collision-free. Such regions have been shown to be useful for both optimization-based and randomized motion planning. Our regions are generated by alternating between two convex \u2026", "year": 2022, "venue": "", "authors": "Alexandre Amice and Hongkai Dai and Peter Werner and Annan Zhang and Russ Tedrake"}, {"title": "Safediffuser: Safe planning with diffusion probabilistic models", "abstract": "Diffusion model-based approaches have shown promise in data-driven planning, but there are no safety guarantees, thus making it hard to be applied for safety-critical applications. To address these challenges, we propose a new method, called SafeDiffuser, to ensure diffusion probabilistic models satisfy specifications by using a class of control barrier functions. The key idea of our approach is to embed the proposed finite-time diffusion invariance into the denoising diffusion procedure, which enables trustworthy diffusion data generation. Moreover, we demonstrate that our finite-time diffusion invariance method through generative models not only maintains generalization performance but also creates robustness in safe data generation. We test our method on a series of safe planning tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, with results showing the advantages of robustness and guarantees over vanilla diffusion models.", "year": 2023, "venue": "", "authors": "Wei Xiao and Tsun-Hsuan Wang and Chuang Gan and Daniela Rus"}, {"title": "Occamnet: A fast neural model for symbolic regression at scale", "abstract": "Neural networks' expressiveness comes at the cost of complex, black-box models that often extrapolate poorly beyond the domain of the training dataset, conflicting with the goal of finding compact analytic expressions to describe scientific data. We introduce OccamNet, a neural network model that finds interpretable, compact, and sparse symbolic fits to data, \\`a la Occam's razor. Our model defines a probability distribution over functions with efficient sampling and function evaluation. We train by sampling functions and biasing the probability mass toward better fitting solutions, backpropagating using cross-entropy matching in a reinforcement-learning loss. OccamNet can identify symbolic fits for a variety of problems, including analytic and non-analytic functions, implicit functions, and simple image classification, and can outperform state-of-the-art symbolic regression methods on real-world regression datasets. Our method requires a minimal memory footprint, fits complicated functions in minutes on a single CPU, and scales on a GPU.", "year": 2020, "venue": "", "authors": "Owen Dugan and Rumen Dangovski and Allan Costa and Samuel Kim and Pawan Goyal and Joseph Jacobson and Marin Solja\u010di\u0107"}, {"title": "Modeldiff: A framework for comparing learning algorithms", "abstract": "We study the problem of (learning) algorithm comparison, where the goal is to find differences between models trained with two different learning algorithms. We begin by formalizing this goal as one of finding distinguishing feature transformations, ie, input transformations that change the predictions of models trained with one learning algorithm but not the other. We then present ModelDiff, a method that leverages the datamodels framework (Ilyas et al., 2022) to compare learning algorithms based on how they use their training data. We demonstrate ModelDiff through three case studies, comparing models trained with/without data augmentation, with/without pre-training, and with different SGD hyperparameters.", "year": 2023, "venue": "", "authors": "Harshay Shah and Sung Min Park and Andrew Ilyas and Aleksander Madry"}, {"title": "On the optimal time/space tradeoff for hash tables", "abstract": "For nearly six decades, the central open question in the study of hash tables has been to determine the optimal achievable tradeoff curve between time and space. State-of-the-art hash tables offer the following guarantee: If keys/values are \u0398(logn) bits each, then it is possible to achieve constant-time insertions/deletions/queries while wasting only O(loglogn) bits of space per key when compared to the information-theoretic optimum\u2014this bound has been proven to be optimal for a number of closely related problems (e.g., stable hashing, dynamic retrieval, and dynamically-resized filters). This paper shows that O(loglogn) wasted bits per key is not the end of the line for hashing. In fact, for any k \u2208 [log* n], it is possible to achieve O(k)-time insertions/deletions, O(1)-time queries, and the k-th iterated logarithm O(log(k) n) wasted bits per key (all with high probability in n), while also supporting dynamic resizing as the size \u2026", "year": 2022, "venue": "", "authors": "Michael A Bender and Mart\u00edn Farach-Colton and John Kuszmaul and William Kuszmaul and Mingmou Liu"}, {"title": "Missingness bias in model debugging", "abstract": "Missingness, or the absence of features from an input, is a concept fundamental to many model debugging tools. However, in computer vision, pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels, which may in turn introduce bias into the debugging process. We study such biases and, in particular, show how transformer-based architectures can enable a more natural implementation of missingness, which side-steps these issues and improves the reliability of model debugging in practice. Our code is available at https://github.com/madrylab/missingness", "year": 2022, "venue": "", "authors": "Saachi Jain and Hadi Salman and Eric Wong and Pengchuan Zhang and Vibhav Vineet and Sai Vemprala and Aleksander Madry"}, {"title": "Intuitively assessing ML model reliability through example-based explanations and editing model inputs", "abstract": " Interpretability methods aim to help users build trust in and understand the capabilities of machine learning models. However, existing approaches often rely on abstract, complex visualizations that poorly map to the task at hand or require non-trivial ML expertise to interpret. Here, we present two interface modules that facilitate intuitively assessing model reliability. To help users better characterize and reason about a model\u2019s uncertainty, we visualize raw and aggregate information about a given input\u2019s nearest neighbors. Using an interactive editor, users can manipulate this input in semantically-meaningful ways, determine the effect on the output, and compare against their prior expectations. We evaluate our approach using an electrocardiogram beat classification case study. Compared to a baseline feature importance interface, we find that 14 physicians are better able to align the model\u2019s uncertainty with \u2026", "year": 2022, "venue": "", "authors": "Harini Suresh and Kathleen M Lewis and John Guttag and Arvind Satyanarayan"}, {"title": "Neural regression, representational similarity, model zoology & neural taskonomy at scale in rodent visual cortex", "abstract": "How well do deep neural networks fare as models of mouse visual cortex? A majority of research to date suggests results far more mixed than those produced in the modeling of primate visual cortex. Here, we perform a large-scale benchmarking of dozens of deep neural network models in mouse visual cortex with both representational similarity analysis and neural regression. Using the Allen Brain Observatory's 2-photon calcium-imaging dataset of activity in over 6,000 reliable rodent visual cortical neurons recorded in response to natural scenes, we replicate previous findings and resolve previous discrepancies, ultimately demonstrating that modern neural networks can in fact be used to explain activity in the mouse visual cortex to a more reasonable degree than previously suggested. Using our benchmark as an atlas, we offer preliminary answers to overarching questions about levels of analysis (eg do models that better predict the representations of individual neurons also predict representational similarity across neural populations?); questions about the properties of models that best predict the visual system overall (eg is convolution or category-supervision necessary to better predict neural activity?); and questions about the mapping between biological and artificial representations (eg does the information processing hierarchy in deep nets match the anatomical hierarchy of mouse visual cortex?). Along the way, we catalogue a number of models (including vision transformers, MLP-Mixers, normalization free networks, Taskonomy encoders and self-supervised models) outside the traditional circuit of convolutional object recognition \u2026", "year": 2021, "venue": "", "authors": "Colin Conwell and David Mayo and Andrei Barbu and Michael Buice and George Alvarez and Boris Katz"}, {"title": "The neural MMO platform for massively multiagent research", "abstract": "Neural MMO is a computationally accessible research platform that combines large agent populations, long time horizons, open-ended tasks, and modular game systems. Existing environments feature subsets of these properties, but Neural MMO is the first to combine them all. We present Neural MMO as free and open source software with active support, ongoing development, documentation, and additional training, logging, and visualization tools to help users adapt to this new setting. Initial baselines on the platform demonstrate that agents trained in large populations explore more and learn a progression of skills. We raise other more difficult problems such as many-team cooperation as open research questions which Neural MMO is well-suited to answer. Finally, we discuss current limitations of the platform, potential mitigations, and plans for continued development.", "year": 2021, "venue": "", "authors": "Joseph Suarez and Yilun Du and Clare Zhu and Igor Mordatch and Phillip Isola"}, {"title": "A parallel packed memory array to store dynamic graphs", "abstract": "The ideal data structure for storing dynamic graphs would support fast updates as well as fast range queries which underlie graph traversals such as breadth-first search. The Packed Memory Array (PMA) seems like a good candidate for this setting because it supports fast updates as well as cache-efficient range queries. Concurrently updating a PMA raises challenges, however, because an update may require rewriting the entire structure.This paper introduces a parallel PMA with intra- and inter-operation parallelism and deadlock-free polylogarithmicspan operations. Our main observation is that the PMA is well-suited to concurrent updates despite occasionally requiring a rewrite of the entire structure because 1) most of the updates only write to a small part of the structure and 2) the worst case is highly parallel and cache-efficient.To evaluate our data structure, we implemented Parallel Packed Compressed \u2026", "year": 2021, "venue": "", "authors": "Brian Wheatman and Helen Xu"}, {"title": "Scalable multi-agent reinforcement learning through intelligent information aggregation", "abstract": "We consider the problem of multi-agent navigation and collision avoidance when observations are limited to the local neighborhood of each agent. We propose InforMARL, a novel architecture for multi-agent reinforcement learning (MARL) which uses local information intelligently to compute paths for all the agents in a decentralized manner. Specifically, InforMARL aggregates information about the local neighborhood of agents for both the actor and the critic using a graph neural network and can be used in conjunction with any standard MARL algorithm. We show that (1) in training, InforMARL has better sample efficiency and performance than baseline approaches, despite using less information, and (2) in testing, it scales well to environments with arbitrary numbers of agents and obstacles. We illustrate these results using four task environments, including one with predetermined goals for each agent, and one in which the agents collectively try to cover all goals.", "year": 2023, "venue": "", "authors": "Siddharth Nayak and Kenneth Choi and Wenqi Ding and Sydney Dolan and Karthik Gopalakrishnan and Hamsa Balakrishnan"}, {"title": "Treeline: an update-in-place key-value store for modern storage", "abstract": "Many modern key-value stores, such as RocksDB, rely on log-structured merge trees (LSMs). Originally designed for spinning disks, LSMs optimize for write performance by only making sequential writes. But this optimization comes at the cost of reads: LSMs must rely on expensive compaction jobs and Bloom filters---all to maintain reasonable read performance. For NVMe SSDs, we argue that trading off read performance for write performance is no longer always needed. With enough parallelism, NVMe SSDs have comparable random and sequential access performance. This change makes update-in-place designs, which traditionally provide excellent read performance, a viable alternative to LSMs. In this paper, we close the gap between log-structured and update-in-place designs on modern SSDs with the help of new components that take advantage of data and workload patterns. Specifically, we explore three key ideas: (A) record caching for efficient point operations, (B) page grouping for high-performance range scans, and (C) insert forecasting to reduce the reorganization costs of accommodating new records. We evaluate these ideas by implementing them in a prototype update-in-place key-value store called TreeLine. On YCSB, we find that TreeLine outperforms RocksDB and LeanStore by 2.20\u00d7 and 2.07\u00d7 respectively on average across the point workloads, and by up to 10.95\u00d7 and 7.52\u00d7 overall.", "year": 2022, "venue": "", "authors": "Geoffrey X Yu and Markos Markakis and Andreas Kipf and Per-\u00c5ke Larson and Umar Farooq Minhas and Tim Kraska"}, {"title": "Towards practical learned indexing", "abstract": "Latest research proposes to replace existing index structures with learned models. However, current learned indexes tend to have many hyperparameters, often do not provide any error guarantees, and are expensive to build. We introduce Practical Learned Index (PLEX). PLEX only has a single hyperparameter  (maximum prediction error) and offers a better trade-off between build and lookup time than state-of-the-art approaches. Similar to RadixSpline, PLEX consists of a spline and a (multi-level) radix layer. It first builds a spline satisfying the given  and then performs an ad-hoc analysis of the distribution of spline points to quickly tune the radix layer.", "year": 2021, "venue": "", "authors": "Mihail Stoian and Andreas Kipf and Ryan Marcus and Tim Kraska"}, {"title": "DICE: data discovery by example", "abstract": "In order to conduct analytical tasks, data scientists often need to find relevant data from an avalanche of sources (e.g., data lakes, large organizational databases). This effort is typically made in an ad hoc, non-systematic manner, which makes it a daunting endeavour. Current data discovery systems typically require the users to find relevant tables manually, usually by issuing multiple queries (e.g., using SQL). However, expressing such queries is nontrivial, as it requires knowledge of the underlying structure (schema) of the data organization in advance. This issue is further exacerbated when data resides in data lakes, where there is no predefined schema that data must conform to. On the other hand, data scientists can often come up with a few example records of interest quickly. Motivated by this observation, we developed DICE---a human-in-the-loop system for Data dIsCovery by Example---that takes user \u2026", "year": 2021, "venue": "", "authors": "El Kindi Rezig and Anshul Bhandari and Anna Fariha and Benjamin Price and Allan Vanterpool and Vijay Gadepally and Michael Stonebraker"}, {"title": "Dynamic time warping in strongly subquadratic time: Algorithms for the low-distance regime and approximate evaluation", "abstract": "Dynamic time warping distance (DTW) is a widely used distance measure between time series. The best known algorithms for computing DTW run in near quadratic time, and conditional lower bounds prohibit the existence of significantly faster algorithms. The lower bounds do not prevent a faster algorithm for the special case in which the DTW is small, however. For an arbitrary metric space  with distances normalized so that the smallest non-zero distance is one, we present an algorithm which computes  for two strings  and  over  in time . We also present an approximation algorithm which computes  within a factor of  in time  for . The algorithm allows for the strings  and  to be taken over an arbitrary well-separated tree metric with logarithmic depth and at most exponential aspect ratio. Extending our techniques further, we also obtain the first approximation algorithm for edit distance to work with characters taken from an arbitrary metric space, providing an -approximation in time , with high probability. Additionally, we present a simple reduction from computing edit distance to computing DTW. Applying our reduction to a conditional lower bound of Bringmann and K\\\"unnemann pertaining to edit distance over , we obtain a conditional lower bound for computing DTW over a three letter alphabet (with distances of zero and one). This improves on a previous result of Abboud, Backurs, and Williams. With a similar approach, we prove a reduction from computing edit distance to computing longest LCS length. This means that one can recover conditional lower bounds for LCS directly \u2026", "year": 2019, "venue": "", "authors": "William Kuszmaul"}, {"title": "Opencilk: A modular and extensible software infrastructure for fast task-parallel code", "abstract": "This paper presents OpenCilk, an open-source software infrastructure for task-parallel programming that allows for substantial code reuse and easy exploration of design choices in language abstraction, compilation strategy, runtime mechanism, and productivity-tool development.The OpenCilk infrastructure consists of three main components: a compiler designed to compile fork-join task-parallel code, an efficient work-stealing runtime scheduler, and a productivity-tool development framework based on compiler instrumentation designed for fork-join parallel computations. OpenCilk is modular --- modifying one component for the most part does not necessitate modifications to the other components --- and easy to extend --- its construction naturally encourages code reuse. Despite being modular and easy to extend, OpenCilk produces high-performing code.We investigated OpenCilk's modularity, extensibility, and \u2026", "year": 2023, "venue": "", "authors": "Tao B Schardl and I-Ting Angelina Lee"}, {"title": "Control barrier functions for systems with multiple control inputs", "abstract": "Control Barrier Functions (CBFs) are becoming popular tools in guaranteeing safety for nonlinear systems and constraints, and they can reduce a constrained optimal control problem into a sequence of Quadratic Programs (QPs) for affine control systems. The recently proposed High Order Control Barrier Functions (HOCBFs) work for arbitrary relative degree constraints. One of the challenges in a HOCBF is to address the relative degree problem when a system has multiple control inputs, i.e., the relative degree could be defined with respect to different components of the control vector. This paper proposes two methods for HOCBFs to deal with systems with multiple control inputs: a general integral control method and a method which is simpler but limited to specific classes of physical systems. When control bounds are involved, the feasibility of the above mentioned QPs can also be significantly improved with the \u2026", "year": 2022, "venue": "", "authors": "Wei Xiao and Christos G Cassandras and Calin A Belta and Daniela Rus"}, {"title": "On the convergence theory of debiased model-agnostic meta-reinforcement learning", "abstract": "We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement Learning (RL) problems, where the goal is to find a policy using data from several tasks represented by Markov Decision Processes (MDPs) that can be updated by one step of\\textit {stochastic} policy gradient for the realized MDP. In particular, using stochastic gradients in MAML update steps is crucial for RL problems since computation of exact gradients requires access to a large number of possible trajectories. For this formulation, we propose a variant of the MAML method, named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and study its convergence properties. We derive the iteration and sample complexity of SG-MRL to find an -first-order stationary point, which, to the best of our knowledge, provides the first convergence guarantee for model-agnostic meta-reinforcement learning algorithms. We further show how our results extend to the case where more than one step of stochastic policy gradient method is used at test time. Finally, we empirically compare SG-MRL and MAML in several deep RL environments.", "year": 2021, "venue": "", "authors": "Alireza Fallah and Kristian Georgiev and Aryan Mokhtari and Asuman Ozdaglar"}, {"title": "The World as a Graph: Improving El Ni\\~ no Forecasts with Graph Neural Networks", "abstract": "Deep learning-based models have recently outperformed state-of-the-art seasonal forecasting models, such as for predicting El Ni\\~no-Southern Oscillation (ENSO). However, current deep learning models are based on convolutional neural networks which are difficult to interpret and can fail to model large-scale atmospheric patterns. In comparison, graph neural networks (GNNs) are capable of modeling large-scale spatial dependencies and are more interpretable due to the explicit modeling of information flow through edge connections. We propose the first application of graph neural networks to seasonal forecasting. We design a novel graph connectivity learning module that enables our GNN model to learn large-scale spatial interactions jointly with the actual ENSO forecasting task. Our model, \\graphino, outperforms state-of-the-art deep learning-based models for forecasts up to six months ahead. Additionally, we show that our model is more interpretable as it learns sensible connectivity structures that correlate with the ENSO anomaly pattern.", "year": 2021, "venue": "", "authors": "Salva R\u00fchling Cachay and Emma Erickson and Arthur Fender C Bucker and Ernest Pokropek and Willa Potosnak and Suyash Bire and Salomey Osei and Bj\u00f6rn L\u00fctjens"}, {"title": "DBOS: A dbms-oriented operating system", "abstract": "This paper lays out the rationale for building a completely new operating system (OS) stack. Rather than build on a single node OS together with separate cluster schedulers, distributed filesystems, and network managers, we argue that a distributed transactional DBMS should be the basis for a scalable cluster OS. We show herein that such a database OS (DBOS) can do scheduling, file management, and inter-process communication with competitive performance to existing systems. In addition, significantly better analytics can be provided as well as a dramatic reduction in code complexity through implementing OS services as standard database queries, while implementing low-latency transactions and high availability only once.", "year": 2021, "venue": "", "authors": "Athinagoras Skiadopoulos and Qian Li and Peter Kraft and Kostis Kaffes and Daniel Hong and Shana Mathew and David Bestor and Michael Cafarella and Vijay Gadepally and Goetz Graefe and Jeremy Kepner and Christos Kozyrakis and Tim Kraska and Michael Stonebraker and Lalith Suresh and Matei Zaharia"}, {"title": "Harnessing mixed offline reinforcement learning datasets via trajectory weighting", "abstract": "Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any offline RL algorithm. We further analyze that the opportunity for performance improvement over the behavior policy correlates with the positive-sided variance of the returns of the trajectories in the dataset. We empirically show that while CQL, IQL, and TD3+BC achieve only a part of this potential policy improvement, these same algorithms combined with our reweighted sampling strategy fully exploit the dataset. Furthermore, we empirically demonstrate that, despite its theoretical limitation, the approach may still be efficient in stochastic environments. The code is available at https://github.com/Improbable-AI/harness-offline-rl.", "year": 2023, "venue": "", "authors": "Zhang-Wei Hong and Pulkit Agrawal and R\u00e9mi Tachet des Combes and Romain Laroche"}, {"title": "Scalable automatic differentiation of multiple parallel paradigms through compiler augmentation", "abstract": "Derivatives are key to numerous science, engineering, and machine learning applications. While existing tools generate derivatives of programs in a single language, modern parallel applications combine a set of frameworks and languages to leverage available performance and function in an evolving hardware landscape. We propose a scheme for differentiating arbitrary DAG-based parallelism that preserves scalability and efficiency, implemented into the LLVM-based Enzyme automatic differentiation framework. By integrating with a full-fledged compiler backend, Enzyme can differentiate numerous parallel frameworks and directly control code generation. Combined with its ability to differentiate any LLVM-based language, this flexibility permits Enzyme to leverage the compiler tool chain for parallel and differentiation-specitic optimizations. We differentiate nine distinct versions of the LULESH and miniBUDE \u2026", "year": 2022, "venue": "", "authors": "William S Moses and Sri Hari Krishna Narayanan and Ludger Paehler and Valentin Churavy and Michel Schanen and Jan H\u00fcckelheim and Johannes Doerfert and Paul Hovland"}, {"title": "SNARF: a learning-enhanced range filter", "abstract": "We present Sparse Numerical Array-Based Range Filters (SNARF), a learned range filter that efficiently supports range queries for numerical data. SNARF creates a model of the data distribution to map the keys into a bit array which is stored in a compressed form. The model along with the compressed bit array which constitutes SNARF are used to answer membership queries.We evaluate SNARF on multiple synthetic and real-world datasets as a stand-alone filter and by integrating it into RocksDB. For range queries, SNARF provides up to 50x better false positive rate than state-of-the-art range filters, such as SuRF and Rosetta, with the same space usage. We also evaluate SNARF in RocksDB as a filter replacement for filtering requests before they access on-disk data structures. For RocksDB, SNARF can improve the execution time of the system up to 10x compared to SuRF and Rosetta for certain read-only \u2026", "year": 2022, "venue": "", "authors": "Kapil Vaidya and Subarna Chatterjee and Eric Knorr and Michael Mitzenmacher and Stratos Idreos and Tim Kraska"}, {"title": "Tools and practices for responsible AI engineering", "abstract": "Responsible Artificial Intelligence (AI) - the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability - represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries - hydra-zen and the rAI-toolbox - that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs.", "year": 2022, "venue": "", "authors": "Ryan Soklaski and Justin Goodwin and Olivia Brown and Michael Yee and Jason Matterer"}, {"title": "Ribbon: cost-effective and qos-aware deep learning model inference using a diverse pool of cloud computing instances", "abstract": "Deep learning model inference is a key service in many businesses and scientific discovery processes. This paper introduces Ribbon, a novel deep learning inference serving system that meets two competing objectives: quality-of-service (QoS) target and cost-effectiveness. The key idea behind Ribbon is to intelligently employ a diverse set of cloud computing instances (heterogeneous instances) to meet the QoS target and maximize cost savings. Ribbon devises a Bayesian Optimization-driven strategy that helps users build the optimal set of heterogeneous instances for their model inference service needs on cloud computing platforms - and, Ribbon demonstrates its superiority over existing approaches of inference serving systems using homogeneous instance pools. Ribbon saves up to 16% of the inference service cost for different learning models including emerging deep learning recommender system \u2026", "year": 2021, "venue": "", "authors": "Baolin Li and Rohan Basu Roy and Tirthak Patel and Vijay Gadepally and Karen Gettings and Devesh Tiwari"}, {"title": "Closed-form continuous-depth models", "abstract": "", "year": 2021, "venue": "", "authors": "Ramin Hasani and Mathias Lechner and Alexander Amini and Lucas Liebenwein and Max Tschaikowski and Gerald Teschl and Daniela Rus"}, {"title": "Graph Neural Networks for Improved El Ni\\~ no Forecasting", "abstract": "Deep learning-based models have recently outperformed state-of-the-art seasonal forecasting models, such as for predicting El Ni\\~no-Southern Oscillation (ENSO). However, current deep learning models are based on convolutional neural networks which are difficult to interpret and can fail to model large-scale atmospheric patterns called teleconnections. Hence, we propose the application of spatiotemporal Graph Neural Networks (GNN) to forecast ENSO at long lead times, finer granularity and improved predictive skill than current state-of-the-art methods. The explicit modeling of information flow via edges may also allow for more interpretable forecasts. Preliminary results are promising and outperform state-of-the art systems for projections 1 and 3 months ahead.", "year": 2020, "venue": "", "authors": "Salva R\u00fchling Cachay and Emma Erickson and Arthur Fender C Bucker and Ernest Pokropek and Willa Potosnak and Salomey Osei and Bj\u00f6rn L\u00fctjens"}, {"title": "{UREQA}: Leveraging {Operation-Aware} error rates for effective quantum circuit mapping on {NISQ-Era} quantum computers", "abstract": "Noisy Intermediate-Scale Quantum (NISQ) computers are enabling development and evaluation of real quantum algorithms, but due to their highly erroneous nature, careful selection of qubits to map the algorithm on to real hardware is required to minimize the error rate of the algorithm output. In this paper, we propose UREQA, a new approach that introduces quantum-operation-aware error rate prediction to minimize of output errors of quantum algorithms running on NISQ devices.", "year": 2020, "venue": "", "authors": "Tirthak Patel and Baolin Li and Rohan Basu Roy and Devesh Tiwari"}, {"title": "Achieving optimal backlog in multi-processor cup games", "abstract": "Many problems in processor scheduling, deamortization, and buffer management can be modeled as single- and multi-processor cup games. At the beginning of the single-processor n-cup game, all cups are empty. In each step of the game, a filler distributes 1\u2212\u0454 units of water among the cups, and then an emptier selects a cup and removes up to 1 unit of water from it. The goal of the emptier is to minimize the amount of water in the fullest cup, also known as the backlog. The greedy algorithm (i.e., empty from the fullest cup) is known to achieve backlog O(logn), and no deterministic algorithm can do better. We show that the performance of the greedy algorithm can be exponentially improved with a small amount of randomization: After each step and for any k \u2265 \u03a9(log\u0454\u22121), the emptier achieves backlog at most O(k) with probability at least 1 \u2212O(2\u22122k). We call our algorithm the smoothed greedy algorithm because if \u2026", "year": 2019, "venue": "", "authors": "Michael A Bender and Mart\u00edn Farach-Colton and William Kuszmaul"}, {"title": "Mosaic pages: Big tlb reach with small pages", "abstract": "The TLB is increasingly a bottleneck for big data applications. In most designs, the number of TLB entries are highly constrained by latency requirements, and growing much more slowly than the working sets of applications. Many solutions to this problem, such as huge pages, perforated pages, or TLB coalescing, rely on physical contiguity for performance gains, yet the cost of defragmenting memory can easily nullify these gains. This paper introduces mosaic pages, which increase TLB reach by compressing multiple, discrete translations into one TLB entry.   Mosaic leverages virtual contiguity for locality, but does not use physical contiguity. Mosaic relies on recent advances in hashing theory to constrain memory mappings, in order to realize this physical address compression without reducing memory utilization or increasing swapping. This paper presents a full-system prototype of Mosaic, in gem5 and modified \u2026", "year": 2023, "venue": "", "authors": "Krishnan Gosakan and Jaehyun Han and William Kuszmaul and Ibrahim N Mubarek and Nirjhar Mukherjee and Karthik Sriram and Guido Tagliavini and Evan West and Michael A Bender and Abhishek Bhattacharjee and Alex Conway and Martin Farach-Colton and Jayneel Gandhi and Rob Johnson and Sudarsun Kannan and Donald E Porter"}, {"title": "Communication-efficient graph neural networks with probabilistic neighborhood expansion analysis and caching", "abstract": "Training and inference with graph neural networks (GNNs) on massive graphs in a distributed environment has been actively studied since the inception of GNNs, owing to the widespread use and success of GNNs in applications such as recommendation systems and financial forensics. This paper is concerned with minibatch training and inference with GNNs in distributed settings, where the necessary partitioning of vertex features across distributed storage causes feature communication to become a major bottleneck that hampers scalability. To significantly reduce the communication volume without compromising prediction accuracy, we propose a policy for caching data associated with frequently accessed vertices in remote partitions. The proposed policy is based on an analysis of vertex-wise inclusion probabilities (VIP) during multi-hop neighborhood sampling, which may expand the neighborhood far beyond the partition boundary of the graph. The VIP analysis not only enables the elimination of the communication bottleneck, but also offers a means to organize in-memory data by prioritizing GPU storage for the most frequently accessed vertex features. We present SALIENT++, which extends the prior state-of-the-art SALIENT system to work with partitioned feature data and leverages the VIP-driven caching policy. SALIENT++ retains the local training efficiency and scalability of SALIENT by using a deep pipeline and drastically reducing communication volume while consuming only a fraction of the storage required by SALIENT. We demonstrate experiments on the Open Graph Benchmark data sets and show that training a 3-layer \u2026", "year": 2023, "venue": "", "authors": "Tim Kaler and Alexandros Iliopoulos and Philip Murzynowski and Tao Schardl and Charles E Leiserson and Jie Chen"}, {"title": "Semantic uncertainty intervals for disentangled latent spaces.", "abstract": "Meaningful uncertainty quantification in computer vision requires reasoning about semantic information\u2014say, the hair color of the person in a photo or the location of a car on the street. To this end, recent breakthroughs in generative modeling allow us to represent semantic information in disentangled latent spaces, but providing uncertainties on the semantic latent variables has remained challenging. In this work, we provide principled uncertainty intervals that are guaranteed to contain the true semantic factors for any underlying generative model. The method does the following:(1) it uses quantile regression to output a heuristic uncertainty interval for each element in the latent space (2) calibrates these uncertainties such that they contain the true value of the latent for a new, unseen input. The endpoints of these calibrated intervals can then be propagated through the generator to produce interpretable uncertainty visualizations for each semantic factor. This technique reliably communicates semantically meaningful, principled, and instance-adaptive uncertainty in inverse problems like image super-resolution and image completion. Code and demos can be found on our project page.", "year": 2022, "venue": "", "authors": "Swami Sankaranarayanan and Anastasios Angelopoulos and Stephen Bates and Yaniv Romano and Phillip Isola"}, {"title": "Social interactions as recursive mdps", "abstract": "While machines and robots must interact with humans, providing them with social skills has been a largely overlooked topic. This is mostly a consequence of the fact that tasks such as navigation, command following, and even game playing are well-defined, while social reasoning still mostly remains a pre-theoretic problem. We demonstrate how social interactions can be effectively incorporated into MDPs by reasoning recursively about the goals of other agents. In essence, our method extends the reward function to include a combination of physical goals (something agents want to accomplish in the configuration space, a traditional MDP) and social goals (something agents want to accomplish relative to the goals of other agents). Our Social MDPs allow specifying reward functions in terms of the estimated reward functions of other agents, modeling interactions such as helping or hindering another agent (by maximizing or minimizing the other agent\u2019s reward) while balancing this with the actual physical goals of each agent. Our formulation allows for an arbitrary function of another agent\u2019s estimated reward structure and physical goals, enabling more complex behaviors such as politely hindering another agent or aggressively helping them. Extending Social MDPs in the same manner as I-POMDPs extension would enable interactions such as convincing another agent that something is true. To what extent the Social MDPs presented here and their potential Social POMDPs variant account for all possible social interactions is unknown, but having a precise mathematical model to guide questions about social interactions has both practical value \u2026", "year": 2022, "venue": "", "authors": "Ravi Tejwani and Yen-Ling Kuo and Tianmin Shu and Boris Katz and Andrei Barbu"}, {"title": "The mit supercloud dataset", "abstract": "Artificial intelligence (AI) and Machine learning (ML) workloads are an increasingly larger share of the compute workloads in traditional High-Performance Computing (HPC) centers and commercial cloud systems. This has led to changes in deployment approaches of HPC clusters and the commercial cloud, as well as a new focus on approaches to optimized resource usage, allocations and deployment of new AI frameworks, and capabilities such as Jupyter notebooks to enable rapid prototyping and deployment. With these changes, there is a need to better understand cluster/datacenter operations with the goal of developing improved scheduling policies, identifying inefficiencies in resource utilization, energy/power consumption, failure prediction, and identifying policy violations. In this paper we introduce the MIT Supercloud Dataset which aims to foster innovative AI/ML approaches to the analysis of large scale \u2026", "year": 2021, "venue": "", "authors": "Siddharth Samsi and Matthew L Weiss and David Bestor and Baolin Li and Michael Jones and Albert Reuther and Daniel Edelman and William Arcand and Chansup Byun and John Holodnack and Matthew Hubbell and Jeremy Kepner and Anna Klein and Joseph McDonald and Adam Michaleas and Peter Michaleas and Lauren Milechin and Julia Mullen and Charles Yee and Benjamin Price and Andrew Prout and Antonio Rosa and Allan Vanterpool and Lindsey McEvoy and Anson Cheng and Devesh Tiwari and Vijay Gadepally"}, {"title": "Predict then interpolate: A simple algorithm to learn stable classifiers", "abstract": "We propose Predict then Interpolate (PI), a simple algorithm for learning correlations that are stable across environments. The algorithm follows from the intuition that when using a classifier trained on one environment to make predictions on examples from another environment, its mistakes are informative as to which correlations are unstable. In this work, we prove that by interpolating the distributions of the correct predictions and the wrong predictions, we can uncover an oracle distribution where the unstable correlation vanishes. Since the oracle interpolation coefficients are not accessible, we use group distributionally robust optimization to minimize the worst-case risk across all such interpolations. We evaluate our method on both text classification and image classification. Empirical results demonstrate that our algorithm is able to learn robust classifiers (outperforms IRM by 23.85% on synthetic environments and 12.41% on natural environments). Our code and data are available at https://github. com/YujiaBao/Predict-then-Interpolate.", "year": 2021, "venue": "", "authors": "Yujia Bao and Shiyu Chang and Regina Barzilay"}, {"title": "Compositional networks enable systematic generalization for grounded language understanding", "abstract": "Humans are remarkably flexible when understanding new sentences that include combinations of concepts they have never encountered before. Recent work has shown that while deep networks can mimic some human language abilities when presented with novel sentences, systematic variation uncovers the limitations in the language-understanding abilities of networks. We demonstrate that these limitations can be overcome by addressing the generalization challenges in the gSCAN dataset, which explicitly measures how well an agent is able to interpret novel linguistic commands grounded in vision, e.g., novel pairings of adjectives and nouns. The key principle we employ is compositionality: that the compositional structure of networks should reflect the compositional structure of the problem domain they address, while allowing other parameters to be learned end-to-end. We build a general-purpose mechanism that enables agents to generalize their language understanding to compositional domains. Crucially, our network has the same state-of-the-art performance as prior work while generalizing its knowledge when prior work does not. Our network also provides a level of interpretability that enables users to inspect what each part of networks learns. Robust grounded language understanding without dramatic failures and without corner cases is critical to building safe and fair robots; we demonstrate the significant role that compositionality can play in achieving that goal.", "year": 2020, "venue": "", "authors": "Yen-Ling Kuo and Boris Katz and Andrei Barbu"}, {"title": "Catalyst: Fast and flexible modeling of reaction networks", "abstract": "We introduce Catalyst.jl, a flexible and feature-filled Julia library for modeling and high-performance simulation of chemical reaction networks (CRNs). Catalyst supports simulating stochastic chemical kinetics (jump process), chemical Langevin equation (stochastic differential equation), and reaction rate equation (ordinary differential equation) representations for CRNs. Through comprehensive benchmarks, we demonstrate that Catalyst simulation runtimes are often one to two orders of magnitude faster than other popular tools. More broadly, Catalyst acts as both a domain-specific language and an intermediate representation for symbolically encoding CRN models as Julia-native objects. This enables a pipeline of symbolically specifying, analyzing, and modifying CRNs; converting Catalyst models to symbolic representations of concrete mathematical models; and generating compiled code for numerical solvers. Leveraging ModelingToolkit.jl and Symbolics.jl, Catalyst models can be analyzed, simplified, and compiled into optimized representations for use in numerical solvers. Finally, we demonstrate Catalyst\u2019s broad extensibility and composability by highlighting how it can compose with a variety of Julia libraries, and how existing open-source biological modeling projects have extended its intermediate representation.", "year": 2023, "venue": "", "authors": "Torkel E Loman and Yingbo Ma and Vasily Ilin and Shashi Gowda and Niklas Korsbo and Nikhil Yewale and Chris Rackauckas and Samuel A Isaacson"}, {"title": "Characterizing multi-instance gpu for machine learning workloads", "abstract": "As machine learning (ML) becomes more and more popular, datacenter operators use hardware accelerators such as GPUs to tackle the high computation demand of ML workloads. However, recent studies show that user-submitted jobs often underutilize the GPU streaming multiprocessor (SM) cores, resulting in hardware resource wastage. Motivated by this observation, GPU vendors have released software and hardware support for GPU resource sharing, for example, the NVIDIA Multi-Instance GPU (MIG) technique on A100 Tensor Core GPUs. In this work, we use several state-of-the-art deep learning (DL) models from various application areas to characterize the performance and energy consumption of the A100 GPU MIG mode operation. Our characterization reveals valuable insights into operating a MIG-enabled GPU datacenter.", "year": 2022, "venue": "", "authors": "Baolin Li and Viiay Gadepally and Siddharth Samsi and Devesh Tiwari"}, {"title": "Bounding the last mile: Efficient learned string indexing", "abstract": "We introduce the RadixStringSpline (RSS) learned index structure for efficiently indexing strings. RSS is a tree of radix splines each indexing a fixed number of bytes. RSS approaches or exceeds the performance of traditional string indexes while using 7-70 less memory. RSS achieves this by using the minimal string prefix to sufficiently distinguish the data unlike most learned approaches which index the entire string. Additionally, the bounded-error nature of RSS accelerates the last mile search and also enables a memory-efficient hash-table lookup accelerator. We benchmark RSS on several real-world string datasets against ART and HOT. Our experiments suggest this line of research may be promising for future memory-intensive database applications.", "year": 2021, "venue": "", "authors": "Benjamin Spector and Andreas Kipf and Kapil Vaidya and Chi Wang and Umar Farooq Minhas and Tim Kraska"}, {"title": "Barriernet: A safety-guaranteed layer for neural networks", "abstract": "This paper introduces differentiable higher-order control barrier functions (CBF) that are end-to-end trainable together with learning systems. CBFs are usually overly conservative, while guaranteeing safety. Here, we address their conservativeness by softening their definitions using environmental dependencies without loosing safety guarantees, and embed them into differentiable quadratic programs. These novel safety layers, termed a BarrierNet, can be used in conjunction with any neural network-based controller, and can be trained by gradient descent. BarrierNet allows the safety constraints of a neural controller be adaptable to changing environments. We evaluate them on a series of control problems such as traffic merging and robot navigations in 2D and 3D space, and demonstrate their effectiveness compared to state-of-the-art approaches.", "year": 2021, "venue": "", "authors": "Wei Xiao and Ramin Hasani and Xiao Li and Daniela Rus"}, {"title": "Physically-consistent generative adversarial networks for coastal flood visualization", "abstract": "As climate change increases the intensity of natural disasters, society needs better tools for adaptation. Floods, for example, are the most frequent natural disaster, and better tools for flood risk communication could increase the support for floodresilient infrastructure development. Our work aims to enable more visual communication of large-scale climate impacts via visualizing the output of coastal flood models as satellite imagery. We propose the first deep learning pipeline to ensure physicalconsistency in synthetic visual satellite imagery. We advanced a state-of-the-art GAN called pix2pixHD, such that it produces imagery that is physically-consistent with the output of an expertvalidated storm surge model (NOAA SLOSH). By evaluating the imagery relative to physics-based flood maps, we find that our proposed framework outperforms baseline models in both physical-consistency and photorealism. We envision our work to be the first step towards a global visualization of how the climate challenge will shape our landscape. Continuing on this path, we show that the proposed pipeline generalizes to visualize reforestation. We also publish a dataset of over 25k labelled image-triplets to study image-to-image translation in Earth observation 1.", "year": 2021, "venue": "", "authors": "Bj\u00f6rn L\u00fctjens and Brandon Leshchinskiy and Christian Requena-Mesa and Farrukh Chishtie and Natalia D\u00edaz-Rodr\u00edguez and Oc\u00e9ane Boulais and Aruna Sankaranarayanan and Aaron Pi\u00f1a and Yarin Gal and Chedy Ra\u00efssi and Alexander Lavin and Dava Newman"}, {"title": "Classifying anomalies for network security", "abstract": "Detecting and classifying anomalous behaviors in computer networks remains a formidable challenge. This work outlines a machine learning technique that uses deep neural networks to detect and classify a variety of network attacks. Our approach is based on that hypothesis that different network attacks generate a distinguishable change in entropy of certain network flow features. To generate a training and validation dataset, we inject synthetic attacks of different types and intensities into raw packet capture data collected from an internet backbone link by the MAWI group. Experimental results show that our machine learning classification model can achieve high accuracy for network attacks in which attack intensities are as low as 5% of overall traffic.", "year": 2020, "venue": "", "authors": "Emily H Do and Vijay N Gadepally"}, {"title": "Llm inference serving: Survey of recent advances and opportunities", "abstract": "This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.", "year": 2024, "venue": "", "authors": "Baolin Li and Yankai Jiang and Vijay Gadepally and Devesh Tiwari"}, {"title": "Discovering conservation laws using optimal transport and manifold learning", "abstract": "Conservation laws are key theoretical and practical tools for understanding, characterizing, and modeling nonlinear dynamical systems. However, for many complex systems, the corresponding conserved quantities are difficult to identify, making it hard to analyze their dynamics and build stable predictive models. Current approaches for discovering conservation laws often depend on detailed dynamical information or rely on black box parametric deep learning methods. We instead reformulate this task as a manifold learning problem and propose a non-parametric approach for discovering conserved quantities. We test this new approach on a variety of physical systems and demonstrate that our method is able to both identify the number of conserved quantities and extract their values. Using tools from optimal transport theory and manifold learning, our proposed method provides a direct geometric approach to \u2026", "year": 2023, "venue": "", "authors": "Peter Y Lu and Rumen Dangovski and Marin Solja\u010di\u0107"}, {"title": "Lsched: A workload-aware learned query scheduler for analytical database systems", "abstract": "Query scheduling is a crucial task for analytical database systems that can greatly affect the query latency. However, existing scheduling approaches are based on heuristics and not optimal. A recent trial proposed to use reinforcement learning for automatically learning end-to-end scheduling policies. However, such trial was not capable of considering the database-specific characteristics (e.g., operator types, pipelining), and hence becomes not efficient for analytical database systems. In this paper, we try to fill this gap and introduce LSched (Learned Scheduler), a fully learned workload-aware query scheduler for in-memory analytical database systems. LSched provides an efficient inter-query and intra-query scheduling for dynamic analytical workloads (i.e., different queries can arrive/depart at any time). We integrated LSched with an efficient in-memory analytical database system, and evaluated it with TPCH \u2026", "year": 2022, "venue": "", "authors": "Ibrahim Sabek and Tenzin Samten Ukyab and Tim Kraska"}, {"title": "Machine learning-enhanced magnetic calibration for airborne magnetic anomaly navigation", "abstract": "View Video Presentation: https://doi.org/10.2514/6.2022-1760.vidUsing the earth's magnetic field for navigation of aircraft has shown promise as a viable alternative to the Global Positioning System (GPS) and other navigation systems. An airborne magnetic navigation system collects real-time magnetic field data and uses predetermined magnetic maps of the earth to estimate location by aiding an inertial navigation system (INS), which would otherwise drift. Magnetic navigation has the benefits of being passive, globally available at all times and in all weather, and not reliant on sight of land or stars. Since the magnetic field strength of a dipole decreases with the inverse cube of distance, magnetic navigation is also nearly unjammable. A corrupting magnetic source would have to be near an aircraft to be effective. However, the magnetic components on the aircraft itself can interfere with the desired magnetic \u2026", "year": 2022, "venue": "", "authors": "Albert Gnadt"}, {"title": "Deep learning for bayesian optimization of scientific problems with high-dimensional structure", "abstract": "Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely a black-box. The data may have some known structure (e.g. symmetries) and/or the data generation process may be a composite process that yields useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and do not easily accommodate known structure. Instead, we use Bayesian neural networks, a class of scalable and flexible surrogate models with inductive biases, to extend BO to complex, structured problems with high dimensionality. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that neural networks often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost.", "year": 2021, "venue": "", "authors": "Samuel Kim and Peter Y Lu and Charlotte Loh and Jamie Smith and Jasper Snoek and Marin Solja\u010di\u0107"}, {"title": "Improving interpretability for cyber vulnerability assessment using focus and context visualizations", "abstract": "Risk scoring provides a simple and quantifiable metric for decision support in cyber security operations, including prioritizing how to address discovered software vulnerabilities. However, scoring systems are often opaque to operators, which makes scores difficult to interpret in the context of their own networks, each other, or in a broader threat landscape. This interpretability challenge is exacerbated by recent applications of artificial intelligence (AI) and machine learning (ML) for vulnerability assessment, where opaque machine reasoning can hinder domain experts' trust in the decision-support toolkit or the actionability of its outputs. In this paper, we address this challenge through a combination of visualizations and analytics that complement existing techniques for vulnerability assessment. We present a study toward designing more interpretable visual encodings for decision support for vulnerability assessment \u2026", "year": 2020, "venue": "", "authors": "Kenneth B Alperin and Allan B Wollaber and Steven R Gomez"}, {"title": "Sustainable supercomputing for ai: Gpu power capping at hpc scale", "abstract": "As research and deployment of AI grows, the computational burden to support and sustain its progress inevitably does too. To train or fine-tune state-of-the-art models in NLP, computer vision, etc., some form of AI hardware acceleration is virtually a requirement. Recent large language models require considerable resources to train and deploy, resulting in significant energy usage, potential carbon emissions, and massive demand for GPUs and other hardware accelerators. However, this surge carries large implications for energy sustainability at the HPC/datacenter level. In this paper, we study the effects of power-capping GPUs at a research supercomputing center on GPU temperature and power draw; we show significant decreases in both temperature and power draw, reducing power consumption and potentially improving hardware life-span, with minimal impact on job performance. To our knowledge, our \u2026", "year": 2023, "venue": "", "authors": "Dan Zhao and Siddharth Samsi and Joseph McDonald and Baolin Li and David Bestor and Michael Jones and Devesh Tiwari and Vijay Gadepally"}, {"title": "Trajectory prediction with linguistic representations", "abstract": "Language allows humans to build mental models that interpret what is happening around them resulting in more accurate long-term predictions. We present a novel trajectory prediction model that uses linguistic intermediate representations to forecast trajectories, and is trained using trajectory samples with partially-annotated captions. The model learns the meaning of each of the words without direct per-word supervision. At inference time, it generates a linguistic description of trajectories which captures maneuvers and interactions over an extended time interval. This generated description is used to refine predictions of the trajectories of multiple agents. We train and validate our model on the Argoverse dataset, and demonstrate improved accuracy results in trajectory prediction. In addition, our model is more interpretable: it presents part of its reasoning in plain language as captions, which can aid model \u2026", "year": 2022, "venue": "", "authors": "Yen-Ling Kuo and Xin Huang and Andrei Barbu and Stephen G McGill and Boris Katz and John J Leonard and Guy Rosman"}, {"title": "Learning to split for automatic bias detection", "abstract": "Classifiers are biased when trained on biased datasets. As a remedy, we propose Learning to Split (ls), an algorithm for automatic bias detection. Given a dataset with input-label pairs, ls learns to split this dataset so that predictors trained on the training split cannot generalize to the testing split. This performance gap suggests that the testing split is under-represented in the dataset, which is a signal of potential bias. Identifying non-generalizable splits is challenging since we have no annotations about the bias. In this work, we show that the prediction correctness of each example in the testing split can be used as a source of weak supervision: generalization performance will drop if we move examples that are predicted correctly away from the testing split, leaving only those that are mis-predicted. ls is task-agnostic and can be applied to any supervised learning problem, ranging from natural language understanding and image classification to molecular property prediction. Empirical results show that ls is able to generate astonishingly challenging splits that correlate with human-identified biases. Moreover, we demonstrate that combining robust learning algorithms (such as group DRO) with splits identified by ls enables automatic de-biasing. Compared to previous state-of-the-art, we substantially improve the worst-group performance (23.4% on average) when the source of biases is unknown during training and validation.", "year": 2022, "venue": "", "authors": "Yujia Bao and Regina Barzilay"}, {"title": "Certified polyhedral decompositions of collision-free configuration space", "abstract": "Understanding the geometry of collision-free configuration space (C-free) in the presence of Cartesian-space obstacles is an essential ingredient for collision-free motion planning. While it is possible to check for collisions at a point using standard algorithms, to date no practical method exists for computing C-free regions with rigorous certificates due to the complexity of mapping Cartesian-space obstacles through the kinematics. In this work, we present the first to our knowledge rigorous method for approximately decomposing a rational parametrization of C-free into certified polyhedral regions. Our method, called C-Iris (C-space Iterative Regional Inflation by Semidefinite programming), generates large, convex polytopes in a rational parameterization of the configuration space which are rigorously certified to be collision-free. Such regions have been shown to be useful for both optimization-based and randomized \u2026", "year": 2024, "venue": "", "authors": "Hongkai Dai and Alexandre Amice and Peter Werner and Annan Zhang and Russ Tedrake"}, {"title": "Iceberg hashing: Optimizing many hash-table criteria at once", "abstract": "Despite being one of the oldest data structures in computer science, hash tables continue to be the focus of a great deal of both theoretical and empirical research. A central reason for this is that many of the fundamental properties that one desires from a hash table are difficult to achieve simultaneously; thus many variants offering different trade-offs have been proposed.This article introduces Iceberg hashing, a hash table that simultaneously offers the strongest known guarantees on a large number of core properties. Iceberg hashing supports constant-time operations while improving on the state of the art for space efficiency, cache efficiency, and low failure probability. Iceberg hashing is also the first hash table to support a load factor of up to 1 - o(1) while being stable, meaning that the position where an element is stored only ever changes when resizes occur. In fact, in the setting where keys are \u0398 (log n) bits, the \u2026", "year": 2023, "venue": "", "authors": "Michael A Bender and Alex Conway and Mart\u00edn Farach-Colton and William Kuszmaul and Guido Tagliavini"}, {"title": "Deep learning and symbolic regression for discovering parametric equations", "abstract": "Symbolic regression is a machine learning technique that can learn the equations governing data and thus has the potential to transform scientific discovery. However, symbolic regression is still limited in the complexity and dimensionality of the systems that it can analyze. Deep learning, on the other hand, has transformed machine learning in its ability to analyze extremely complex and high-dimensional datasets. We propose a neural network architecture to extend symbolic regression to parametric systems where some coefficient may vary, but the structure of the underlying governing equation remains constant. We demonstrate our method on various analytic expressions and partial differential equations (PDEs) with varying coefficients and show that it extrapolates well outside of the training domain. The proposed neural-network-based architecture can also be enhanced by integrating with other deep learning \u2026", "year": 2023, "venue": "", "authors": "Michael Zhang and Samuel Kim and Peter Y Lu and Marin Solja\u010di\u0107"}, {"title": "On balancing bias and variance in unsupervised multi-source-free domain adaptation", "abstract": "Due to privacy, storage, and other constraints, there is a growing need for unsupervised domain adaptation techniques in machine learning that do not require access to the data used to train a collection of source models. Existing methods for multi-source-free domain adaptation (MSFDA) typically train a target model using pseudo-labeled data produced by the source models, which focus on improving the pseudo-labeling techniques or proposing new training objectives. Instead, we aim to analyze the fundamental limits of MSFDA. In particular, we develop an information-theoretic bound on the generalization error of the resulting target model, which illustrates an inherent bias-variance trade-off. We then provide insights on how to balance this trade-off from three perspectives, including domain aggregation, selective pseudo-labeling, and joint feature alignment, which leads to the design of novel algorithms. Experiments on multiple datasets validate our theoretical analysis and demonstrate the state-of-art performance of the proposed algorithm, especially on some of the most challenging datasets, including Office-Home and DomainNet.", "year": 2023, "venue": "", "authors": "Maohao Shen and Yuheng Bu and Gregory W Wornell"}, {"title": "Learning to extrapolate: A transductive approach", "abstract": "Machine learning systems, especially with overparameterized deep neural networks, can generalize to novel test instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support test points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparameterized function approximators while enabling extrapolation to out-of-support test points when possible. This is accomplished by noting that under certain conditions, a \"transductive\" reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem under certain conditions. We instantiate a simple, practical algorithm applicable to various supervised learning and imitation learning tasks.", "year": 2023, "venue": "", "authors": "Aviv Netanyahu and Abhishek Gupta and Max Simchowitz and Kaiqing Zhang and Pulkit Agrawal"}, {"title": "High-performance gpu-to-cpu transpilation and optimization via high-level parallel constructs", "abstract": "While parallelism remains the main source of performance, architectural implementations and programming models change with each new hardware generation, often leading to costly application re-engineering. Most tools for performance portability require manual and costly application porting to yet another programming model.We propose an alternative approach that automatically translates programs written in one programming model (CUDA), into another (CPU threads) based on Polygeist/MLIR. Our approach includes a representation of parallel constructs that allows conventional compiler transformations to apply transparently and without modification and enables parallelism-specific optimizations. We evaluate our framework by transpiling and optimizing the CUDA Rodinia benchmark suite for a multi-core CPU and achieve a 58% geomean speedup over handwritten OpenMP code. Further, we show how \u2026", "year": 2023, "venue": "", "authors": "William S Moses and Ivan R Ivanov and Jens Domke and Toshio Endo and Johannes Doerfert and Oleksandr Zinenko"}, {"title": "Zero botnets: An observe-pursue-counter approach", "abstract": "Adversarial Internet robots (botnets) represent a growing threat to the safe use and stability of the Internet. Botnets can play a role in launching adversary reconnaissance (scanning and phishing), influence operations (upvoting), and financing operations (ransomware, market manipulation, denial of service, spamming, and ad click fraud) while obfuscating tailored tactical operations. Reducing the presence of botnets on the Internet, with the aspirational target of zero, is a powerful vision for galvanizing policy action. Setting a global goal, encouraging international cooperation, creating incentives for improving networks, and supporting entities for botnet takedowns are among several policies that could advance this goal. These policies raise significant questions regarding proper authorities/access that cannot be answered in the abstract. Systems analysis has been widely used in other domains to achieve sufficient detail to enable these questions to be dealt with in concrete terms. Defeating botnets using an observe-pursue-counter architecture is analyzed, the technical feasibility is affirmed, and the authorities/access questions are significantly narrowed. Recommended next steps include: supporting the international botnet takedown community, expanding network observatories, enhancing the underlying network science at scale, conducting detailed systems analysis, and developing appropriate policy frameworks.", "year": 2022, "venue": "", "authors": "Jeremy Kepner and Jonathan Bernays and Stephen Buckley and Kenjiro Cho and Cary Conrad and Leslie Daigle and Keeley Erhardt and Vijay Gadepally and Barry Greene and Michael Jones and Robert Knake and Bruce Maggs and Peter Michaleas and Chad Meiners and Andrew Morris and Alex Pentland and Sandeep Pisharody and Sarah Powazek and Andrew Prout and Philip Reiner and Koichi Suzuki and Kenji Takahashi and Tony Tauber and Leah Walker and Douglas Stetson"}, {"title": "Pce-pinns: Physics-informed neural networks for uncertainty propagation in ocean modeling", "abstract": "Climate models project an uncertainty range of possible warming scenarios from 1.5 to 5 degree Celsius global temperature increase until 2100, according to the CMIP6 model ensemble. Climate risk management and infrastructure adaptation requires the accurate quantification of the uncertainties at the local level. Ensembles of high-resolution climate models could accurately quantify the uncertainties, but most physics-based climate models are computationally too expensive to run as ensemble. Recent works in physics-informed neural networks (PINNs) have combined deep learning and the physical sciences to learn up to 15k faster copies of climate submodels. However, the application of PINNs in climate modeling has so far been mostly limited to deterministic models. We leverage a novel method that combines polynomial chaos expansion (PCE), a classic technique for uncertainty propagation, with PINNs. The PCE-PINNs learn a fast surrogate model that is demonstrated for uncertainty propagation of known parameter uncertainties. We showcase the effectiveness in ocean modeling by using the local advection-diffusion equation.", "year": 2021, "venue": "", "authors": "Bj\u00f6rn L\u00fctjens and Catherine H Crawford and Mark Veillette and Dava Newman"}, {"title": "Contention resolution without collision detection", "abstract": "This paper focuses on the contention resolution problem on a shared communication channel that does not support collision detection. A shared communication channel is a multiple access channel, which consists of a sequence of synchronized time slots. Players on the channel may attempt to broadcast a packet (message) in any time slot. A player's broadcast succeeds if no other player broadcasts during that slot. If two or more players broadcast in the same time slot, then the broadcasts collide and both broadcasts fail. The lack of collision detection means that a player monitoring the channel cannot differentiate between the case of two or more players broadcasting in the same slot (a collision) and zero players broadcasting. In the contention-resolution problem, players arrive on the channel over time, and each player has one packet to transmit. The goal is to coordinate the players so that each player is able to \u2026", "year": 2020, "venue": "", "authors": "Michael A Bender and Tsvi Kopelowitz and William Kuszmaul and Seth Pettie"}, {"title": "Indoor and outdoor 3d scene graph generation via language-enabled spatial ontologies", "abstract": "This letter proposes an approach to build 3D scene graphs in arbitrary indoor and outdoor environments. Such extension is challenging; the hierarchy of concepts that describe an outdoor environment is more complex than for indoors, and manually defining such hierarchy is time-consuming and does not scale. Furthermore, the lack of training data prevents the straightforward application of learning-based tools used in indoor settings. To address these challenges, we propose two novel extensions. First, we develop methods to build a spatial ontology defining concepts and relations relevant for indoor and outdoor robot operation. In particular, we use a Large Language Model (LLM) to build such an ontology, thus largely reducing the amount of manual effort required. Second, we leverage the spatial ontology for 3D scene graph construction using Logic Tensor Networks (LTN) to add logical rules, or axioms (e.g., \u201ca \u2026", "year": 2024, "venue": "", "authors": "Jared Strader and Nathan Hughes and William Chen and Alberto Speranzon and Luca Carlone"}, {"title": "Topogivity: A machine-learned chemical rule for discovering topological materials", "abstract": "Topological materials present unconventional electronic properties that make them attractive for both basic science and next-generation technological applications. The majority of currently known topological materials have been discovered using methods that involve symmetry-based analysis of the quantum wave function. Here we use machine learning to develop a simple-to-use heuristic chemical rule that diagnoses with a high accuracy whether a material is topological using only its chemical formula. This heuristic rule is based on a notion that we term topogivity, a machine-learned numerical value for each element that loosely captures its tendency to form topological materials. We next implement a high-throughput procedure for discovering topological materials based on the heuristic topogivity-rule prediction followed by ab initio validation. This way, we discover new topological materials that are not \u2026", "year": 2023, "venue": "", "authors": "Andrew Ma and Yang Zhang and Thomas Christensen and Hoi Chun Po and Li Jing and Liang Fu and Marin Soljacic"}, {"title": "Differentiable control barrier functions for vision-based end-to-end autonomous driving", "abstract": "Guaranteeing safety of perception-based learning systems is challenging due to the absence of ground-truth state information unlike in state-aware control scenarios. In this paper, we introduce a safety guaranteed learning framework for vision-based end-to-end autonomous driving. To this end, we design a learning system equipped with differentiable control barrier functions (dCBFs) that is trained end-to-end by gradient descent. Our models are composed of conventional neural network architectures and dCBFs. They are interpretable at scale, achieve great test performance under limited training data, and are safety guaranteed in a series of autonomous driving scenarios such as lane keeping and obstacle avoidance. We evaluated our framework in a sim-to-real environment, and tested on a real autonomous car, achieving safe lane following and obstacle avoidance via Augmented Reality (AR) and real parked vehicles.", "year": 2022, "venue": "", "authors": "Wei Xiao and Tsun-Hsuan Wang and Makram Chahine and Alexander Amini and Ramin Hasani and Daniela Rus"}, {"title": "Physics-informed GANs for coastal flood visualization", "abstract": "As climate change increases the intensity of natural disasters, society needs better tools for adaptation. Floods, for example, are the most frequent natural disaster, but during hurricanes the area is largely covered by clouds and emergency managers must rely on nonintuitive flood visualizations for mission planning. To assist these emergency managers, we have created a deep learning pipeline that generates visual satellite images of current and future coastal flooding. We advanced a state-of-the-art GAN called pix2pixHD, such that it produces imagery that is physically-consistent with the output of an expert-validated storm surge model (NOAA SLOSH). By evaluating the imagery relative to physics-based flood maps, we find that our proposed framework outperforms baseline models in both physical-consistency and photorealism. While this work focused on the visualization of coastal floods, we envision the creation of a global visualization of how climate change will shape our earth.", "year": 2020, "venue": "", "authors": "Bj\u00f6rn L\u00fctjens and Brandon Leshchinskiy and Christian Requena-Mesa and Farrukh Chishtie and Natalia D\u00edaz-Rodriguez and Oc\u00e9ane Boulais and Aaron Pi\u00f1a and Dava Newman and Alexander Lavin and Yarin Gal and Chedy Ra\u00efssi"}, {"title": "Approximating robot configuration spaces with few convex sets using clique covers of visibility graphs", "abstract": "Many computations in robotics can be dramatically accelerated if the robot configuration space is described as a collection of simple sets. For example, recently developed motion planners rely on a convex decomposition of the free space to design collision-free trajectories using fast convex optimization. In this work, we present an efficient method for approximately covering complex configuration spaces with a small number of polytopes. The approach constructs a visibility graph using sampling and generates a clique cover of this graph to find clusters of samples that have mutual line of sight. These clusters are then inflated into large, full-dimensional, polytopes. We evaluate our method on a variety of robotic systems and show that it consistently covers larger portions of free configuration space, with fewer polytopes, and in a fraction of the time compared to previous methods.", "year": 2024, "venue": "", "authors": "Peter Werner and Alexandre Amice and Tobia Marcucci and Daniela Rus and Russ Tedrake"}, {"title": "Toward sustainable genai using generation directives for carbon-friendly large language model inference", "abstract": "The rapid advancement of Generative Artificial Intelligence (GenAI) across diverse sectors raises significant environmental concerns, notably the carbon emissions from their cloud and high performance computing (HPC) infrastructure. This paper presents Sprout, an innovative framework designed to address these concerns by reducing the carbon footprint of generative Large Language Model (LLM) inference services. Sprout leverages the innovative concept of \"generation directives\" to guide the autoregressive generation process, thereby enhancing carbon efficiency. Our proposed method meticulously balances the need for ecological sustainability with the demand for high-quality generation outcomes. Employing a directive optimizer for the strategic assignment of generation directives to user prompts and an original offline quality evaluator, Sprout demonstrates a significant reduction in carbon emissions by over 40% in real-world evaluations using the Llama2 LLM and global electricity grid data. This research marks a critical step toward aligning AI technology with sustainable practices, highlighting the potential for mitigating environmental impacts in the rapidly expanding domain of generative artificial intelligence.", "year": 2024, "venue": "", "authors": "Baolin Li and Yankai Jiang and Vijay Gadepally and Devesh Tiwari"}, {"title": "Automated translation and accelerated solving of differential equations on multiple GPU platforms", "abstract": "We demonstrate a high-performance vendor-agnostic method for massively parallel solving of ensembles of ordinary differential equations (ODEs) and stochastic differential equations (SDEs) on GPUs. The method is integrated with a widely used differential equation solver library in a high-level language (Julia\u2019s DifferentialEquations. jl) and enables GPU acceleration without requiring code changes by the user. Our approach achieves state-of-the-art performance compared to hand-optimized CUDA-C++ kernels while performing 20\u2013100\u00d7 faster than the vectorizing map (vmap) approach implemented in JAX and PyTorch. Performance evaluation on NVIDIA, AMD, Intel, and Apple GPUs demonstrates performance portability and vendor-agnosticism. We show composability with MPI to enable distributed multi-GPU workflows. The implemented solvers are fully featured\u2013supporting event handling, automatic \u2026", "year": 2024, "venue": "", "authors": "Utkarsh Utkarsh and Valentin Churavy and Yingbo Ma and Tim Besard and Prakitr Srisuma and Tim Gymnich and Adam R Gerlach and Alan Edelman and George Barbastathis and Richard D Braatz and Christopher Rackauckas"}, {"title": "How hard are computer vision datasets? calibrating dataset difficulty to viewing time", "abstract": "Humans outperform object recognizers despite the fact that models perform well on current datasets, including those explicitly designed to challenge machines with debiased images or distribution shift. This problem persists, in part, because we have no guidance on the absolute difficulty of an image or dataset making it hard to objectively assess progress toward human-level performance, to cover the range of human abilities, and to increase the challenge posed by a dataset. We develop a dataset difficulty metric MVT, Minimum Viewing Time, that addresses these three problems. Subjects view an image that flashes on screen and then classify the object in the image. Images that require brief flashes to recognize are easy, those which require seconds of viewing are hard. We compute the ImageNet and ObjectNet image difficulty distribution, which we find significantly undersamples hard images. Nearly 90% of current benchmark performance is derived from images that are easy for humans. Rather than hoping that we will make harder datasets, we can for the first time objectively guide dataset difficulty during development. We can also subset recognition performance as a function of difficulty: model performance drops precipitously while human performance remains stable. Difficulty provides a new lens through which to view model performance, one which uncovers new scaling laws: vision-language models stand out as being the most robust and human-like while all other techniques scale poorly. We release tools to automatically compute MVT, along with image sets which are tagged by difficulty. Objective image difficulty has practical \u2026", "year": 2023, "venue": "", "authors": "David Mayo and Jesse Cummings and Xinyu Lin and Dan Gutfreund and Boris Katz and Andrei Barbu"}, {"title": "Distributionally adaptive meta reinforcement learning", "abstract": "Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirically show its efficacy on simulated robotics problems under a wide range of distribution shifts.", "year": 2022, "venue": "", "authors": "Anurag Ajay and Abhishek Gupta and Dibya Ghosh and Sergey Levine and Pulkit Agrawal"}, {"title": "All-purpose hashing", "abstract": "Despite being one of the oldest data structures in computer science, hash tables continue to be the focus of a great deal of both theoretical and empirical research. A central reason for this is that many of the fundamental properties that one desires from a hash table are difficult to achieve simultaneously; thus many variants offering different trade-offs have been proposed. This paper introduces Iceberg hashing, a hash table that simultaneously offers the strongest known guarantees on a large number of core properties. Iceberg hashing supports constant-time operations while improving on the state of the art for space efficiency, cache efficiency, and low failure probability. Iceberg hashing is also the first hash table to support a load factor of up to 1\u2212 o (1) while being stable, meaning that the position where an element is stored only ever changes when resizes occur. In fact, in the setting where keys are \u0398 (log n) bits \u2026", "year": 2021, "venue": "", "authors": "Michael A Bender and Alex Conway and Mart\u00edn Farach-Colton and William Kuszmaul and Guido Tagliavini"}, {"title": "We Can Explain Your Research in Layman's Terms: Towards Automating Science Journalism at Scale", "abstract": "We propose to study Automating Science Journalism (ASJ), the process of producing a layman's terms summary of a research article, as a new benchmark for long neural abstractive summarization and story generation. Automating science journalism is a challenging task as it requires paraphrasing complex scientific concepts to be grasped by the general public. Thus, we create a specialized dataset that contains scientific papers and their Science Daily press releases. We demonstrate numerous sequence to sequence (seq2seq) applications using Science Daily with the aim of facilitating further research on language generation, which requires extreme paraphrasing and coping with long research articles. We further improve the quality of the press releases using co-training with scientific abstracts of sources or partitioned press releases. Finally, we apply evaluation measures beyond ROUGE and we demonstrate improved performance for our method over strong baselines, which we further confirm by quantitative and qualitative evaluation.", "year": 2021, "venue": "", "authors": "Rumen Dangovski and Michelle Shen and Dawson Byrd and Li Jing and Desislava Tsvetkova and Preslav Nakov and Marin Solja\u010di\u0107"}, {"title": "Achieving optimal backlog in the vanilla multi-processor cup game", "abstract": "In each step of the p-processor cup game on n cups, a filler distributes up to p units of water among the cups, subject only to the constraint that no cup receives more than 1 unit of water; an emptier then removes up to 1 unit of water from each of p cups. Designing strategies for the emptier that minimize backlog (i.e., the height of the fullest cup) is important for applications in processor scheduling, buffer management in networks, quality of service guarantees, and deamortization.We prove that the greedy algorithm (i.e., the empty-from-fullest-cups algorithm) achieves backlog O(log n) for any p \u2265 1. This resolves a long-standing open problem for p > 1, and is asymptotically optimal as long as n \u2265 2p.If the filler is an oblivious adversary, then we prove that there is a randomized emptying algorithm that achieve backlog O(log p + log log n) with probability 1 \u2013 2\u2212 polylog(n) for 2polylog(n) steps. This is known to be \u2026", "year": 2020, "venue": "", "authors": "William Kuszmaul"}, {"title": "Beyond uniform sampling: Offline reinforcement learning with imbalanced datasets", "abstract": "Offline reinforcement learning (RL) enables learning a decision-making policy without interaction with the environment. This makes it particularly beneficial in situations where such interactions are costly. However, a known challenge for offline RL algorithms is the distributional mismatch between the state-action distributions of the learned policy and the dataset, which can significantly impact performance. State-of-the-art algorithms address it by constraining the policy to align with the state-action pairs in the dataset. However, this strategy struggles on datasets that predominantly consist of trajectories collected by low-performing policies and only a few trajectories from high-performing ones. Indeed, the constraint to align with the data leads the policy to imitate low-performing behaviors predominating the dataset. Our key insight to address this issue is to constrain the policy to the policy that collected the good parts of the dataset rather than all data. To this end, we optimize the importance sampling weights to emulate sampling data from a data distribution generated by a nearly optimal policy. Our method exhibits considerable performance gains (up to five times better) over the existing approaches in state-of-the-art offline RL algorithms over 72 imbalanced datasets with varying types of imbalance.", "year": 2023, "venue": "", "authors": "Zhang-Wei Hong and Aviral Kumar and Sathwik Karnik and Abhishek Bhandwaldar and Akash Srivastava and Joni Pajarinen and Romain Laroche and Abhishek Gupta and Pulkit Agrawal"}, {"title": "Capacity of noisy permutation channels", "abstract": "We establish the capacity of a class of communication channels introduced by Makur. The  -letter input from a finite alphabet is passed through a discrete memoryless channel   and then the output  -letter sequence is uniformly permuted. We show that the maximal communication rate (normalized by  ) equals   whenever   is strictly positive. This is done by establishing a converse bound matching the achievability of Makur. The two main ingredients of our proof are: 1) a sharp bound on the Kullback-Leibler divergence of a uniformly sampled vector from a type class and observed through a DMC to an iid vector; and 2) the covering  -net of a probability simplex with Kullback-Leibler divergence as a metric. In addition to strictly positive DMC we also find the noisy permutation capacity for  -ary erasure channels, the Z-channel and others.", "year": 2023, "venue": "", "authors": "Jennifer Tang and Yury Polyanskiy"}, {"title": "Understanding and increasing efficiency of frank-wolfe adversarial training", "abstract": "Deep neural networks are easily fooled by small perturbations known as adversarial attacks. Adversarial Training (AT) is a technique that approximately solves a robust optimization problem to minimize the worst-case loss and is widely regarded as the most effective defense against such attacks. Due to the high computation time for generating strong adversarial examples in the AT process, single-step approaches have been proposed to reduce training time. However, these methods suffer from catastrophic overfitting where adversarial accuracy drops during training, and although improvements have been proposed, they increase training time and robustness is far from that of multi-step AT. We develop a theoretical framework for adversarial training with FW optimization (FW-AT) that reveals a geometric connection between the loss landscape and the distortion of l-inf FW attacks (the attack's l-2 norm). Specifically, we analytically show that high distortion of FW attacks is equivalent to small gradient variation along the attack path. It is then experimentally demonstrated on various deep neural network architectures that l-inf attacks against robust models achieve near maximal l-2 distortion, while standard networks have lower distortion. Furthermore, it is experimentally shown that catastrophic overfitting is strongly correlated with low distortion of FW attacks. This mathematical transparency differentiates FW from the more popular Projected Gradient Descent (PGD) optimization. To demonstrate the utility of our theoretical framework we develop FW-AT-Adapt, a novel adversarial training algorithm which uses a simple distortion measure to adapt the \u2026", "year": 2022, "venue": "", "authors": "Theodoros Tsiligkaridis and Jay Roberts"}, {"title": "The multiplicative version of azuma's inequality, with an application to contention analysis", "abstract": "Azuma's inequality is a tool for proving concentration bounds on random variables. The inequality can be thought of as a natural generalization of additive Chernoff bounds. On the other hand, the analogous generalization of multiplicative Chernoff bounds has, to our knowledge, never been explicitly formulated. We formulate a multiplicative-error version of Azuma's inequality. We then show how to apply this new inequality in order to greatly simplify (and correct) the analysis of contention delays in multithreaded systems managed by randomized work stealing.", "year": 2021, "venue": "", "authors": "William Kuszmaul and Qi Qi"}, {"title": "On estimating edit distance: Alignment, dimension reduction, and embeddings", "abstract": "Edit distance is a fundamental measure of distance between strings and has been widely studied in computer science. While the problem of estimating edit distance has been studied extensively, the equally important question of actually producing an alignment (i.e., the sequence of edits) has received far less attention. Somewhat surprisingly, we show that any algorithm to estimate edit distance can be used in a black-box fashion to produce an approximate alignment of strings, with modest loss in approximation factor and small loss in run time. Plugging in the result of Andoni, Krauthgamer, and Onak, we obtain an alignment that is a  approximation in time . Closely related to the study of approximation algorithms is the study of metric embeddings for edit distance. We show that min-hash techniques can be useful in designing edit distance embeddings through three results: (1) An embedding from Ulam distance (edit distance over permutations) to Hamming space that matches the best known distortion of  and also implicitly encodes a sequence of edits between the strings; (2) In the case where the edit distance between the input strings is known to have an upper bound , we show that embeddings of edit distance into Hamming space with distortion  can be modified in a black-box fashion to give distortion  for a class of periodic-free strings; (3) A randomized dimension-reduction map with contraction  and asymptotically optimal expected distortion , improving on the previous  distortion result of Batu, Ergun, and Sahinalp.", "year": 2018, "venue": "", "authors": "Moses Charikar and Ofir Geri and Michael P Kim and William Kuszmaul"}, {"title": "The journey, not the destination: How data guides diffusion models", "abstract": "Diffusion models trained on large datasets can synthesize photo-realistic images of remarkable quality and diversity. However, attributing these images back to the training data-that is, identifying specific training examples which caused an image to be generated-remains a challenge. In this paper, we propose a framework that: (i) provides a formal notion of data attribution in the context of diffusion models, and (ii) allows us to counterfactually validate such attributions. Then, we provide a method for computing these attributions efficiently. Finally, we apply our method to find (and evaluate) such attributions for denoising diffusion probabilistic models trained on CIFAR-10 and latent diffusion models trained on MS COCO. We provide code at https://github.com/MadryLab/journey-TRAK .", "year": 2023, "venue": "", "authors": "Kristian Georgiev and Joshua Vendrow and Hadi Salman and Sung Min Park and Aleksander Madry"}, {"title": "Intentional creation of carbon-rich dark earth soils in the Amazon", "abstract": "Fertile soil known as Amazonian dark earth is central to the debate over the size and ecological impact of ancient human populations in the Amazon. Dark earth is typically associated with human occupation, but it is uncertain whether it was created intentionally. Dark earth may also be a substantial carbon sink, but its spatial extent and carbon inventory are unknown. We demonstrate spatial and compositional similarities between ancient and modern dark earth and document modern Indigenous practices that enrich soil, which we use to propose a model for the formation of ancient dark earth. This comparison suggests that ancient Amazonians managed soil to improve fertility and increase crop productivity. These practices also sequestered and stored carbon in the soil for centuries, and we show that some ancient sites contain as much carbon as the above-ground rainforest biomass. Our results demonstrate the \u2026", "year": 2023, "venue": "", "authors": "Morgan J Schmidt and Samuel L Goldberg and Michael Heckenberger and Carlos Fausto and Bruna Franchetto and Jennifer Watling and Helena Lima and Bruno Moraes and Wetherbee B Dorshow and Joshua Toney and Yamalui Kuikuro and Kumessi Waura and Huke Kuikuro and Taku Wate Kuikuro and Takum\u00e3 Kuikuro and Yahila Kuikuro and Afukaka Kuikuro and Wenceslau Teixeira and Bruna Rocha and Vinicius Honorato and Hugo Tavares and Marcos Magalh\u00e3es and Carlos Augusto Barbosa and Jo\u00e3o Aires Da Fonseca and Kelton Mendes and Lu\u00eds Reynaldo Ferracci\u00fa Alleoni and Carlos Eduardo Pellegrino Cerri and Manuel Arroyo-Kalin and Eduardo Neves and J Taylor Perron"}, {"title": "Can learned models replace hash functions?", "abstract": "Hashing is a fundamental operation in database management, playing a key role in the implementation of numerous core database data structures and algorithms. Traditional hash functions aim to mimic a function that maps a key to a random value, which can result in collisions, where multiple keys are mapped to the same value. There are many well-known schemes like chaining, probing, and cuckoo hashing to handle collisions. In this work, we aim to study if using learned models instead of traditional hash functions can reduce collisions and whether such a reduction translates to improved performance, particularly for indexing and joins. We show that learned models reduce collisions in some cases, which depend on how the data is distributed. To evaluate the effectiveness of learned models as hash function, we test them with bucket chaining, linear probing, and cuckoo hash tables. We find that learned models can (1) yield a 1.4x lower probe latency, and (2) reduce the non-partitioned hash join runtime with 28% over the next best baseline for certain datasets. On the other hand, if the data distribution is not suitable, we either do not see gains or see worse performance. In summary, we find that learned models can indeed outperform hash functions, but only for certain data distributions.", "year": 2022, "venue": "", "authors": "Ibrahim Sabek and Kapil Vaidya and Dominik Horn and Andreas Kipf and Michael Mitzenmacher and Tim Kraska"}, {"title": "Catalyst: fast biochemical modeling with Julia", "abstract": "We introduce Catalyst.jl, a flexible and feature-filled Julia library for modeling and high performance simulation of chemical reaction networks (CRNs). Catalyst acts as both a domain-specific language and an intermediate representation for symbolically encoding CRN models as Julia-native objects. This enables a pipeline of symbolically specifying, analyzing, and modifying reaction networks; converting Catalyst models to symbolic representations of concrete mathematical models; and generating compiled code for use in numerical solvers. Currently Catalyst supports conversion to symbolic discrete stochastic chemical kinetics (jump process), chemical Langevin (stochastic differential equation), and mass-action reaction rate equation (ordinary differential equation) models. Leveraging ModelingToolkit.jl and Symbolics.jl, Catalyst models can be analyzed, simplified, and compiled into optimized representations for use in a broad variety of numerical solvers. The performance of the numerical solvers Catalyst targets is illustrated across a variety of reaction networks by benchmarking stochastic simulation algorithm and ODE solver performance. We demonstrate the extendability and composability of Catalyst by highlighting both how it can compose with a variety of Julia libraries, and how existing open source projects have extended the intermediate representation. These benchmarks demonstrate significant performance improvements compared to several popular reaction network simulators.", "year": 2022, "venue": "", "authors": "Torkel E Loman and Yingbo Ma and Vasily Ilin and Shashi Gowda and Niklas Korsbo and Nikhil Yewale and Chris Rackauckas and Samuel A Isaacson"}, {"title": "Surrogate-and invariance-boosted contrastive learning for data-scarce applications in science", "abstract": "Deep learning techniques have been increasingly applied to the natural sciences, e.g., for property prediction and optimization or material discovery. A fundamental ingredient of such approaches is the vast quantity of labeled data needed to train the model. This poses severe challenges in data-scarce settings where obtaining labels requires substantial computational or labor resources. Noting that problems in natural sciences often benefit from easily obtainable auxiliary information sources, we introduce surrogate- and invariance-boosted contrastive learning (SIB-CL), a deep learning framework which incorporates three inexpensive and easily obtainable auxiliary information sources to overcome data scarcity. Specifically, these are: abundant unlabeled data, prior knowledge of symmetries or invariances, and surrogate data obtained at near-zero cost. We demonstrate SIB-CL\u2019s effectiveness and generality on \u2026", "year": 2022, "venue": "", "authors": "Charlotte Loh and Thomas Christensen and Rumen Dangovski and Samuel Kim and Marin Solja\u010di\u0107"}, {"title": "Tighter expected generalization error bounds via convexity of information measures", "abstract": "Generalization error bounds are essential to understanding machine learning algorithms. This paper presents novel expected generalization error upper bounds based on the average joint distribution between the output hypothesis and each input training sample. Multiple generalization error upper bounds based on different information measures are provided, including Wasserstein distance, total variation distance, KL divergence, and Jensen-Shannon divergence. Due to the convexity of the information measures, the proposed bounds in terms of Wasserstein distance and total variation distance are shown to be tighter than their counterparts based on individual samples in the literature. An example is provided to demonstrate the tightness of the proposed generalization error bounds.", "year": 2022, "venue": "", "authors": "Gholamali Aminian and Yuheng Bu and Gregory W Wornell and Miguel RD Rodrigues"}, {"title": "Layer-parallel training with gpu concurrency of deep residual neural networks via nonlinear multigrid", "abstract": "A Multigrid Full Approximation Storage algorithm for solving Deep Residual Networks is developed to enable neural network parallelized layer-wise training and concurrent computational kernel execution on GPUs. This work demonstrates a 10.2x speedup over traditional layer-wise model parallelism techniques using the same number of compute units.", "year": 2020, "venue": "", "authors": "Andrew Kirby and Siddharth Samsi and Michael Jones and Albert Reuther and Jeremy Kepner and Vijay Gadepally"}, {"title": "Tiny pointers", "abstract": "This paper introduces a new data-structural object that we call the tiny pointer. In many applications, traditional   -bit pointers can be replaced with   -bit tiny pointers at the cost of only a constant-factor time overhead and a small probability of failure. We develop a comprehensive theory of tiny pointers, and give optimal constructions for both fixed-size tiny pointers (i.e., settings in which all of the tiny pointers must be the same size) and variable-size tiny pointers (i.e., settings in which the average tiny-pointer size must be small, but some tiny pointers can be larger). If a tiny pointer references an item in an array filled to load factor   , then the optimal tiny-pointer size is    bits in the fixed-size case, and    expected bits in the variable-size case.Our tiny-pointer constructions also require us to revisit several classic problems having to do with balls and bins; these results may be of independent interest.Using tiny pointers, we apply tiny pointers to five classic data-structure problems. We \u2026", "year": 2024, "venue": "", "authors": "Michael A Bender and Alex Conway and Mart\u00edn Farach-Colton and William Kuszmaul and Guido Tagliavini"}, {"title": "What is a good metric to study generalization of minimax learners?", "abstract": "Minimax optimization has served as the backbone of many machine learning problems. Although the convergence behavior of optimization algorithms has been extensively studied in minimax settings, their generalization guarantees, ie, how the model trained on empirical data performs on the unseen testing data, have been relatively under-explored. A fundamental question remains elusive: What is a good metric to study generalization of minimax learners? In this paper, we aim to answer this question by first showing that primal risk, a universal metric to study generalization in minimization problems, fails in simple examples of minimax problems. Furthermore, another popular metric, the primal-dual risk, also fails to characterize the generalization behavior for minimax problems with nonconvexity, due to non-existence of saddle points. We thus propose a new metric to study generalization of minimax learners: the primal gap, to circumvent these issues. Next, we derive generalization bounds for the primal gap in nonconvex-concave settings. As byproducts of our analysis, we also solve two open questions: establishing generalization bounds for primal risk and primal-dual risk in this setting, and in the strong sense, ie, without assuming that the maximization and expectation can be interchanged. Finally, we leverage this new metric to compare the generalization behavior of two popular algorithms-gradient descent-ascent (GDA) and gradient descent-max (GDMax) in minimax optimization.", "year": 2022, "venue": "", "authors": "Asuman Ozdaglar and Sarath Pattathil and Jiawei Zhang and Kaiqing Zhang"}, {"title": "Combining diverse feature priors", "abstract": "To improve model generalization, model designers often restrict the features that their models use, either implicitly or explicitly. In this work, we explore the design space of leveraging such feature priors by viewing them as distinct perspectives on the data. Specifically, we find that models trained with diverse sets of explicit feature priors have less overlapping failure modes, and can thus be combined more effectively. Moreover, we demonstrate that jointly training such models on additional (unlabeled) data allows them to correct each other\u2019s mistakes, which, in turn, leads to better generalization and resilience to spurious correlations.", "year": 2022, "venue": "", "authors": "Saachi Jain and Dimitris Tsipras and Aleksander Madry"}, {"title": "Sparse flows: Pruning continuous-depth models", "abstract": "Continuous deep learning architectures enable learning of flexible probabilistic models for predictive modeling as neural ordinary differential equations (ODEs), and for generative modeling as continuous normalizing flows. In this work, we design a framework to decipher the internal dynamics of these continuous depth models by pruning their network architectures. Our empirical results suggest that pruning improves generalization for neural ODEs in generative modeling. We empirically show that the improvement is because pruning helps avoid mode-collapse and flatten the loss surface. Moreover, pruning finds efficient neural ODE representations with up to 98% less parameters compared to the original network, without loss of accuracy. We hope our results will invigorate further research into the performance-size trade-offs of modern continuous-depth models.", "year": 2021, "venue": "", "authors": "Lucas Liebenwein and Ramin Hasani and Alexander Amini and Daniela Rus"}, {"title": "Lea: A learned encoding advisor for column stores", "abstract": " Data warehouses organize data in a columnar format to enable faster scans and better compression. Modern systems offer a variety of column encodings that can reduce storage footprint and improve query performance. Selecting a good encoding scheme for a particular column is an optimization problem that depends on the data, the query workload, and the underlying hardware. We introduce Learned Encoding Advisor (LEA), a learned approach to column encoding selection. LEA is trained on synthetic datasets with various distributions on the target system. Once trained, LEA uses sample data and statistics (such as cardinality) from the user\u2019s database to predict the optimal column encodings. LEA can optimize for encoded size, query performance, or a combination of the two. Compared to the heuristic-based encoding advisor of a commercial column store on TPC-H, LEA achieves 19% lower query latency \u2026", "year": 2021, "venue": "", "authors": "Lujing Cen and Andreas Kipf and Ryan Marcus and Tim Kraska"}, {"title": "Signal enhancement for magnetic navigation challenge problem", "abstract": "Harnessing the magnetic field of the Earth for navigation has shown promise as a viable alternative to other navigation systems. A magnetic navigation system collects its own magnetic field data using a magnetometer and uses magnetic anomaly maps to determine the current location. The greatest challenge with magnetic navigation arises when the magnetic field measurements from the magnetometer encompass the magnetic field from not just the Earth, but also from the vehicle on which it is mounted. It is difficult to separate the Earth magnetic anomaly field, which is crucial for navigation, from the total magnetic field reading from the sensor. The purpose of this challenge problem is to decouple the Earth and aircraft magnetic signals in order to derive a clean signal from which to perform magnetic navigation. Baseline testing on the dataset has shown that the Earth magnetic field can be extracted from the total magnetic field using machine learning (ML). The challenge is to remove the aircraft magnetic field from the total magnetic field using a trained model. This challenge offers an opportunity to construct an effective model for removing the aircraft magnetic field from the dataset by using a scientific machine learning (SciML) approach comprised of an ML algorithm integrated with the physics of magnetic navigation.", "year": 2020, "venue": "", "authors": "Albert R Gnadt and Joseph Belarge and Aaron Canciani and Glenn Carl and Lauren Conger and Joseph Curro and Alan Edelman and Peter Morales and Aaron P Nielsen and Michael F O'Keeffe and Christopher V Rackauckas and Jonathan Taylor and Allan B Wollaber"}, {"title": "Flushing without cascades", "abstract": "Buffer-and-flush is a technique for transforming standard external-memory search trees into write-optimized search trees. In exchange for faster amortized insertions, buffer-and-flush can sometimes significantly increase the latency of operations by causing cascades of flushes. In this paper, we show that flushing cascades are not a fundamental consequence of the buffer-flushing technique, and can be removed entirely using randomization techniques.The underlying implementation of buffer flushing relies on a buffer-eviction strategy at each node in the tree. The ability for the user to select the buffer eviction strategy based on the workload has been shown to be important for performance, both in theory and in practice.In order to support arbitrary buffer-eviction strategies, we introduce the notion of a universal flush, which uses a universal eviction policy that can simulate any other eviction policy. This abstracts away \u2026", "year": 2020, "venue": "", "authors": "Michael A Bender and Rathish Das and Mart\u00edn Farach-Colton and Rob Johnson and William Kuszmaul"}, {"title": "Fast algorithms for finding pattern avoiders and counting pattern occurrences in permutations", "abstract": "Given a set  of permutation patterns of length at most , we present an algorithm for building , the set of permutations of length at most  avoiding the patterns in , in time . Additionally, we present an -time algorithm for counting the number of copies of patterns from  in each permutation in . Surprisingly, when , this runtime can be improved to , spending only constant time per permutation. Whereas the previous best algorithms, based on generate-and-check, take exponential time per permutation analyzed, all of our algorithms take time at most polynomial per outputted permutation.", "year": 2018, "venue": "", "authors": "William Kuszmaul"}, {"title": "Learning to see physical properties with active sensing motor policies", "abstract": "Knowledge of terrain's physical properties inferred from color images can aid in making efficient robotic locomotion plans. However, unlike image classification, it is unintuitive for humans to label image patches with physical properties. Without labeled data, building a vision system that takes as input the observed terrain and predicts physical properties remains challenging. We present a method that overcomes this challenge by self-supervised labeling of images captured by robots during real-world traversal with physical property estimators trained in simulation. To ensure accurate labeling, we introduce Active Sensing Motor Policies (ASMP), which are trained to explore locomotion behaviors that increase the accuracy of estimating physical parameters. For instance, the quadruped robot learns to swipe its foot against the ground to estimate the friction coefficient accurately. We show that the visual system trained with a small amount of real-world traversal data accurately predicts physical parameters. The trained system is robust and works even with overhead images captured by a drone despite being trained on data collected by cameras attached to a quadruped robot walking on the ground.", "year": 2023, "venue": "", "authors": "Gabriel B Margolis and Xiang Fu and Yandong Ji and Pulkit Agrawal"}, {"title": "Information-theoretic characterizations of generalization error for the Gibbs algorithm", "abstract": "Various approaches have been developed to upper bound the generalization error of a supervised learning algorithm. However, existing bounds are often loose and even vacuous when evaluated in practice. As a result, they may fail to characterize the exact generalization ability of a learning algorithm. Our main contributions are exact characterizations of the expected generalization error of the well-known Gibbs algorithm (a.k.a. Gibbs posterior) using different information measures, in particular, the symmetrized KL information between the input training samples and the output hypothesis. Our result can be applied to tighten existing expected generalization errors and PAC-Bayesian bounds. Our information-theoretic approach is versatile, as it also characterizes the generalization error of the Gibbs algorithm with a data-dependent regularizer and that of the Gibbs algorithm in the asymptotic regime, where it \u2026", "year": 2023, "venue": "", "authors": "Gholamali Aminian and Yuheng Bu and Laura Toni and Miguel RD Rodrigues and Gregory W Wornell"}, {"title": "Linear probing revisited: Tombstones mark the demise of primary clustering", "abstract": "The linear-probing hash table is one of the oldest and most widely used data structures in computer science. However, linear probing famously comes with a major draw-back: as soon as the hash table reaches a high memory utilization, elements within the hash table begin to cluster together, causing insertions to become slow. This phenomenon, now known as primary clustering, was first captured by Donald Knuth in 1963; at a load factor of , the expected time per insertion is , rather than the more desirable . We show that there is more to the story than the classic analysis would seem to suggest. It turns out that small design decisions in how deletions are implemented have dramatic effects on the asymptotic performance of insertions. If these design decisions are made correctly, then even a hash table that is continuously at a load factor  can achieve average insertion time . A key insight is that the tombstones left behind by deletions \u2026", "year": 2022, "venue": "", "authors": "Michael A Bender and Bradley C Kuszmaul and William Kuszmaul"}, {"title": "Learning force control for legged manipulation", "abstract": "Controlling the contact force during interactions is an inherent requirement for locomotion and manipulation tasks. Current reinforcement learning approaches to locomotion and manipulation rely implicitly on forceful interaction to accomplish tasks but do not explicitly regulate it. This paper proposes a reinforcement learning task specification that focuses on matching desired contact force levels. Integrating force control with the coordination of a robot\u2019s body and arm, we present an end-to-end policy for legged manipulator control. Force control enables us to realize compliant gripper and whole-body pulling movements that have not been previously demonstrated using a learned policy. It also facilitates a characterization of the force-tracking performance of learned policies in simulation and the real world, indicating their performance potential for force-critical tasks. Video is available at the project website: https://tif \u2026", "year": 2024, "venue": "", "authors": "Tifanny Portela and Gabriel B Margolis and Yandong Ji and Pulkit Agrawal"}, {"title": "Designing and implementing an AI education program for learners with diverse background at scale", "abstract": "This Research to Practice Full Paper presents an AI Education program. In January 2021 MIT entered into an agreement with the United States Air Force (USAF) and the Department of Defense (DoD) to design and offer a new educational research program focusing on Artificial Intelligence (AI) training. The goal of this collaboration is to design and advance educational research activities that promote maximum learning outcomes at scale for learners with diverse roles and educational backgrounds, ranging from Air Force and DoD personnel to the general public. This program is expected to offer different learning tracks addressing different groups of USAF employees based on their unique professional needs and backgrounds. The first pilot is currently underway and will provide the research team with data and insights that will inform the next iteration of the program, with the ultimate goal of formulating \u2026", "year": 2022, "venue": "", "authors": "Andres F Salazar-Gomez and Aikaterini Bagiati and Nicholas Minicucci and Kathleen D Kennedy and Xiaoxue Du and Cynthia Breazeal"}, {"title": "A green (er) world for ai", "abstract": "As research and practice in artificial intelligence (A.I.) grow in leaps and bounds, the resources necessary to sustain and support their operations also grow at an increasing pace. While innovations and applications from A.I. have brought significant advances, from applications to vision and natural language to improvements to fields like medical imaging and materials engineering, their costs should not be neglected. As we embrace a world with ever-increasing amounts of data as well as research & development of A.I. applications, we are sure to face an ever-mounting energy footprint to sustain these computational budgets, data storage needs, and more. But, is this sustainable and, more importantly, what kind of setting is best positioned to nurture such sustainable A.I. in both research and practice? In this paper, we outline our outlook for Green A.I.\u2014a more sustainable, energy-efficient and energy-aware \u2026", "year": 2022, "venue": "", "authors": "Dan Zhao and Nathan C Frey and Joseph McDonald and Matthew Hubbell and David Bestor and Michael Jones and Andrew Prout and Vijay Gadepally and Siddharth Samsi"}, {"title": "Spatial temporal analysis of 40,000,000,000,000 internet darkspace packets", "abstract": "The Internet has never been more important to our society, and understanding the behavior of the Internet is essential. The Center for Applied Internet Data Analysis (CAIDA) Telescope observes a continuous stream of packets from an unsolicited darkspace representing 1/256 of the Internet. During 2019 and 2020 over 40,000,000,000,000 unique packets were collected representing the largest ever assembled public corpus of Internet traffic. Using the combined resources of the Supercomputing Centers at UC San Diego, Lawrence Berkeley National Laboratory, and MIT, the spatial temporal structure of anonymized source-destination pairs from the CAIDA Telescope data has been analyzed with GraphBLAS hierarchical hyper-sparse matrices. These analyses provide unique insight on this unsolicited Internet darkspace traffic with the discovery of many previously unseen scaling relations. The data show a \u2026", "year": 2021, "venue": "", "authors": "Jeremy Kepner and Michael Jones and Daniel Andersen and Ayd\u0131n Bulu\u00e7 and Chansup Byun and K Claffy and Timothy Davis and William Arcand and Jonathan Bernays and David Bestor and William Bergeron and Vijay Gadepally and Micheal Houle and Matthew Hubbell and Anna Klein and Chad Meiners and Lauren Milechin and Julie Mullen and Sandeep Pisharody and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Doug Stetson and Adam Tse and Charles Yee and Peter Michaleas"}, {"title": "Scalable and flexible deep Bayesian optimization with auxiliary information for scientific problems", "abstract": "", "year": 2021, "venue": "", "authors": "Samuel Kim and Peter Y Lu and Charlotte Loh and Jamie Smith and Jasper Snoek and Marin Soljacic"}, {"title": "Tight bounds for parallel paging and green paging", "abstract": "In the parallel paging problem, there are p processors that share a cache of size k. The goal is to partition the cache among the processors over time in order to minimize their average completion time. For this long-standing open problem, we give tight upper and lower bounds of \u0398(logp) on the competitive ratio with O(1) resource augmentation.A key idea in both our algorithms and lower bounds is to relate the problem of parallel paging to the seemingly unrelated problem of green paging. In green paging, there is an energy-optimized processor that can temporarily turn off one or more of its cache banks (thereby reducing power consumption), so that the cache size varies between a maximum size k and a minimum size k/p. The goal is to minimize the total energy consumed by the computation, which is proportional to the integral of the cache size over time.We show that any efficient solution to green paging can be \u2026", "year": 2021, "venue": "", "authors": "Kunal Agrawal and Michael A Bender and Rathish Das and William Kuszmaul and Enoch Peserico and Michele Scquizzato"}, {"title": "BP-Tree: overcoming the point-range operation tradeoff for in-memory B-trees", "abstract": "B-trees are the go-to data structure for in-memory indexes in databases and storage systems. B-trees support both point operations (i.e., inserts and finds) and range operations (i.e., iterators and maps). However, there is an inherent tradeoff between point and range operations since the optimal node size for point operations is much smaller than the optimal node size for range operations. Existing implementations use a relatively small node size to achieve fast point operations at the cost of range operation throughput. We present the BP-tree, a variant of the B-tree, that overcomes the decades-old point-range operation tradeoff in traditional B-trees. In the BP-tree, the leaf nodes are much larger in size than the internal nodes to support faster range scans. To avoid any slowdown in point operations due to large leaf nodes, we introduce a new insert-optimized array called the buffered partitioned array (BPA) to \u2026", "year": 2024, "venue": "", "authors": "Helen Xu and Amanda Li and Brian Wheatman and Manoj Marneni and Prashant Pandey"}, {"title": "Kaleidoscope: Semantically-grounded, context-specific ML model evaluation", "abstract": " Desired model behavior often differs across contexts (e.g., different geographies, communities, or institutions), but there is little infrastructure to facilitate context-specific evaluations key to deployment decisions and building trust. Here, we present Kaleidoscope, a system for evaluating models in terms of user-driven, domain-relevant concepts. Kaleidoscope\u2019s iterative workflow enables generalizing from a few examples into a larger, diverse set representing an important concept. These example sets can be used to test model outputs or shifts in model behavior in semantically-meaningful ways. For instance, we might construct a \u201cxenophobic comments\u201d set and test that its examples are more likely to be flagged by a content moderation model than a \u201ccivil discussion\u201d set. To evaluate Kaleidoscope, we compare it against template- and DSL-based grouping methods, and conduct a usability study with 13 Reddit users \u2026", "year": 2023, "venue": "", "authors": "Harini Suresh and Divya Shanmugam and Tiffany Chen and Annie G Bryan and Alexander D'Amour and John Guttag and Arvind Satyanarayan"}, {"title": "Benchmarking resource usage for efficient distributed deep learning", "abstract": "Deep learning (DL) workflows demand an ever-increasing budget of compute and energy in order to achieve outsized gains. As such, it becomes essential to understand how different deep neural networks (DNNs) and training leverage increasing compute and energy resources-especially specialized computationally-intensive models across different domains and applications. In this paper, we conduct over 3,400 experiments training an array of deep networks representing various domains/tasks-natural language processing, computer vision, and chemistry-on up to 424 graphics processing units (GPUs). During training, our experiments systematically vary compute resource characteristics and energy -saving mechanisms such as power utilization and GPU clock rate limits to capture and illustrate the different trade-offs and scaling behaviors each representative model exhibits under various resource and energy \u2026", "year": 2022, "venue": "", "authors": "Nathan C Frey and Baolin Li and Joseph McDonald and Dan Zhao and Michael Jones and David Bestor and Devesh Tiwari and Vijay Gadepally and Siddharth Samsi"}, {"title": "Interpretable autonomous flight via compact visualizable neural circuit policies", "abstract": "We learn interpretable end-to-end controllers based on Neural Circuit Policies (NCPs) to enable goal reaching and dynamic obstacle avoidance in flight domains. In addition to being able to learn high-quality control, NCP networks are designed with a small number of neurons. This property allows for the learned policies to be interpreted at the neuron level and interrogated, leading to more robust understanding of why the artificial agents make the decisions that they do. We also demonstrate transfer of the learned policy to physical flight hardware by deploying a small NCP (200 KB of memory) capable of real-time inference on a Raspberry Pi Zero controlling a DJI Tello drone. Designing interpretable artificial agents is crucial for building trustworthy AIs, both as fully autonomous systems and also for parallel autonomy, where humans and AIs work on collaboratively solving problems in the same environment.", "year": 2022, "venue": "", "authors": "Paul Tylkin and Tsun-Hsuan Wang and Kyle Palko and Ross Allen and Ho Chit Siu and Daniel Wrafter and Tim Seyde and Alexander Amini and Daniela Rus"}, {"title": "AbstractDifferentiation. jl: Backend-agnostic differentiable programming in Julia", "abstract": "No single Automatic Differentiation (AD) system is the optimal choice for all problems. This means informed selection of an AD system and combinations can be a problem-specific variable that can greatly impact performance. In the Julia programming language, the major AD systems target the same input and thus in theory can compose. Hitherto, switching between AD packages in the Julia Language required end-users to familiarize themselves with the user-facing API of the respective packages. Furthermore, implementing a new, usable AD package required AD package developers to write boilerplate code to define convenience API functions for end-users. As a response to these issues, we present AbstractDifferentiation.jl for the automatized generation of an extensive, unified, user-facing API for any AD package. By splitting the complexity between AD users and AD developers, AD package developers only need to implement one or two primitive definitions to support various utilities for AD users like Jacobians, Hessians and lazy product operators from native primitives such as pullbacks or pushforwards, thus removing tedious -- but so far inevitable -- boilerplate code, and enabling the easy switching and composing between AD implementations for end-users.", "year": 2021, "venue": "", "authors": "Frank Sch\u00e4fer and Mohamed Tarek and Lyndon White and Chris Rackauckas"}, {"title": "Paging and the address-translation problem", "abstract": "The classical paging problem, introduced by Sleator and Tarjan in 1985, formalizes the problem of caching pages in RAM in order to minimize IOs. Their online formulation ignores the cost of address translation: programs refer to data via virtual addresses, and these must be translated into physical locations in RAM. Although the cost of an individual address translation is much smaller than that of an IO, every memory access involves an address translation, whereas IOs can be infrequent. In practice, one can spend money to avoid paging by over-provisioning RAM; in contrast, address translation is effectively unavoidable. Thus address-translation costs can sometimes dominate paging costs, and systems must simultaneously optimize both.  To mitigate the cost of address translation, all modern CPUs have translation lookaside buffers (TLBs), which are hardware caches of common address translations. What \u2026", "year": 2021, "venue": "", "authors": "Michael A Bender and Abhishek Bhattacharjee and Alex Conway and Mart\u00edn Farach-Colton and Rob Johnson and Sudarsun Kannan and William Kuszmaul and Nirjhar Mukherjee and Don Porter and Guido Tagliavini and Janet Vorobyeva and Evan West"}, {"title": "Failure prediction by confidence estimation of uncertainty-aware Dirichlet networks", "abstract": "Reliably assessing model confidence in deep learning and predicting errors likely to be made are key elements in providing safety for model deployment, in particular for applications with dire consequences. In this paper, it is first shown that uncertainty-aware deep Dirichlet neural networks provide an improved separation between the confidence of correct and incorrect predictions in the true class probability (TCP) metric. Second, as the true class is unknown at test time, a new criterion is proposed for learning the true class probability by matching prediction confidence scores while taking imbalance and TCP constraints into account for correct predictions and failures. Experimental results show our method improves upon the maximum class probability (MCP) baseline and predicted TCP for standard networks on several image classification tasks with various network architectures.", "year": 2021, "venue": "", "authors": "Theodoros Tsiligkaridis"}, {"title": "Randomized cup game algorithms against strong adversaries", "abstract": "In each step of the cup game on n cups, a filler distributes up to 1 \u2013 \u220a water among the cups, and then an emptier removes 1 unit of water from a single cup. The emptier's goal is to minimize the height of the fullest cup, also known as the backlog. The cup emptying game has found extensive applications to processor scheduling, network-switch buffer management, quality of service guarantees, and data-structure deamortization.The greedy emptying algorithm (i.e., always remove from the fullest cup) is known to achieve backlog O(log n) and to be the optimal deterministic algorithm. Randomized algorithms can do significantly better, achieving backlog O (log log n) with high probability, as long as \u220a is not too small. In order to achieve these improvements, the known randomized algorithms require that the filler is an oblivious adversary, unaware of which cups the emptier chooses to empty out of at each step. Such \u2026", "year": 2021, "venue": "", "authors": "Michael A Bender and William Kuszmaul"}, {"title": "PARAD: A Work-Efficient Parallel Algorithm for Reverse-Mode Automatic Differentiation\u2217", "abstract": "Automatic differentiation (AD) is a technique for computing the derivative of function F: Rn \u2192 Rm defined by a computer program. Modern applications of AD, such as machine learning, typically use AD to facilitate gradient-based optimization of an objective function for which m\u226an (often m=1). As a result, these applications typically use reverse (or adjoint) mode AD to compute the gradient of F efficiently, in time \u0398(m\u00b7T1(F)), where T1 is the work (serial running time) of F. Although the serial running time of reverse-mode AD has a well known relationship to the total work of F, general-purpose reverse-mode AD has proven challenging to parallelize in a work-efficient and scalable fashion, as simple approaches tend to result in poor performance or scalability.This paper introduces PARAD, a work-efficient parallel algorithm for reverse-mode AD of determinacy-race-free recursive fork-join programs. We analyze the \u2026", "year": 2021, "venue": "", "authors": "Tim Kaler and Tao B Schardl and Brian Xie and Charles E Leiserson and Jie Chen and Aldo Pareja and Georgios Kollias"}, {"title": "The one-way communication complexity of dynamic time warping distance", "abstract": "We resolve the randomized one-way communication complexity of Dynamic Time Warping (DTW) distance. We show that there is an efficient one-way communication protocol using  bits for the problem of computing an -approximation for DTW between strings  and  of length , and we prove a lower bound of  bits for the same problem. Our communication protocol works for strings over an arbitrary metric of polynomial size and aspect ratio, and we optimize the logarithmic factors depending on properties of the underlying metric, such as when the points are low-dimensional integer vectors equipped with various metrics or have bounded doubling dimension. We also consider linear sketches of DTW, showing that such sketches must have size .", "year": 2019, "venue": "", "authors": "Vladimir Braverman and Moses Charikar and William Kuszmaul and David P Woodruff and Lin F Yang"}, {"title": "Efficiently approximating edit distance between pseudorandom strings", "abstract": "We present an algorithm for approximating the edit distance ed(x, y) between two strings x and y in time parameterized by the degree to which one of the strings x satisfies a natural pseudorandomness property. The pseudorandomness model is asymmetric in that no requirements are placed on the second string y, which may be constructed by an adversary with full knowledge of x.We say that x is (p, B)-pseudorandom if all pairs a and b of disjoint B-letter substrings of x satisfy ed(a, b) \u2265 pB. Given parameters p and B, our algorithm computes the edit distance between a (p, B)-pseudorandom string x and an arbitrary string y within a factor of O(1/p) in time \u00d5(nB), with high probability. If x is generated at random, then with high probability it will be (\u03a9(1), O(log n))-pseudorandom, allowing us to compute ed(x, y) within a constant factor in near linear time. For strings x of varying degrees of pseudorandomness, our \u2026", "year": 2019, "venue": "", "authors": "William Kuszmaul"}, {"title": "Kairos: Building cost-efficient machine learning inference systems with heterogeneous cloud resources", "abstract": "Online inference is becoming a key service product for many businesses, deployed in cloud platforms to meet customer demands. Despite their revenue-generation capability, these services need to operate under tight Quality-of-Service (QoS) and cost budget constraints. This paper introduces KAIROS, a novel runtime framework that maximizes the query throughput while meeting QoS target and a cost budget. KAIROS designs and implements novel techniques to build a pool of heterogeneous compute hardware without online exploration overhead, and distribute inference queries optimally at runtime. Our evaluation using industry-grade machine learning (ML) models shows that KAIROS yields up to 2x the throughput of an optimal homogeneous solution, and outperforms state-of-the-art schemes by up to 70%, despite advantageous implementations of the competing schemes to ignore their exploration overhead.", "year": 2023, "venue": "", "authors": "Baolin Li and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "MultiEarth 2023--Multimodal Learning for Earth and Environment Workshop and Challenge", "abstract": "The Multimodal Learning for Earth and Environment Workshop (MultiEarth 2023) is the second annual CVPR workshop aimed at the monitoring and analysis of the health of Earth ecosystems by leveraging the vast amount of remote sensing data that is continuously being collected. The primary objective of this workshop is to bring together the Earth and environmental science communities as well as the multimodal representation learning communities to explore new ways of harnessing technological advancements in support of environmental monitoring. The MultiEarth Workshop also seeks to provide a common benchmark for processing multimodal remote sensing information by organizing public challenges focused on monitoring the Amazon rainforest. These challenges include estimating deforestation, detecting forest fires, translating synthetic aperture radar (SAR) images to the visible domain, and projecting environmental trends. This paper presents the challenge guidelines, datasets, and evaluation metrics. Our challenge website is available at https://sites.google.com/view/rainforest-challenge/multiearth-2023.", "year": 2023, "venue": "", "authors": "Miriam Cha and Gregory Angelides and Mark Hamilton and Andy Soszynski and Brandon Swenson and Nathaniel Maidel and Phillip Isola and Taylor Perron and Bill Freeman"}, {"title": "Geosynchronous satellite behavior classification via unsupervised machine learning", "abstract": "Most satellites in the geosynchronous orbital regime (GEO) spend the vast majority of their operational lifetimes performing station-keeping maneuvers to counteract the natural perturbative forces of the near-Earth space environment. Consistently performing such maneuvers allows satellites to appear nearly stationary in the sky to observers on the ground and enables unique mission capabilities unavailable at lower and higher altitudes. The mechanics of these station-keeping maneuvers, however, are rarely discussed in the public domain. Common descriptions of GEO satellite station-keeping behaviors, such as \u201cEast-West\u201d(EW) and \u201cNorth-South\u201d(NS), assign a small number of labels to diverse station-keeping activities based on observed adherence to longitudinal and latitudinal position constraints, respectively. Because satellite operators often choose to change their station-keeping methodologies over time\u2014or abandon the practice altogether while engaging in other on-orbit activities, such as longitudinal-shift maneuvers, on-orbit servicing, or retirement\u2014more detailed GEO satellite behavioral classes may be better suited for describing historical activities. This paper describes a methodology for identifying a novel series of behavioral classes that correspond to GEO satellite station-keeping and natural drift via unsupervised machine learning. The presented algorithm applies adapted k-means clustering to time-series longitudinal positions derived from historical orbital element data encoded in publicly available two-line element data sets (TLEs). Preliminary results are discussed after deploying the algorithm to identify six clusters \u2026", "year": 2023, "venue": "", "authors": "Thomas G Roberts and Haley E Solera and Richard Linares"}, {"title": "Revisiting the adversarial robustness-accuracy tradeoff in robot learning", "abstract": "Adversarial training (i.e., training on adversarially perturbed input data) is a well-studied method for making neural networks robust to potential adversarial attacks during inference. However, the improved robustness does not come for free but rather is accompanied by a decrease in overall model accuracy and performance. Recent work has shown that, in practical robot learning applications, the effects of adversarial training do not pose a fair trade-off but inflict a net loss when measured in holistic robot performance. This work revisits the robustness-accuracy trade-off in robot learning by systematically analyzing if recent advances in robust training methods and theory in conjunction with adversarial robot learning, are capable of making adversarial training suitable for real-world robot applications. We evaluate three different robot learning tasks ranging from autonomous driving in a high-fidelity environment \u2026", "year": 2023, "venue": "", "authors": "Mathias Lechner and Alexander Amini and Daniela Rus and Thomas A Henzinger"}, {"title": "Data-driven blind synchronization and interference rejection for digital communication signals", "abstract": "We study the potential of data-driven deep learning methods for separation of two communication signals from an observation of their mixture. In particular, we assume knowledge on the generation process of one of the signals, dubbed signal of interest (SOI), and no knowledge on the generation process of the second signal, referred to as interference. This form of the single-channel source separation problem is also referred to as interference rejection. We show that capturing high-resolution temporal structures (nonstationarities), which enables accurate synchronization to both the SOI and the interference, leads to substantial performance gains. With this key insight, we propose a domain-informed neural network (NN) design that is able to improve upon both \u201coff-the-shelf\u201d NNs and classical detection and interference rejection methods, as demonstrated in our simulations. Our findings highlight the key role \u2026", "year": 2022, "venue": "", "authors": "Alejandro Lancho and Amir Weiss and Gary CF Lee and Jennifer Tang and Yuheng Bu and Yury Polyanskiy and Gregory W Wornell"}, {"title": "Testing the skill of a species distribution model using a 21st century virtual ecosystem", "abstract": "Plankton communities play an important role in marine food webs, in biogeochemical cycling, and in Earth's climate; yet observations are sparse, and predictions of how they might respond to climate change vary. Correlative species distribution models (SDM's) have been applied to predicting biogeography based on relationships to observed environmental variables. To investigate sources of uncertainty, we use a correlative SDM to predict the plankton biogeography of a 21st century marine ecosystem model (Darwin). Darwin output is sampled to mimic historical ocean observations, and the SDM is trained using generalized additive models. We find that predictive skill varies across test cases, and between functional groups, with errors that are more attributable to spatiotemporal sampling bias than sample size. End\u2010of\u2010century predictions are poor, limited by changes in target\u2010predictor relationships over time \u2026", "year": 2021, "venue": "", "authors": "LR Bardon and BA Ward and Stephanie Dutkiewicz and BB Cael"}, {"title": "Multi-temporal analysis and scaling relations of 100,000,000,000 network packets", "abstract": "Our society has never been more dependent on computer networks. Effective utilization of networks requires a detailed understanding of the normal background behaviors of network traffic. Large-scale measurements of networks are computationally challenging. Building on prior work in interactive supercomputing and GraphBLAS hypersparse hierarchical traffic matrices, we have developed an efficient method for computing a wide variety of streaming network quantities on diverse time scales. Applying these methods to 100,000,000,000 anonymized source-destination pairs collected at a network gateway reveals many previously unobserved scaling relationships. These observations provide new insights into normal network background traffic that could be used for anomaly detection, AI feature engineering, and testing theoretical models of streaming networks.", "year": 2020, "venue": "", "authors": "Jeremy Kepner and Chad Meiners and Chansup Byun and Sarah McGuire and Timothy Davis and William Arcand and Jonathan Bernays and David Bestor and William Bergeron and Vijay Gadepally and Raul Harnasch and Matthew Hubbell and Micheal Houle and Micheal Jones and Andrew Kirby and Anna Klein and Lauren Milechin and Julie Mullen and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Doug Stetson and Adam Tse and Charles Yee and Peter Michaleas"}, {"title": "Green paging and parallel paging", "abstract": "We study two fundamental variants of the classic paging problem: green paging and parallel paging. In green paging one can choose the exact memory capacity in use at any given instant, between a maximum of k and a minimum of k/p pages; the goal is to minimize the integral of this number over the time required to complete a computation (note that running at lower capacity is not necessarily better, since might disproportionately increase the total completion time). In parallel paging, a memory of k pages is shared between p processors, each carrying out a separate computation; the goal is to minimize the respective completion times.We show how these two different problems are strictly related: any efficient solution to green paging can be converted into an efficient solution to parallel paging, and any lower bound for green paging can be converted into a lower bound for parallel paging---in both cases in a black \u2026", "year": 2020, "venue": "", "authors": "Kunal Agrawal and Michael A Bender and Rathish Das and William Kuszmaul and Enoch Peserico and Michele Scquizzato"}, {"title": "Gigastep-one billion steps per second multi-agent reinforcement learning", "abstract": "Multi-agent reinforcement learning (MARL) research is faced with a trade-off: it either uses complex environments requiring large compute resources, which makes it inaccessible to researchers with limited resources, or relies on simpler dynamics for faster execution, which makes the transferability of the results to more realistic tasks challenging. Motivated by these challenges, we present Gigastep, a fully vectorizable, MARL environment implemented in JAX, capable of executing up to one billion environment steps per second on consumer-grade hardware. Its design allows for comprehensive MARL experimentation, including a complex, high-dimensional space defined by 3D dynamics, stochasticity, and partial observations. Gigastep supports both collaborative and adversarial tasks, continuous and discrete action spaces, and provides RGB image and feature vector observations, allowing the evaluation of a wide range of MARL algorithms. We validate Gigastep's usability through an extensive set of experiments, underscoring its role in widening participation and promoting inclusivity in the MARL research community.", "year": 2023, "venue": "", "authors": "Mathias Lechner and Tim Seyde and Tsun-Hsuan Johnson Wang and Wei Xiao and Ramin Hasani and Joshua Rountree and Daniela Rus"}, {"title": "Toward automated instructor pilots in legacy Air Force systems: Physiology-based flight difficulty classification via machine learning", "abstract": "The United States Air Force (USAF) is struggling to train enough pilots to meet operational requirements. Technology has advanced rapidly over the last 70 years but USAF pilot training has not. Modern operational requirements demand a change and, for this reason, USAF senior leadership has advocated for innovation. The automation of instructor and evaluator pilots in select bottlenecks (e.g., simulators) is one such measure. However, to implement this vision, numerous technical issues must be mitigated. Accurate classification of flight difficulty is a foundational problem underpinning many of these technical issues, which requires either the acquisition of new systems or the development of new procedures. Therefore, given this need and the costly nature of purchasing new equipment, physiological-based classification of flight difficulty is our focus herein. Leveraging multimodal data from a designed experiment \u2026", "year": 2023, "venue": "", "authors": "William N Caballero and Nathan Gaw and Phillip R Jenkins and Chancellor Johnstone"}, {"title": "Low-rank univariate sum of squares has no spurious local minima", "abstract": "We study the problem of decomposing a polynomial  into a sum of  squares by minimizing a quadratically penalized objective . This objective is nonconvex and is equivalent to the rank- Burer\u2013Monteiro factorization of a semidefinite program (SDP) encoding the sum of squares decomposition. We show that for all univariate polynomials , if , then  has no spurious second-order critical points, showing that all local optima are also global optima. This is in contrast to previous work showing that for general SDPs, in addition to genericity conditions,  has to be roughly the square root of the number of constraints (the degree of ) for there to be no spurious second-order critical points. Our proof uses tools from computational algebraic geometry and can be interpreted as constructing a certificate using the first- and second-order necessary conditions. We also show that by choosing a norm \u2026", "year": 2023, "venue": "", "authors": "Beno\u00eet Legat and Chenyang Yuan and Pablo Parrilo"}, {"title": "Weak recovery threshold for the hypergraph stochastic block model", "abstract": "We study the weak recovery problem on the -uniform hypergraph stochastic block model (-HSBM) with two balanced communities. In HSBM a random graph is constructed by placing hyperedges with higher density if all vertices of a hyperedge share the same binary label, and weak recovery asks to recover a non-trivial fraction of the labels. We introduce a multi-terminal version of strong data processing inequalities (SDPIs), which we call the multi-terminal SDPI, and use it to prove a variety of impossibility results for weak recovery. In particular, we prove that weak recovery is impossible below the Kesten-Stigum (KS) threshold if , or a strength parameter  is at least . Prior work Pal and Zhu (2021) established that weak recovery in HSBM is always possible above the KS threshold. Consequently, there is no information-computation gap for these cases, which (partially) resolves a conjecture of Angelini et al.(2015). To our knowledge this is the first impossibility result for HSBM weak recovery. As usual, we reduce the study of non-recovery of HSBM to the study of non-reconstruction in a related broadcasting on hypertrees (BOHT) model. While we show that BOHT\u2019s reconstruction threshold coincides with KS for , surprisingly, we demonstrate that for  reconstruction is possible also below KS. This shows an interesting phase transition in the parameter , and suggests that for , there might be an information-computation gap for the HSBM. For  and large degree we propose an approach for showing non-reconstruction below KS, suggesting that  is the correct threshold for onset of the new phase.", "year": 2023, "venue": "", "authors": "Yuzhou Gu and Yury Polyanskiy"}, {"title": "Saliency cards: A framework to characterize and compare saliency methods", "abstract": "Saliency methods are a common class of machine learning interpretability techniques that calculate how important each input feature is to a model\u2019s output. We find that, with the rapid pace of development, users struggle to stay informed of the strengths and limitations of new methods and, thus, choose methods for unprincipled reasons (e.g., popularity). Moreover, despite a corresponding rise in evaluation metrics, existing approaches assume universal desiderata for saliency methods (e.g., faithfulness) that do not account for diverse user needs. In response, we introduce saliency cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics. Through a review of 25 saliency method papers and 33 method evaluations, we identify 10 attributes that users should account for when choosing a method. We group these attributes into three categories that \u2026", "year": 2023, "venue": "", "authors": "Angie Boggust and Harini Suresh and Hendrik Strobelt and John Guttag and Arvind Satyanarayan"}, {"title": "Dataset distillation fixes dataset reconstruction attacks", "abstract": "", "year": 2023, "venue": "", "authors": "Noel Loo and Ramin Hasani and Mathias Lechner and Daniela Rus"}, {"title": "Asynchronous massive access and neighbor discovery using OFDMA", "abstract": "The fundamental communication problem in the wireless Internet-of-Things (IoT) is to discover a massive number of devices and to provide them with reliable access to shared channels. Oftentimes these devices transmit short messages randomly and sporadically. This paper proposes a novel signaling scheme for grant-free massive access, where each device encodes its identity and/or information in a sparse set of tones. Such transmissions are implemented in the form of orthogonal frequency-division multiple access (OFDMA). Under some mild conditions and assuming device delays to be bounded unknown multiples of sampling intervals, sparse OFDMA is proved to enable arbitrarily reliable asynchronous device identification and message decoding with a codelength that is  , where   denotes the device population,   denotes the actual number of active devices, and   is essentially equal \u2026", "year": 2022, "venue": "", "authors": "Xu Chen and Lina Liu and Dongning Guo and Gregory W Wornell"}, {"title": "IcebergHT: High performance PMEM hash tables through stability and low associativity", "abstract": "Modern hash table designs strive to minimize space while maximizing speed. The most important factor in speed is the number of cache lines accessed during updates and queries. This is especially important on PMEM, which is slower than DRAM and in which writes are more expensive than reads. This paper proposes two stronger design objectives: stability and low-associativity. A stable hash table doesn't move items around, and a hash table has low associativity if there are only a few locations where an item can be stored. Low associativity ensures that queries need to examine only a few memory locations, and stability ensures that insertions write to very few cache lines. Stability also simplifies scaling and crash safety. We present IcebergHT, a fast, crash-safe, concurrent, and space-efficient hash table for PMEM based on the design principles of stability and low associativity. IcebergHT combines in-memory metadata with a new hashing technique, iceberg hashing, that is (1) space efficient, (2) stable, and (3) supports low associativity. In contrast, existing hash-tables either modify numerous cache lines during insertions (e.g. cuckoo hashing), access numerous cache lines during queries (e.g. linear probing), or waste space (e.g. chaining). Moreover, the combination of (1)-(3) yields several emergent benefits: IcebergHT scales better than other hash tables, supports crash-safety, and has excellent performance on PMEM (where writes are particularly expensive).", "year": 2022, "venue": "", "authors": "Prashant Pandey and Michael A Bender and Alex Conway and Mart\u00edn Farach-Colton and William Kuszmaul and Guido Tagliavini and Rob Johnson"}, {"title": "How asymmetry helps buffer management: achieving optimal tail size in cup games", "abstract": "The cup game on n cups is a multi-step game with two players, a filler and an emptier. At each step, the filler distributes 1 unit of water among the cups, and then the emptier selects a single cup to remove (up to) 1 unit of water from. There are several objective functions that the emptier might wish to minimize. One of the strongest guarantees would be to minimize tail size, which is defined to be the number of cups with fill 2 or greater. A simple lower-bound construction shows that the optimal tail size for deterministic emptying algorithms is \u0398(n), however. We present a simple randomized emptying algorithm that achieves tail size \u00d5(logn) with high probability in n for poly n steps. Moreover, we show that this is tight up to doubly logarithmic factors. We also extend our results to the multi-processor cup game, achieving tail size \u00d5(logn + p) on p processors with high probability in n. We show that the dependence on p is \u2026", "year": 2021, "venue": "", "authors": "William Kuszmaul"}, {"title": "Towards a benchmark for learned systems", "abstract": "This paper aims to initiate a discussion around benchmarking data management systems with machine-learned components. Traditional benchmarks such as TPC or YCSB are insufficient to analyze and understand these learned systems because they evaluate the performance under a stable workload and data distribution. Learned systems automatically specialize and adapt database components to a changing workload, database, and execution environment, thereby making conventional metrics such as average throughput ill-suited to understand their performance fully. Moreover, the standard cost-per-performance metrics fail to account for essential trade-offs related to the training cost of models and the elimination of manual database tuning. We present several ideas for designing new benchmarks that are better suited to evaluate learned systems. The main challenges entail developing new metrics to \u2026", "year": 2021, "venue": "", "authors": "Laurent Bindschaedler and Andreas Kipf and Tim Kraska and Ryan Marcus and Umar Farooq Minhas"}, {"title": "Polygeist: Affine c in mlir", "abstract": "We present Polygeist, a new tool that reroutes polyhedral compilation flows to use the representation available in the recent MLIR compilation infrastructure. It consists of two parts: a C and C++ frontend capable of converting a wide variety of existing codes into MLIR suitable for polyhedral transformation, and a bi-directional conversion between MLIR\u2019s polyhedral representation and existing polyhedral exchange formats. We demonstrate Polygeist\u2019s flow by converting the entire Polybench/C benchmark suite into MLIR, and by performing an IR-to-IR optimization leveraging an existing polyhedral compiler (Pluto). Our flow produces results within 1.25% of the state-of-the-art Clang compiler, enabling direct comparison of source-to-source and IR-to-binary compilers. We believe Polygeist can improve the interoperation between MLIR and the existing polyhedral tooling, benefiting both the research and the production compiler communities.", "year": 2021, "venue": "", "authors": "William S Moses and Lorenzo Chelini and Ruizhe Zhao and Oleksandr Zinenko"}, {"title": "Online List Labeling: Breaking the  Barrier", "abstract": "The online list-labeling problem is an algorithmic primitive with a large literature of upper bounds, lower bounds, and applications. The goal is to store a dynamically changing set of  items in an array of  slots, while maintaining the invariant that the items appear in sorted order and while minimizing the relabeling cost, defined to be the number of items that are moved per insertion/deletion. For the linear regime, where , an upper bound of  on the relabeling cost has been known since 1981. A lower bound of  is known for deterministic algorithms and for so-called smooth algorithms, but the best general lower bound remains . The central open question in the field is whether  is optimal for all algorithms. In this paper, we give a randomized data structure that achieves an expected relabeling cost of  per operation. More generally, if  for , the expected relabeling cost \u2026", "year": 2024, "venue": "", "authors": "Michael A Bender and Alex Conway and Mart\u00edn Farach-Colton and Hanna Koml\u00f3s and William Kuszmaul and Nicole Wein"}, {"title": "SeeSaw: interactive ad-hoc search over image databases", "abstract": "As image datasets become ubiquitous, the problem of ad-hoc searches over image data is increasingly important. Many high-level data tasks in machine learning, such as constructing datasets for training and testing object detectors, imply finding ad-hoc objects or scenes within large image datasets as a key sub-problem. New foundational visual-semantic embeddings trained on massive web datasets such as Contrastive Language-Image Pre-Training (CLIP) can help users start searches on their own data, but we find there is a long tail of queries where these models fall short in practice. Seesaw is a system for interactive ad-hoc searches on image datasets that integrates state-of-the-art embeddings like CLIP with user feedback in the form of box annotations to help users quickly locate images of interest in their data even in the long tail of harder queries. One key challenge for Seesaw is that, in practice, many \u2026", "year": 2023, "venue": "", "authors": "Oscar Moll and Manuel Favela and Samuel Madden and Vijay Gadepally and Michael Cafarella"}, {"title": "Uniqueness of BP fixed point for the Potts model and applications to community detection", "abstract": "In the study of sparse stochastic block models (SBMs) one often needs to analyze a distributional recursion, known as the belief propagation (BP) recursion. Uniqueness of the fixed point of this recursion implies several results about the SBM, including optimal recovery algorithms for SBM (Mossel et al.(2016)) and SBM with side information (Mossel and Xu (2016)), and a formula for SBM mutual information (Abbe et al.(2021)). The 2-community case corresponds to an Ising model, for which Yu and Polyanskiy (2022) established uniqueness for all cases. In this paper we analyze the -ary Potts model, ie, broadcasting of -ary spins on a Galton-Watson tree with expected offspring degree  through Potts channels with second-largest eigenvalue . We allow the intermediate vertices to be observed through noisy channels (side information). We prove that BP uniqueness holds with and without side information when  for some absolute constant  independent of . For large  and , this is asymptotically achieving the Kesten-Stigum threshold . These results imply mutual information formulas and optimal recovery algorithms for the -community SBM in the corresponding ranges. For , Sly (2011); Mossel et al.(2022) showed that there exist choices of  below Kesten-Stigum (ie ) but reconstruction is possible. Somewhat surprisingly, we show that in such regimes BP uniqueness does not hold at least in the presence of weak side information. Our technical tool is a theory of -ary symmetric channels, that we initiate here, generalizing the classical and widely-utilized information-theoretic \u2026", "year": 2023, "venue": "", "authors": "Yuzhou Gu and Yury Polyanskiy"}, {"title": "Understanding reconstruction attacks with the neural tangent kernel and dataset distillation", "abstract": "Modern deep learning requires large volumes of data, which could contain sensitive or private information that cannot be leaked. Recent work has shown for homogeneous neural networks a large portion of this training data could be reconstructed with only access to the trained network parameters. While the attack was shown to work empirically, there exists little formal understanding of its effective regime which datapoints are susceptible to reconstruction. In this work, we first build a stronger version of the dataset reconstruction attack and show how it can provably recover the \\emph{entire training set} in the infinite width regime. We then empirically study the characteristics of this attack on two-layer networks and reveal that its success heavily depends on deviations from the frozen infinite-width Neural Tangent Kernel limit. Next, we study the nature of easily-reconstructed images. We show that both theoretically and empirically, reconstructed images tend to \"outliers\" in the dataset, and that these reconstruction attacks can be used for \\textit{dataset distillation}, that is, we can retrain on reconstructed images and obtain high predictive accuracy.", "year": 2023, "venue": "", "authors": "Noel Loo and Ramin Hasani and Mathias Lechner and Alexander Amini and Daniela Rus"}, {"title": "Graphblas on the edge: Anonymized high performance streaming of network traffic", "abstract": "Long range detection is a cornerstone of defense in many operating domains (land, sea, undersea, air, space,\u2026,). In the cyber domain, long range detection requires the analysis of significant network traffic from a variety of observatories and outposts. Construction of anonymized hypersparse traffic matrices on edge network devices can be a key enabler by providing significant data compression in a rapidly analyzable format that protects privacy. GraphBLAS is ideally suited for both constructing and analyzing anonymized hypersparse traffic matrices. The performance of GraphBLAS on an Accolade Technologies edge network device is demonstrated on a near worse case traffic scenario using a continuous stream of CAIDA Telescope darknet packets. The performance for varying numbers of traffic buffers, threads, and processor cores is explored. Anonymized hypersparse traffic matrices can be constructed at a \u2026", "year": 2022, "venue": "", "authors": "Michael Jones and Jeremy Kepner and Daniel Andersen and Aydin Bulu\u00e7 and Chansup Byun and K Claffy and Timothy Davis and William Arcand and Jonathan Bernays and David Bestor and William Bergeron and Vijay Gadepally and Micheal Houle and Matthew Hubbell and Hayden Jananthan and Anna Klein and Chad Meiners and Lauren Milechin and Julie Mullen and Sandeep Pisharody and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Jon Sreekanth and Doug Stetson and Charles Yee and Peter Michaleas"}, {"title": "Learning stable classifiers by transferring unstable features", "abstract": "While unbiased machine learning models are essential for many applications, bias is a human-defined concept that can vary across tasks. Given only input-label pairs, algorithms may lack sufficient information to distinguish stable (causal) features from unstable (spurious) features. However, related tasks often share similar biases\u2013an observation we may leverage to develop stable classifiers in the transfer setting. In this work, we explicitly inform the target classifier about unstable features in the source tasks. Specifically, we derive a representation that encodes the unstable features by contrasting different data environments in the source task. We achieve robustness by clustering data of the target task according to this representation and minimizing the worst-case risk across these clusters. We evaluate our method on both text and image classifications. Empirical results demonstrate that our algorithm is able to maintain robustness on the target task for both synthetically generated environments and real-world environments. Our code is available at https://github. com/YujiaBao/Tofu.", "year": 2022, "venue": "", "authors": "Yujia Bao and Shiyu Chang and Regina Barzilay"}, {"title": "Discrete approximate information states in partially observable environments", "abstract": "The notion of approximate information states (AIS) was introduced in [1] as a methodology for learning task-relevant state representations for control in partially observable systems. They proposed particular learning objectives which attempt to reconstruct the cost and next state and provide a bound on the suboptimality of the closed-loop performance, but it is unclear whether these bounds are tight or actually lead to good performance in practice. Here we study this methodology by examining the special case of discrete approximate information states (DAIS). In this setting, we can solve for the globally optimal policy using value iteration for the DAIS model, allowing us to disambiguate the performance of the AIS objective from the policy search. Going further, for small problems with finite information states, we reformulate the DAIS learning problem as a novel mixed-integer program (MIP) and solve it to its global \u2026", "year": 2022, "venue": "", "authors": "Lujie Yang and Kaiqing Zhang and Alexandre Amice and Yunzhu Li and Russ Tedrake"}, {"title": "Multimodal physiological monitoring during virtual reality piloting tasks", "abstract": "This dataset includes multimodal physiologic, flight performance, and user interaction data streams, collected as participants performed virtual flight tasks of varying difficulty. In virtual reality, individuals flew an\" Instrument Landing System\"(ILS) protocol, in which they had to land an aircraft mostly relying on the cockpit instrument readings. Participants were presented with four levels of difficulty, which were generated by varying wind speed, turbulence, and visibility. Each of the participants performed 12 runs, split into 3 blocks of four consecutive runs, one run at each difficulty, in a single experimental session. The sequence of difficulty levels was presented in a counterbalanced manner across blocks. Flight performance was quantified as a function of horizontal and vertical deviation from an ideal path towards the runway as well as deviation from the prescribed ideal speed of 115 knots. Multimodal physiological signals were aggregated and synchronized using Lab Streaming Layer. Descriptions of data quality are provided to assess each data stream. The starter code provides examples of loading and plotting the time synchronized data streams, extracting sample features from the eye tracking data, and building models to predict pilot performance from the physiology data streams.", "year": 2022, "venue": "", "authors": "Hrishikesh Rao and Emilie Cowen and Sophia Yuditskaya and Laura Brattain and Jamie Koerner and Gregory Ciccarelli and Ronisha Carter and Vivienne Sze and Tamara Broderick and Hayley Reynolds and Kyle McAlpin and Thomas Heldt"}, {"title": "SAR-to-EO image translation with multi-conditional adversarial networks", "abstract": "This paper explores the use of multi-conditional adversarial networks for SAR-to-EO image translation. Previous methods condition adversarial networks only on the input SAR. We show that by incorporating multiple complementary modalities such as Google maps and IR can further improve SAR-to-EO image translation especially on preserving sharp edges of manmade objects. We demonstrate effectiveness of our approach on a diverse set of datasets including SEN12MS, DFC2020, and SpaceNet6. Our experimental results suggest that additional information provided by complementary modalities improves the performance of SAR-to-EO image translation compared to the models trained on paired SAR and EO data only. To best of our knowledge, our approach is the first to leverage multiple modalities for improving SAR-to-EO image translation performance.", "year": 2021, "venue": "", "authors": "Armando Cabrera and Miriam Cha and Prafull Sharma and Michael Newey"}, {"title": "Closing the gap between cache-oblivious and cache-adaptive analysis", "abstract": "Cache-adaptive analysis was introduced to analyze the performance of an algorithm when the cache (or internal memory) available to the algorithm dynamically changes size. These memory-size fluctuations are, in fact, the common case in multi-core machines, where threads share cache and RAM. An algorithm is said to be efficiently cache-adaptive if it achieves optimal utilization of the dynamically changing cache. Cache-adaptive analysis was inspired by cache-oblivious analysis. Many (or even most) optimal cache-oblivious algorithms have an -regular recursive structure. Such -regular algorithms include Longest Common Subsequence, All Pairs Shortest Paths, Matrix Multiplication, Edit Distance, Gaussian Elimination Paradigm, etc. Bender et al. (2016) showed that some of these optimal cache-oblivious algorithms remain optimal even when cache changes size dynamically, but that in general \u2026", "year": 2020, "venue": "", "authors": "Michael A Bender and Rezaul A Chowdhury and Rathish Das and Rob Johnson and William Kuszmaul and Andrea Lincoln and Quanquan C Liu and Jayson Lynch and Helen Xu"}, {"title": "The data-driven radio frequency signal separation challenge", "abstract": "The radio-frequency (RF) signal separation challenge involves recovering a signal-of-interest (SOI) from a super-imposed co-channel interference signal. The SOI is a digital communication waveform of known modulation, pulse-shape, timing, etc. The interferer is unknown and must be learned from data. Submissions featured a blend of signal processing strategies, leveraging RF-specific domain knowledge and novel neural network architectures with careful hyperparameter selection/optimization. The resulting solutions establish new benchmarks for data-driven RF modeling and interference cancellation.", "year": 2024, "venue": "", "authors": "Tejas Jayashankar and Binoy Kurien and Alejandro Lancho and Gary CF Lee and Yury Polyanskiy and Amir Weiss and Gregory W Wornell"}, {"title": "Score-based source separation with applications to digital communication signals", "abstract": "We propose a new method for separating superimposed sources using diffusion-based generative models. Our method relies only on separately trained statistical priors of independent sources to establish a new objective function guided by $\\textit {maximum a posteriori} $ estimation with an $\\textit {$\\alpha $-posterior} $, across multiple levels of Gaussian smoothing. Motivated by applications in radio-frequency (RF) systems, we are interested in sources with underlying discrete nature and the recovery of encoded bits from a signal of interest, as measured by the bit error rate (BER). Experimental results with RF mixtures demonstrate that our method results in a BER reduction of 95\\% over classical and existing learning-based methods. Our analysis demonstrates that our proposed method yields solutions that asymptotically approach the modes of an underlying discrete distribution. Furthermore, our method can be viewed as a multi-source extension to the recently proposed score distillation sampling scheme, shedding additional light on its use beyond conditional sampling. The project webpage is available at https://alpha-rgs. github. io.", "year": 2023, "venue": "", "authors": "Tejas Jayashankar and Gary CF Lee and Alejandro Lancho and Amir Weiss and Yury Polyanskiy and Gregory Wornell"}, {"title": "Continuous deep equilibrium models: Training neural odes faster by integrating them to infinity", "abstract": "Implicit models separate the definition of a layer from the description of its solution process. While implicit layers allow features such as depth to adapt automatically to new scenarios and inputs, this adaptivity makes its computational expense challenging to predict. In this manuscript, we increase the \u201cimplicitness\u201d of the DEQ by redefining the method in terms of an infinite time neural ODE, which paradoxically decreases the training cost over a standard neural ODE by . Additionally, we address the question: is there a way to simultaneously achieve the robustness of implicit layers while allowing the reduced computational expense of an explicit layer? To solve this, we develop Skip and Skip Reg. DEQ, an implicit-explicit (IMEX) layer that simultaneously trains an explicit prediction followed by an implicit correction. We show that training this explicit predictor is free and even decreases the training time by \u2026", "year": 2023, "venue": "", "authors": "Avik Pal and Alan Edelman and Chris Rackauckas"}, {"title": "Geosynchronous satellite pattern of life node detection and classification", "abstract": "Differentiating nominal and anomalous satellite behaviors is a challenging aspect of Space Situational Awareness (SSA) due to the general lack of transparency in satellite operations. Although geosynchronous (GEO) satellites typically adhere to station-keeping routines to maintain their Earth-relative position, these schedules are highly variable due to their dependence on guidance protocols and control metrics specific to each payload. For example, the periodicity of GEO station-keeping can change even between satellites with the same operator and bus, and control metrics can change for individual satellites over time. This diversity complicates the process of characterizing control objectives and recognizing the early signs of a shift in a satellite\u2019s pattern of life (PoL) since some understanding of a satellite\u2019s nominal behavior is necessary to identify abnormal behavior. Broader classification of GEO behavioral modes may help to simplify these problems. In this paper, PoLs are modeled as a sequence of behavioral modes separated by nodes that represent changes in a satellite\u2019s control objectives. This work presents an algorithmic approach for detecting nodes from geodetic coordinates and cataloging them according to a discrete class system. This system was formulated from a subject-matter-expert analysis of satellite geographic positional data that was generated from historical two-line elements (TLEs). The algorithm is applied to 18 GEO satellites in the the United States Space Force\u2019s (USSF) 18th Space Control Squadron (18 SpCS) catalog, and the conformity of the results to the defined node classifications is discussed. Case studies \u2026", "year": 2023, "venue": "", "authors": "Haley E Solera and Thomas G Roberts and Richard Linares"}, {"title": "A hash table without hash functions, and how to get the most out of your random bits", "abstract": "This paper considers the basic question of how strong of a probabilistic guarantee can a hash table, storing -bit key/value pairs, offer? Past work on this question has been bottlenecked by limitations of the known families of hash functions: The only hash tables to achieve failure probabilities less than  require access to fully-random hash functions-if the same hash tables are implemented using the known explicit families of hash functions, their failure probabilities become . To get around these obstacles, we show how to construct a randomized data structure that has the same guarantees as a hash table, but that avoids the direct use of hash functions. Building on this, we able to construct a hash table using  random bits that achieves failure probability  for an arbitrary positive constant . In fact, we show that this guarantee can even be achieved by a succinct dictionary, that is, by a \u2026", "year": 2022, "venue": "", "authors": "William Kuszmaul"}, {"title": "On the importance of calibration in semi-supervised learning", "abstract": "State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful in leveraging a mix of labeled and unlabeled data by combining techniques of consistency regularization and pseudo-labeling. During pseudo-labeling, the model's predictions on unlabeled data are used for training and thus, model calibration is important in mitigating confirmation bias. Yet, many SOTA methods are optimized for model performance, with little focus directed to improve model calibration. In this work, we empirically demonstrate that model calibration is strongly correlated with model performance and propose to improve calibration via approximate Bayesian techniques. We introduce a family of new SSL models that optimizes for calibration and demonstrate their effectiveness across standard vision benchmarks of CIFAR-10, CIFAR-100 and ImageNet, giving up to 15.9% improvement in test accuracy. Furthermore, we also demonstrate their effectiveness in additional realistic and challenging problems, such as class-imbalanced datasets and in photonics science.", "year": 2022, "venue": "", "authors": "Charlotte Loh and Rumen Dangovski and Shivchander Sudalairaj and Seungwook Han and Ligong Han and Leonid Karlinsky and Marin Soljacic and Akash Srivastava"}, {"title": "DelayDiffEq: Generating delay differential equation solvers via recursive embedding of ordinary differential equation solvers", "abstract": "Traditional solvers for delay differential equations (DDEs) are designed around only a single method and do not effectively use the infrastructure of their more-developed ordinary differential equation (ODE) counterparts. In this work we present DelayDiffEq, a Julia package for numerically solving delay differential equations (DDEs) which leverages the multitude of numerical algorithms in OrdinaryDiffEq for solving both stiff and non-stiff ODEs, and manages to solve challenging stiff DDEs. We describe how compiling the ODE integrator within itself, and accounting for discontinuity propagation, leads to a design that is effective for DDEs while using all of the ODE internals. We highlight some difficulties that a numerical DDE solver has to address, and explain how DelayDiffEq deals with these problems. We show how DelayDiffEq is able to solve difficult equations, how its stiff DDE solvers give efficiency on problems with time-scale separation, and how the design allows for generality and flexibility in usage such as being repurposed for generating solvers for stochastic delay differential equations.", "year": 2022, "venue": "", "authors": "David Widmann and Chris Rackauckas"}, {"title": "Exploiting temporal structures of cyclostationary signals for data-driven single-channel source separation", "abstract": "We study the problem of single-channel source separation (SCSS), and focus on cyclostationary signals, which are particularly suitable in a variety of application domains. Unlike classical SCSS approaches, we consider a setting where only examples of the sources are available rather than their models, inspiring a data-driven approach. For source models with underlying cyclostationary Gaussian constituents, we establish a lower bound on the attainable mean-square-error (MSE) for any separation method, model-based or data-driven. Our analysis further reveals the operation for optimal separation and the associated implementation challenges. As a computationally attractive alternative, we propose a deep learning approach using a U-Net architecture, which is competitive with the minimum MSE estimator. We demonstrate in simulation that, with suitable domain-informed architectural choices, our U-Net method \u2026", "year": 2022, "venue": "", "authors": "Gary CF Lee and Amir Weiss and Alejandro Lancho and Jennifer Tang and Yuheng Bu and Yury Polyanskiy and Gregory W Wornell"}, {"title": "Incorporating rich social interactions into mdps", "abstract": "Much of what we do as humans is engage socially with other agents, a skill that robots must also eventually possess. We demonstrate that a rich theory of social interactions originating from microsociology can be formalized by extending a nested MDP where agents reason about arbitrary functions of each other's rewards. This extended Social MDP allows us to encode the five basic interactions that underlie microsociology: cooperation, conflict, coercion, competition, and exchange. The result is a robotic agent capable of executing social interactions in new environments with no interaction-specific training; like humans it can engage socially in novel ways even without a single example of that social interaction. Moreover, the estimations of these Social MDPs align closely with the judge-ments of humans when considering which social interaction is taking place in an environment. This method both sheds light on \u2026", "year": 2022, "venue": "", "authors": "Ravi Tejwani and Yen-Ling Kuo and Tianmin Shu and Bennett Stankovits and Dan Gutfreund and Joshua B Tenenbaum and Boris Katz and Andrei Barbu"}, {"title": "Realizing forward defense in the cyber domain", "abstract": "With the recognition of cyberspace as an operating domain, concerted effort is now being placed on addressing it in the whole-of-domain manner found in land, sea, undersea, air, and space domains. Among the first steps in this effort is applying the standard supporting concepts of security, defense, and deterrence to the cyber domain. This paper presents an architecture that helps realize forward defense in cyberspace, wherein adversarial actions are repulsed as close to the origin as possible. However, substantial work remains in making the architecture an operational reality including furthering fundamental research cyber science, conducting design trade-off analysis, and developing appropriate public policy frameworks.", "year": 2021, "venue": "", "authors": "Sandeep Pisharody and Jonathan Bernays and Vijay Gadepally and Michael Jones and Jeremy Kepner and Chad Meiners and Peter Michaleas and Adam Tse and Doug Stetson"}, {"title": "Spectral pinns: Fast uncertainty propagation with physics-informed neural networks", "abstract": "Physics-informed neural networks (PINNs) promise to significantly speed up partial differential equation (PDE) solvers. However, most PINNs can only solve deterministic PDEs. Here, we consider \\textit{stochastic} PDEs that contain partially unknown parameters. We aim to quickly quantify the impact of uncertain parameters onto the solution of a PDE - that is - we want to perform fast uncertainty propagation. Classical uncertainty propagation methods such as Monte Carlo sampling, stochastic Galerkin, collocation, or discrete projection methods become computationally too expensive with an increasing number of stochastic parameters. For example, the well-known spectral or polynomial chaos expansions achieve to separate the spatiotemporal and probabilistic domains and offer theoretical guarantees and fast computation of stochastic summaries (e.g., mean), but can be computationally expensive to form. Our Spectral-PINNs approximate the underlying spectral coefficients with a neural network and reduce the computational cost of the spectral expansion while maintaining guarantees. We derive the method for partial differential equations, discuss runtime, demonstrate initial results on the convection-diffusion equation, and provide steps towards convergence guarantees.", "year": 2021, "venue": "", "authors": "Bj\u00f6rn L\u00fctjens and Catherine H Crawford and Mark Veillette and Dava Newman"}, {"title": "Fast mapping onto census blocks", "abstract": "Pandemic measures such as social distancing and contact tracing can be enhanced by rapidly integrating dynamic location data and demographic data. Projecting billions of longitude and latitude locations onto hundreds of thousands of highly irregular demographic census block polygons is computationally challenging in both research and deployment contexts. This paper describes two approaches labeled \u201csimple\u201d and \u201cfast\u201d. The simple approach can be implemented in any scripting language (Matlab/Octave, Python, Julia, R) and is easily integrated and customized to a variety of research goals. This simple approach uses a novel combination of hierarchy, sparse bounding boxes, polygon crossing-number, vectorization, and parallel processing to achieve 100,000,000+ projections per second on 100 servers. The simple approach is compact, does not increase data storage requirements, and is applicable to any \u2026", "year": 2020, "venue": "", "authors": "Jeremy Kepner and Andreas Kipf and Darren Engwirda and Navin Vembar and Michael Jones and Lauren Milechin and Vijay Gadepally and Chris Hill and Tim Kraska and William Arcand and David Bestor and William Bergeron and Chansup Byun and Matthew Hubbell and Michael Houle and Andrew Kirby and Anna Klein and Julie Mullen and Andrew Prout and Albert Reuther and Antonio Rosa and Sid Samsi and Charles Yee and Peter Michaleas"}, {"title": "AutoKG: Constructing virtual knowledge graphs from unstructured documents for question answering", "abstract": "Knowledge graphs (KGs) have the advantage of providing fine-grained detail for question-answering systems. Unfortunately, building a reliable KG is time-consuming and expensive as it requires human intervention. To overcome this issue, we propose a novel framework to automatically construct a KG from unstructured documents that does not require external alignment. We first extract surface-form knowledge tuples from unstructured documents and encode them with contextual information. Entities with similar context semantics are then linked through internal alignment to form a graph structure. This allows us to extract the desired information from multiple documents by traversing the generated KG without a manual process. We examine its performance in retrieval based QA systems by reformulating the WikiMovies and MetaQA datasets into a tuple-level retrieval task. The experimental results show that our method outperforms traditional retrieval methods by a large margin.", "year": 2020, "venue": "", "authors": "Seunghak Yu and Tianxing He and James Glass"}, {"title": "Some new results on the maximum growth factor in Gaussian elimination", "abstract": "This paper combines modern numerical computation with theoretical results to improve our understanding of the growth factor problem for Gaussian elimination. On the computational side we obtain lower bounds for the maximum growth for complete pivoting for  and  using the Julia JuMP optimization package. At  we obtain a growth factor bigger than . The numerical evidence suggests that the maximum growth factor is bigger than  if and only if . We also present a number of theoretical results. We show that the maximum growth factor over matrices with entries restricted to a subset of the reals is nearly equal to the maximum growth factor over all real matrices. We also show that the growth factors under floating point arithmetic and exact arithmetic are nearly identical. Finally, through numerical search, and stability and extrapolation results, we provide improved lower bounds for the maximum growth factor \u2026", "year": 2024, "venue": "", "authors": "Alan Edelman and John Urschel"}, {"title": "AI-assisted analysis of content, structure, and sentiment in MOOC discussion forums", "abstract": "Discussion forums are a key component of online learning platforms, allowing learners to ask for help, provide help to others, and connect with others in the learning community. Analyzing patterns of forum usage and their association with course outcomes can provide valuable insight into how learners actually use discussion forums, and suggest strategies for shaping forum dynamics to improve learner experiences and outcomes. However, the fine-grained coding of forum posts required for this kind of analysis is a manually intensive process that can be challenging for large datasets, e.g., those that result from popular MOOCs. To address this issue, we propose an AI-assisted labeling process that uses advanced natural language processing techniques to train machine learning models capable of labeling a large dataset while minimizing human annotation effort. We fine-tune pretrained transformer-based deep learning models on category, structure, and emotion classification tasks. The transformer-based models outperform a more traditional baseline that uses support vector machines and a bag-of-words input representation. The transformer-based models also perform better when we augment the input features for an individual post with additional context from the post's thread (e.g., the thread title). We validate model quality through a combination of internal performance metrics, human auditing, and common-sense checks. For our Python MOOC dataset, we find that annotating approximately 1% of the forum posts achieves performance levels that are reliable for downstream analysis. Using labels from the validated AI models, we \u2026", "year": 2023, "venue": "", "authors": "Michael Yee and Anindya Roy and Meghan Perdue and Consuelo Cuevas and Keegan Quigley and Ana Bell and Ahaan Rungta and Shigeru Miyagawa"}, {"title": "Ai ssa challenge problem: Satellite pattern-of-life characterization dataset and benchmark suite", "abstract": "With the increasing number of resident space objects (RSOs), it has become imperative to enhance tracking and orbit prediction capabilities to safeguard space assets from the threat of object-on-object collision. One effective approach to achieve this goal is by characterizing active satellites\u2019 patterns of life (PoL), or the sequences of behavioral modes\u2014periods of consistent on-orbit behavior, such as those in which satellites adhere to various station-keeping protocols\u2014that they pursue throughout their operational lifetimes. Understanding a satellite\u2019s PoL can contribute to better orbit prediction and improved tracking capabilities. In this work, a novel benchmarking tool for geosynchronous satellite pattern-of-life characterization was created to compare different AI approaches developed by independent research teams that push the boundary of this problem. To this end, the author team has developed a labeled Satellite Patternof-Life Identification Dataset (SPLID) containing astrometric data and satellites\u2019 historical behavioral modes with which AI-enabled algorithms can characterize satellite PoLs. The SPLID dataset consists of synthetic space object data, true space object data generated from Vector Covariance Messages (VCMs), and true space object data generated from high-accuracy ephemerides provided by satellite owner-operators.In addition, this paper introduces a new AI SSA challenge competition that will be held in conjunction with the release of the SPLID dataset, where participants are tasked to build models that accurately label and time stamp the behavioral modes of GEO satellites over a six-month period. The satellite PoL \u2026", "year": 2023, "venue": "", "authors": "Peng Mun Siew and Haley E Solera and Thomas G Roberts and Daniel Jang and Victor Rodriguez-Fernandez and Jonathan P How and Richard Linares"}, {"title": "Automatic debiased machine learning for covariate shifts", "abstract": "In this paper we address the problem of bias in machine learning of parameters following covariate shifts. Covariate shift occurs when the distribution of input features change between the training and deployment stages. Regularization and model selection associated with machine learning biases many parameter estimates. In this paper, we propose an automatic debiased machine learning approach to correct for this bias under covariate shifts. The proposed approach leverages state-of-the-art techniques in debiased machine learning to debias estimators of policy and causal parameters when covariate shift is present. The debiasing is automatic in only relying on the parameter of interest and not requiring the form of the form of the bias. We show that our estimator is asymptotically normal as the sample size grows. Finally, we demonstrate the proposed method on a regression problem using a Monte-Carlo simulation.", "year": 2023, "venue": "", "authors": "Victor Chernozhukov and Michael Newey and Whitney K Newey and Rahul Singh and Vasilis Srygkanis"}, {"title": "On the forward invariance of neural odes", "abstract": "We propose a new method to ensure neural ordinary differential equations (ODEs) satisfy output specifications by using invariance set propagation. Our approach uses a class of control barrier functions to transform output specifications into constraints on the parameters and inputs of the learning system. This setup allows us to achieve output specification guarantees simply by changing the constrained parameters/inputs both during training and inference. Moreover, we demonstrate that our invariance set propagation through data-controlled neural ODEs not only maintains generalization performance but also creates an additional degree of robustness by enabling causal manipulation of the system\u2019s parameters/inputs. We test our method on a series of representation learning tasks, including modeling physical dynamics and convexity portraits, as well as safe collision avoidance for autonomous vehicles.", "year": 2023, "venue": "", "authors": "Wei Xiao and Tsun-Hsuan Wang and Ramin Hasani and Mathias Lechner and Yutong Ban and Chuang Gan and Daniela Rus"}, {"title": "Satellite navigation and coordination with limited information sharing", "abstract": "We explore space traffic management as an application of collision-free navigation in multi-agent systems where vehicles have limited observation and communication ranges. We investigate the effectiveness of transferring a collision avoidance multi-agent reinforcement (MARL) model trained on a ground environment to a space one. We demonstrate that the transfer learning model outperforms a model that is trained directly on the space environment. Furthermore, we find that our approach works well even when we consider the perturbations to satellite dynamics caused by the Earth\u2019s oblateness. Finally, we show how our methods can be used to evaluate the benefits of information-sharing between satellite operators in order to improve coordination.", "year": 2023, "venue": "", "authors": "Sydney Dolan and Siddharth Nayak and Hamsa Balakrishnan"}, {"title": "Deep learning on home drone: Searching for the optimal architecture", "abstract": "We suggest the first system that runs real-time semantic segmentation via deep learning on the weak microcomputer Raspberry Pi Zero v2 (whose price was $15) attached to a toy drone. In particular, since the Raspberry Pi weighs less than 16 grams, and its size is half of a credit card, we could easily attach it to the common commercial DJI Tello toy-drone (<$100, <90 grams, 98 $\\times 92.5\\times 41$ mm). The result is an autonomous drone (no laptop nor human in the loop) that can detect and classify objects in real-time from a video stream of an onboard monocular RGB camera (no GPS or LIDAR sensors). The companion videos demonstrate how this Tello drone scans the lab for people (e.g. for the use of firefighters or security forces) and for an empty parking slot outside the lab. Existing deep learning solutions are either much too slow for real-time computation on such IoT devices, or provide results of \u2026", "year": 2023, "venue": "", "authors": "Alaa Maalouf and Yotam Gurfinkel and Barak Diker and Oren Gal and Daniela Rus and Dan Feldman"}, {"title": "Derivation and extensions of the tolles-lawson model for aeromagnetic compensation", "abstract": "This note is intended to serve as a straightforward reference that summarizes and expands on the linear aeromagnetic compensation model first introduced by Tolles and Lawson in 1950. The Tolles-Lawson model provides a simple, physical representation of an aircraft's magnetic field, composed of permanent, induced, and eddy current terms, and applies an approximation (a Taylor expansion) to enable fitting coefficients with a general linear model. Here, the Tolles-Lawson model is derived, paying stricter attention to where assumptions are made, the model calibration procedure is described, and some additional comments on a second-order correction and a means of constructing the vector aircraft field are provided.", "year": 2022, "venue": "", "authors": "Albert R Gnadt and Allan B Wollaber and Aaron P Nielsen"}, {"title": "NICE: Robust scheduling through reinforcement learning-guided integer programming", "abstract": "Integer programs provide a powerful abstraction for representing a wide range of real-world scheduling problems. Despite their ability to model general scheduling problems, solving large-scale integer programs (IP) remains a computational challenge in practice. The incorporation of more complex objectives such as robustness to disruptions further exacerbates the computational challenge. We present NICE (Neural network IP Coefficient Extraction), a novel technique that combines reinforcement learning and integer programming to tackle the problem of robust scheduling. More specifically, NICE uses reinforcement learning to approximately represent complex objectives in an integer programming formulation. We use NICE to determine assignments of pilots to a flight crew schedule so as to reduce the impact of disruptions. We compare NICE with (1) a baseline integer programming formulation that produces a feasible crew schedule, and (2) a robust integer programming formulation that explicitly tries to minimize the impact of disruptions. Our experiments show that, across a variety of scenarios, NICE produces schedules resulting in 33% to 48% fewer disruptions than the baseline formulation. Moreover, in more severely constrained scheduling scenarios in which the robust integer program fails to produce a schedule within 90 minutes, NICE is able to build robust schedules in less than 2 seconds on average.", "year": 2022, "venue": "", "authors": "Luke Kenworthy and Siddharth Nayak and Christopher Chin and Hamsa Balakrishnan"}, {"title": "LSI: a learned secondary index structure", "abstract": "Learned index structures have been shown to achieve favorable lookup performance and space consumption compared to their traditional counterparts such as B-trees. However, most learned index studies have focused on the primary indexing setting, where the base data is sorted. In this work, we investigate whether learned indexes sustain their advantage in the secondary indexing setting. We introduce Learned Secondary Index (LSI), a first attempt to use learned indexes for indexing unsorted data. LSI works by building a learned index over a permutation vector, which allows binary search to performed on the unsorted base data using random access. We additionally augment LSI with a fingerprint vector to accelerate equality lookups. We show that LSI achieves comparable lookup performance to state-of-the-art secondary indexes while being up to 6x more space efficient.", "year": 2022, "venue": "", "authors": "Andreas Kipf and Dominik Horn and Pascal Pfeil and Ryan Marcus and Tim Kraska"}, {"title": "Differential methods for assessing sensitivity in biological models", "abstract": "Differential sensitivity analysis is indispensable in fitting parameters, understanding uncertainty, and forecasting the results of both thought and lab experiments. Although there are many methods currently available for performing differential sensitivity analysis of biological models, it can be difficult to determine which method is best suited for a particular model. In this paper, we explain a variety of differential sensitivity methods and assess their value in some typical biological models. First, we explain the mathematical basis for three numerical methods: adjoint sensitivity analysis, complex perturbation sensitivity analysis, and forward mode sensitivity analysis. We then carry out four instructive case studies. (a) The CARRGO model for tumor-immune interaction highlights the additional information that differential sensitivity analysis provides beyond traditional naive sensitivity methods, (b) the deterministic SIR model demonstrates the value of using second-order sensitivity in refining model predictions, (c) the stochastic SIR model shows how differential sensitivity can be attacked in stochastic modeling, and (d) a discrete birth-death-migration model illustrates how the complex perturbation method of differential sensitivity can be generalized to a broader range of biological models. Finally, we compare the speed, accuracy, and ease of use of these methods. We find that forward mode automatic differentiation has the quickest computational time, while the complex perturbation method is the simplest to implement and the most generalizable.", "year": 2022, "venue": "", "authors": "Rachel Mester and Alfonso Landeros and Chris Rackauckas and Kenneth Lange"}, {"title": "How COVID-19 affected computer science MOOC learner behavior and achievements: A demographic study", "abstract": "Learner data from multiple instances of two introductory Python MOOCs, offered before and during the COVID-19 pandemic, were analyzed to see if and how learner behavior and overall learning outcomes changed during the pandemic. We found a surge in interest during the pandemic, but the certification rate fell short of the pre-pandemic average. Moreover, this drop in certification rate is more noticeable for the advanced course in the sequence, and is more pronounced in groups traditionally underrepresented in computer-science and coding communities. Additionally, we analyzed learners' interaction with one another on the course discussion forum, and performed a sentiment analysis of the forum comments using natural language processing, which revealed behavioral trends that differed significantly during the pandemic.", "year": 2022, "venue": "", "authors": "Anindya Roy and Michael Yee and Meghan Perdue and Julius Stein and Ana Bell and Ronisha Carter and Shigeru Miyagawa"}, {"title": "Hybrid power-law models of network traffic", "abstract": "The availability of large scale streaming network data has reinforced the ubiquity of power-law distributions in observations and enabled precision measurements of the distribution parameters. The increased accuracy of these measurements allows new underlying generative network models to be explored. The preferential attachment model is a natural starting point for these models. This work adds additional model components to account for observed phenomena in the distributions. In this model, preferential attachment is supplemented to provide a more accurate theoretical model of network traffic. Specifically, a probabilistic complex network model is proposed using preferential attachment as well as additional parameters to describe the newly observed prevalence of leaves and unattached nodes. Example distributions from this model are generated by considering random sampling of the networks created by \u2026", "year": 2021, "venue": "", "authors": "Pat Devlin and Jeremy Kepner and Ashley Luo and Erin Meger"}, {"title": "Binary dynamic time warping in linear time", "abstract": "Dynamic time warping distance (DTW) is a widely used distance measure between time series . It was shown by Abboud, Backurs, and Williams that in the \\emph{binary case}, where , DTW can be computed in time . We improve this running time . Moreover, if  and  are run-length encoded, then there is an algorithm running in time , where  and  are the number of runs in  and , respectively. This improves on the previous best bound of  due to Dupont and Marteau.", "year": 2021, "venue": "", "authors": "William Kuszmaul"}, {"title": "The variable-processor cup game", "abstract": "The problem of scheduling tasks on  processors so that no task ever gets too far behind is often described as a game with cups and water. In the -processor cup game on  cups, there are two players, a filler and an emptier, that take turns adding and removing water from a set of  cups. In each turn, the filler adds  units of water to the cups, placing at most  unit of water in each cup, and then the emptier selects  cups to remove up to  unit of water from. The emptier's goal is to minimize the backlog, which is the height of the fullest cup. The -processor cup game has been studied in many different settings, dating back to the late 1960's. All of the past work shares one common assumption: that  is fixed. This paper initiates the study of what happens when the number of available processors  varies over time, resulting in what we call the \\emph{variable-processor cup game}. Remarkably, the optimal bounds for the variable-processor cup game differ dramatically from its classical counterpart. Whereas the -processor cup has optimal backlog , the variable-processor game has optimal backlog . Moreover, there is an efficient filling strategy that yields backlog  in quasi-polynomial time against any deterministic emptying strategy. We additionally show that straightforward uses of randomization cannot be used to help the emptier. In particular, for any positive constant , and any -greedy-like randomized emptying algorithm , there is a filling strategy that achieves backlog  against  in quasi-polynomial time.", "year": 2020, "venue": "", "authors": "William Kuszmaul and Alek Westover"}, {"title": "EcoLife: Carbon-Aware Serverless Function Scheduling for Sustainable Computing", "abstract": "This work introduces ECOLIFE, the first carbon-aware serverless function scheduler to co-optimize carbon footprint and performance. ECOLIFE builds on the key insight of intelligently exploiting multi-generation hardware to achieve high performance and lower carbon footprint. ECOLIFE designs multiple novel extensions to Particle Swarm Optimization (PSO) in the context of serverless execution environment to achieve high performance while effectively reducing the carbon footprint.", "year": 2024, "venue": "", "authors": "Yankai Jiang and Rohan Basu Roy and Baolin Li and Devesh Tiwari"}, {"title": "Revealing vision-language integration in the brain with multimodal networks", "abstract": "We use (multi)modal deep neural networks (DNNs) to probe for sites of multimodal integration in the human brain by predicting stereoen-cephalography (SEEG) recordings taken while human subjects watched movies. We operationalize sites of multimodal integration as regions where a multimodal vision-language model predicts recordings better than unimodal language, unimodal vision, or linearly-integrated language-vision models. Our target DNN models span different architectures (e.g., convolutional networks and transformers) and multimodal training techniques (e.g., cross-attention and contrastive learning). As a key enabling step, we first demonstrate that trained vision and language models systematically outperform their randomly initialized counterparts in their ability to predict SEEG signals. We then compare unimodal and multimodal models against one another. Because our target DNN models often \u2026", "year": 2024, "venue": "", "authors": "Vighnesh Subramaniam and Colin Conwell and Christopher Wang and Gabriel Kreiman and Boris Katz and Ignacio Cases and Andrei Barbu"}, {"title": "TENG: Time-evolving natural gradient for solving PDEs with deep neural nets toward machine precision", "abstract": "Partial differential equations (PDEs) are instrumental for modeling dynamical systems in science and engineering. The advent of neural networks has initiated a significant shift in tackling these complexities though challenges in accuracy persist, especially for initial value problems. In this paper, we introduce the $\\textit{Time-Evolving Natural Gradient (TENG)}$, generalizing time-dependent variational principles and optimization-based time integration, leveraging natural gradient optimization to obtain high accuracy in neural-network-based PDE solutions. Our comprehensive development includes algorithms like TENG-Euler and its high-order variants, such as TENG-Heun, tailored for enhanced precision and efficiency. TENG's effectiveness is further validated through its performance, surpassing current leading methods and achieving $\\textit{machine precision}$ in step-by-step optimizations across a spectrum of PDEs, including the heat equation, Allen-Cahn equation, and Burgers' equation.", "year": 2024, "venue": "", "authors": "Zhuo Chen and Jacob McCarran and Esteban Vizcaino and Marin Solja\u010di\u0107 and Di Luo"}, {"title": "IcebergHT: high performance hash tables through stability and low associativity", "abstract": "Modern hash table designs for DRAM and PMEM strive to minimize space while maximizing speed. The most important factor in speed is the number of cache lines accessed during updates and queries. On PMEM, there is an additional consideration, which is to minimize the number of writes, because on PMEM writes are more expensive than reads. This paper proposes two design objectives, stability and low-associativity, that enable us to build hash tables that minimize cache-line accesses for all operations. A hash table is stable if it does not move items around, and a hash table has low associativity if there are only a few locations where an item can be stored. Low associativity ensures that queries need to examine only a few memory locations, and stability ensures that insertions write to very few cache lines. Stability also simplifies concurrency and, on PMEM, crash safety. We present IcebergHT, a fast \u2026", "year": 2023, "venue": "", "authors": "Prashant Pandey and Michael A Bender and Alex Conway and Martin Farach-Colton and William Kuszmaul and Guido Tagliavini and Rob Johnson"}, {"title": "Tight bounds for monotone minimal perfect hashing", "abstract": "The monotone minimal perfect hash function (MMPHF) problem is the following indexing problem. Given a set  \\(S=\\{s_{1},\\ldots,s_{n}\\}\\)  of  \\(n\\)  distinct keys from a universe  \\(U\\)  of size  \\(u\\) , create a data structure  \\(\\text{D}\\)  that answers the following query:  \\(\\begin{equation*} \\rm{R\\small{ANK}}(q) = \\begin{cases}  \\text{rank of } q \\text{ in } S & q\\in S\\\\  \\text{arbitrary answer} & \\text{otherwise.}\\end{cases}\\end{equation*}\\) Solutions to the MMPHF problem are in widespread use in both theory and practice.The best upper bound known for the problem encodes  \\(\\text{D}\\)  in  \\(O(n\\log\\log\\log u)\\)  bits and performs queries in  \\(O(\\log u)\\)  time. It has been an open problem to either improve the space upper bound or to show that this somewhat odd looking bound is tight.In this paper, we show the latter: any data structure (deterministic or randomized) for monotone minimal perfect hashing of any collection of  \\(n \u2026", "year": 2023, "venue": "", "authors": "Sepehr Assadi and Martin Farach-Colton and William Kuszmaul"}, {"title": "The aligned multimodal movie treebank: An audio, video, dependency-parse treebank", "abstract": "Treebanks have traditionally included only text and were derived from written sources such as newspapers or the web. We introduce the Aligned Multimodal Movie Treebank (AMMT), an English language treebank derived from dialog in Hollywood movies which includes transcriptions of the audio-visual streams with word-level alignment, as well as part of speech tags and dependency parses in the Universal Dependencies formalism. AMMT consists of 31,264 sentences and 218,090 words, that will amount to the 3rd largest UD English treebank and the only multimodal treebank in UD. To help with the web-based annotation effort, we also introduce the Efficient Audio Alignment Annotator (EAAA), a companion tool that enables annotators to significantly speed-up their annotation processes.", "year": 2022, "venue": "", "authors": "Adam Yaari and Jan DeWitt and Henry Hu and Bennett Stankovits and Sue Felshin and Yevgeni Berzak and Helena Aparicio and Boris Katz and Ignacio Cases and Andrei Barbu"}, {"title": "Balanced allocations: The heavily loaded case with deletions", "abstract": "In the 2-choice allocation problem, m balls are placed into n bins, and each ball must choose between two random bins  that it has been assigned to. It has been known for more than two decades, that if each ball follows the GREEDY strategy (i.e., always pick the less-full bin), then the maximum load will be  with high probability in n (and  with high probability in m). It has remained an open question whether the same bounds hold in the dynamic version of the same game, where balls are inserted/deleted with no more than m balls present at a time.We show that, somewhat surprisingly, these bounds do not hold in the dynamic setting: already on 4 bins, there exists a sequence of insertions/deletions that cause the GREEDY strategy to incur a maximum load of  with probability \u2014this is the same bound that one gets in the single-choice allocation model where each ball is \u2026", "year": 2022, "venue": "", "authors": "Nikhil Bansal and William Kuszmaul"}, {"title": "Energy-aware neural architecture selection and hyperparameter optimization", "abstract": "Artificial Intelligence (AI) and Deep Learning in particular have increasing computational requirements, with a corresponding increase in energy consumption. There is a tremendous opportunity to reduce the computational cost and environmental impact of deep learning by accelerating neural network architecture search and hyperparameter optimization, as well as explicitly designing neural architectures that optimize for both energy efficiency and performance. Here, we introduce a framework called training performance estimation (TPE), which builds upon existing techniques for training speed estimation in order to monitor energy consumption and rank model performance-without training models to convergence-saving up to 90% of time and energy of the full training budget. We benchmark TPE in the computationally intensive, well-studied domain of computer vision and in the emerging field of graph neural \u2026", "year": 2022, "venue": "", "authors": "Nathan C Frey and Dan Zhao and Simon Axelrod and Michael Jones and David Bestor and Vijay Gadepally and Rafael G\u00f3mez-Bombarelli and Siddharth Samsi"}, {"title": "Temporal correlation of internet observatories and outposts", "abstract": "The Internet has become a critical component of modern civilization requiring scientific exploration akin to endeavors to understand the land, sea, air, and space environments. Understanding the baseline statistical distributions of traffic are essential to the scientific understanding of the Internet. Correlating data from different Internet observatories and outposts can be a useful tool for gaining insights into these distributions. This work compares observed sources from the largest Internet telescope (the CAIDA darknet telescope) with those from a commercial outpost (the GreyNoise honeyfarm). Neither of these locations actively emit Internet traffic and provide distinct observations of unsolicited Internet traffic (primarily botnets and scanners). Newly developed GraphBLAS hyperspace matrices and D4M associative array technologies enable the efficient analysis of these data on significant scales. The CAIDA sources \u2026", "year": 2022, "venue": "", "authors": "Jeremy Kepner and Michael Jones and Daniel Andersen and Aydin Buluc and Chansup Byun and K Claffy and Timothy Davis and William Arcand and Jonathan Bernays and David Bestor and William Bergeron and Vijay Gadepally and Daniel Grant and Micheal Houle and Matthew Hubbell and Hayden Jananthan and Anna Klein and Chad Meiners and Lauren Milechin and Andrew Morris and Julie Mullen and Sandeep Pisharody and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Doug Stetson and Charles Yee and Peter Michaleas"}, {"title": "Stochastic and worst-case generalized sorting revisited", "abstract": "The generalized sorting problem is a restricted version of standard comparison sorting where we wish to sort  elements but only a subset of pairs are allowed to be compared. Formally, there is some known graph  on the  elements , and the goal is to determine the true order of the elements using as few comparisons as possible, where all comparisons () must be edges in . We are promised that if the true ordering is  for  an unknown permutation of the vertices , then  for all : this Hamiltonian path ensures that sorting is actually possible. In this work, we improve the bounds for generalized sorting on both random graphs and worst-case graphs. For Erd\u0151s-Renyi random graphs  (with the promised Hamiltonian path added to ensure sorting is possible), we provide an algorithm for generalized sorting with an expected  comparisons, which we prove to be optimal for query complexity. This \u2026", "year": 2022, "venue": "", "authors": "William Kuszmaul and Shyam Narayanan"}, {"title": "Adapting deep learning models to new meteorological contexts using transfer learning", "abstract": "Meteorological applications such as precipitation nowcasting, synthetic radar generation, statistical downscaling and others have benefited from deep learning (DL) approaches, however several challenges remain for widespread adaptation of these complex models in operational systems. One of these challenges is adequate generalizability; deep learning models trained from datasets collected in specific contexts should not be expected to perform as well when applied to different contexts required by large operational systems. One obvious mitigation for this is to collect massive amounts of training data that cover all expected meteorological contexts, however this is not only costly and difficult to manage, but is also not possible in many parts of the globe where certain sensing platforms are sparse. In this paper, we describe an application of transfer learning to perform domain transfer for deep learning models \u2026", "year": 2021, "venue": "", "authors": "Pooya Khorrami and Olga Simek and Brian Cheung and Mark Veillette and Rumen Dangovski and Ileana Rugina and Marin Soljacic and Pulkit Agrawal"}, {"title": "Simulating a logistics enterprise using an asymmetrical wargame simulation with soar reinforcement learning and coevolutionary algorithms", "abstract": "We demonstrate an innovative framework (CoEvSoarRL) that leverages machine learning algorithms to optimize and simulate a resilient and agile logistics enterprise to improve the readiness and sustainment, as well as reduce the operational risk. The CoEvSoarRL is an asymmetrical wargame simulation that leverages reinforcement learning and coevolutionary algorithms to improve the functions of a total logistics enterprise value chain. We address two of the key challenges: (1) the need to apply holistic prediction, optimization, and wargame simulation to improve the total logistics enterprise readiness; (2) the uncertainty and lack of data which require large-scale systematic what-if scenarios and analysis of alternatives to simulate potential new and unknown situations. Our CoEvSoarRL learns a model of a logistic enterprise environment from historical data with Soar reinforcement learning. Then the Soar model \u2026", "year": 2021, "venue": "", "authors": "Ying Zhao and Erik Hemberg and Nate Derbinsky and Gabino Mata and Una-May O'Reilly"}, {"title": "Composable and reusable neural surrogates to predict system response of causal model components", "abstract": "Surrogate models, or machine learning based emulators of simulators, have been shown to be a powerful tool for accelerating simulations. However, capturing the system response of general nonlinear systems is still an open area of investigation. In this paper we propose a new surrogate architecture which is capable of capturing the input/output response of causal models to automatically replace large aspects of block model diagrams with neural-accelerated forms. We denote this technique the Nonlinear Response Continuous-Time Echo State Network (NR-CTESN) and describe a training mechanism for it to accurately predict the simulation response to exogenous inputs. We then describe a science-guided or physics-informed surrogate architecture based on Cellular Neural Networks to enable the NR-CTESN to accurately reproduce discontinuous output signals. We demonstrate this architecture on an inverter circuit and a Sky130 Digital to Analog Converter (DAC), showcasing a 9x and 300x acceleration of the respective simulations. These results showcase that the NR-CTESN can learn emulate the behavior of components within composable modeling frameworks and thus be reused in new applications without requiring retraining.  Together this showcases a machine learning technique that can be used to generate nonlinear model order reductions of model components in SPICE simulators, Functional Markup Interface (FMI) representations of causal model components, and beyond.", "year": 2021, "venue": "", "authors": "Ranjan Anantharaman and Anas Abdelrehim and Francesco Martinuzzi and Sharan Yalburgi and Elliot Saba and Keno Fischer and Glen Hertz and Pepijn de Vos and Chris Laughman and Yingbo Ma and Viral Shah and Alan Edelman and Chris Rackauckas"}, {"title": "Towards data discovery by example", "abstract": "Data scientists today have to query an avalanche of multi-source data (e.g., data lakes, company databases) for diverse analytical tasks. Data discovery is labor-intensive as users have to find the right tables, and the combination thereof to answer their queries. Data discovery systems automatically find and link (e.g., joins) tables across various sources to aid users in finding the data they need. In this paper, we outline our ongoing efforts to build a data discovery by example system, DICE, that iteratively searches for new tables guided by user-provided data examples. Additionally, DICE asks users to validate results to improve the discovery process over multiple iterations.", "year": 2021, "venue": "", "authors": "El Kindi Rezig and Allan Vanterpool and Vijay Gadepally and Benjamin Price and Michael Cafarella and Michael Stonebraker"}, {"title": "Counting permutations modulo pattern-replacement equivalences for three-letter patterns", "abstract": "We study a family of equivalence relations on , the group of permutations on  letters, created in a manner similar to that of the Knuth relation and the forgotten relation. For our purposes, two permutations are in the same equivalence class if one can be reached from the other through a series of pattern-replacements using patterns whose order permutations are in the same part of a predetermined partition of . When the partition is of  and has one nontrivial part and that part is of size greater than two, we provide formulas for the number of classes created in each previously unsolved case. When the partition is of  and has two nontrivial parts, each of size two (as do the Knuth and forgotten relations), we enumerate the classes for  of the  unresolved cases. In two of these cases, enumerations arise which are the same as those yielded by the Knuth and forgotten relations. The reasons for this phenomenon are still largely a mystery.", "year": 2013, "venue": "", "authors": "William Kuszmaul"}, {"title": "New Monte Carlo Model for the Space Environment", "abstract": "This paper introduces a novel Monte Carlo (MC) method to simulate the evolution of the low-Earth-orbit environment, enhancing the MIT Orbital Capacity Analysis Tool (MOCAT). In recent decades, numerous space environment models have been developed by government agencies and research groups to understand and predict the dynamics of space debris. Our MC approach advances this by simulating the trajectories of space objects and modeling their interactions, such as collisions and explosions. This aids in analyzing the trends of space-object and debris populations. A key innovation of our method is the computational efficiency in orbit propagation, which is crucial for handling potentially large numbers of objects over centuries. We present validation results against the Inter-Agency Space Debris Coordination Committee study and explore various scenarios, including ones without future launches and \u2026", "year": 2025, "venue": "", "authors": "Daniel Jang and Davide Gusmini and Peng Mun Siew and Andrea D\u2019Ambrosio and Simone Servadio and Pablo Machuca and Richard Linares"}, {"title": "On-fiber photonic computing", "abstract": "In the 1800s, Charles Babbage envisioned computers as analog devices. However, it was not until 150 years later that a Mechanical Analog Computer was constructed for the US Navy to solve differential equations. With the end of Moore's Law, photonic computing is revitalizing the promise of analog computing by leveraging photons' speed, bandwidth, and energy efficiency for faster, more efficient, and scalable analog computing systems. This paper argues that the networking community should augment pluggable transponders with photonic computing capabilities to enable a backward-compatible solution for in-network computing. We propose on-fiber photonic computing to perform computing operations inside network transponders while the data is in the optical domain. We discuss the components required to enable the seamless integration of computation into the very fabric of optical communication links \u2026", "year": 2023, "venue": "", "authors": "Mingran Yang and Zhizhen Zhong and Manya Ghobadi"}, {"title": "Strongly history-independent storage allocation: New upper and lower bounds", "abstract": "A data structure is said to be strongly history independent if its state is fully determined by its current set of elements (and random bits). One of the most basic questions that strongly history-independent algorithms face is storage allocation: given a set S of up to  elements, assign them to distinct positions in an array of size n. If we ask that the allocation be strongly history independent, then what is the optimal asymptotic cost of performing an insertion or deletion? On the upper-bound side, Berger et al. (ICALP \u201922) showed how to achieve expected cost . In this paper, we offer a nearly matching lower bound of . As corollaries, we get nearly tight lower bounds for strongly history-independent hashing (STOC \u201901, FOCS \u201907, ICALP\u2019 08) and for the so-called memoryless worker-task assignment problem (SSS \u201917, ICALP \u201920, ICALP \u201922). Next we consider the problem of partitioning an array of size n among many items of different \u2026", "year": 2023, "venue": "", "authors": "William Kuszmaul"}, {"title": "Multi-symmetry ensembles: improving diversity and generalization via opposing symmetries", "abstract": "Deep ensembles (DE) have been successful in improving model performance by learning diverse members via the stochasticity of random initialization. While recent works have attempted to promote further diversity in DE via hyperparameters or regularizing loss functions, these methods primarily still rely on a stochastic approach to explore the hypothesis space. In this work, we present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse ensembles by capturing the multiplicity of hypotheses along symmetry axes, which explore the hypothesis space beyond stochastic perturbations of model weights and hyperparameters. We leverage recent advances in contrastive representation learning to create models that separately capture opposing hypotheses of invariant and equivariant functional classes and present a simple ensembling approach to efficiently combine appropriate hypotheses for a given task. We show that MSE effectively captures the multiplicity of conflicting hypotheses that is often required in large, diverse datasets like ImageNet. As a result of their inherent diversity, MSE improves classification performance, uncertainty quantification, and generalization across a series of transfer tasks. Our code is available at https://github. com/clott3/multi-sym-ensem", "year": 2023, "venue": "", "authors": "Charlotte Loh and Seungwook Han and Shivchander Sudalairaj and Rumen Dangovski and Kai Xu and Florian Wenzel and Marin Soljacic and Akash Srivastava"}, {"title": "Understanding automatic differentiation pitfalls", "abstract": "Automatic differentiation, also known as backpropagation, AD, autodiff, or algorithmic differentiation, is a popular technique for computing derivatives of computer programs accurately and efficiently. Sometimes, however, the derivatives computed by AD could be interpreted as incorrect. These pitfalls occur systematically across tools and approaches. In this paper we broadly categorize problematic usages of AD and illustrate each category with examples such as chaos, time-averaged oscillations, discretizations, fixed-point loops, lookup tables, and linear solvers. We also review debugging techniques and their effectiveness in these situations. With this article we hope to help readers avoid unexpected behavior, detect problems more easily when they occur, and have more realistic expectations from AD tools.", "year": 2023, "venue": "", "authors": "Jan H\u00fcckelheim and Harshitha Menon and William Moses and Bruce Christianson and Paul Hovland and Laurent Hasco\u00ebt"}, {"title": "Efficient representation of large-alphabet probability distributions", "abstract": "A number of engineering and scientific problems require representing and manipulating probability distributions over large alphabets, which we may think of as long vectors of reals summing to 1. In some cases it is required to represent such a vector with only bits per entry. A natural choice is to partition the interval into uniform bins and quantize entries to each bin independently. We show that a minor modification of this procedure\u2013applying an entrywise non-linear function (compander) prior to quantization\u2013yields an extremely effective quantization method. For example, for and-sized alphabets, the quality of representation improves from a loss (under KL divergence) of bits/entry to bits/entry. Compared to floating point representations, our compander method improves the loss from to bits/entry. These numbers hold for both real-world data (word frequencies in books and DNA-mer counts) and for synthetic randomly \u2026", "year": 2023, "venue": "", "authors": "Aviv Adler and Jennifer Tang and Yury Polyanskiy"}, {"title": "Sustainable hpc: Modeling, characterization, and implications of carbon footprint in modern hpc systems", "abstract": "The rapid growth in demand for HPC systems has led to a rise in carbon footprint, which requires urgent intervention. In this work, we present a comprehensive analysis of the carbon footprint of high-performance computing (HPC) systems, considering the carbon footprint during both the hardware manufacturing and system operational stages. Our work employs HPC hardware component carbon footprint modeling, regional carbon intensity analysis, and experimental characterization of the system life cycle to highlight the importance of quantifying the carbon footprint of HPC systems.", "year": 2023, "venue": "", "authors": "Baolin Li and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "Green carbon footprint for model inference serving via exploiting mixed-quality models and gpu partitioning", "abstract": "This paper presents a solution to the challenge of mitigating carbon emissions from hosting large-scale machine learning (ML) inference services. ML inference is critical to modern technology products, but it is also a significant contributor to carbon footprint. We introduce Clover, a carbon-friendly ML inference service runtime system that balances performance, accuracy, and carbon emissions through mixed-quality models and GPU resource partitioning. Our experimental results demonstrate that Clover is effective in substantially reducing carbon emissions while maintaining high accuracy and meeting service level agreement (SLA) targets.", "year": 2023, "venue": "", "authors": "Baolin Li and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "AI-assisted discovery of quantitative and formal models in social science", "abstract": "In social science, formal and quantitative models, such as ones describing economic growth and collective action, are used to formulate mechanistic explanations, provide predictions, and uncover questions about observed phenomena. Here, we demonstrate the use of a machine learning system to aid the discovery of symbolic models that capture nonlinear and dynamical relationships in social science datasets. By extending neuro-symbolic methods to find compact functions and differential equations in noisy and longitudinal data, we show that our system can be used to discover interpretable models from real-world data in economics and sociology. Augmenting existing workflows with symbolic regression can help uncover novel relationships and explore counterfactual models during the scientific process. We propose that this AI-assisted framework can bridge parametric and non-parametric models commonly employed in social science research by systematically exploring the space of nonlinear models and enabling fine-grained control over expressivity and interpretability.", "year": 2022, "venue": "", "authors": "Julia Balla and Sihao Huang and Owen Dugan and Rumen Dangovski and Marin Soljacic"}, {"title": "Large scale enrichment and statistical cyber characterization of network traffic", "abstract": "Modern network sensors continuously produce enormous quantities of raw data that are beyond the capacity of human analysts. Cross-correlation of network sensors increases this challenge by enriching every network event with additional metadata. These large volumes of enriched network data present opportunities to statistically characterize network traffic and quickly answer a key question: \u201cWhat are the primary cyber characteristics of my network data?\u201d The Python GraphBLAS and PyD4M analysis frameworks enable anonymized statistical analysis to be performed quickly and efficiently on very large network data sets. This approach is tested using billions of anonymized network data samples from the largest Internet observatory (CAIDA Telescope) and tens of millions of anonymized records from the largest commercially available background enrichment capability (GreyNoise). The analysis confirms that \u2026", "year": 2022, "venue": "", "authors": "Ivan Kawaminami and Arminda Estrada and Youssef Elsakkary and Hayden Jananthan and Aydin Bulu\u00e7 and Tim Davis and Daniel Grant and Michael Jones and Chad Meiners and Andrew Morris and Sandeep Pisharody and Jeremy Kepner"}, {"title": "Online parallel paging with optimal makespan", "abstract": "The classical paging problem can be described as follows: given a cache that can hold up to k pages (or blocks) and a sequence of requests to pages, how should we manage the cache so as to maximize performance-or, in other words, complete the sequence as quickly as possible. Whereas this sequential paging problem has been well understood for decades, the parallel version, where the cache is shared among p processors each issuing its own sequence of page requests, has been much more resistant. In this problem we are given p request sequences R1, R2, . . . , Rp , each of which accesses a disjoint set of pages, and we ask the question: how should the paging algorithm manage the cache to optimize the completion time of all sequences (i.e., the makespan). As for the classical sequential problem, the goal is to design an online paging algorithm that achieves an optimal competitive ratio, using O(1 \u2026", "year": 2022, "venue": "", "authors": "Kunal Agrawal and Michael A Bender and Rathish Das and William Kuszmaul and Enoch Peserico and Michele Scquizzato"}, {"title": "Serving machine learning inference using heterogeneous hardware", "abstract": "The growing popularity of machine learning algorithms and the wide availability of hardware accelerators have brought up new challenges on inference serving. This paper explores the opportunity to serve inference queries with a heterogeneous system. The system has a central optimizer that allocates heterogeneous hardware resources to cooperatively serve queries. The optimizer supports both energy minimization and throughput maximization while satisfying a latency target. The optimized heterogeneous serving system is evaluated against a homogeneous system, on two representative real-world applications of radar nowcasting and object detection. Our evaluation results show that the power-optimized heterogeneous system can achieve up to 36% of power saving, and the throughput-optimized heterogeneous system can increase query throughput by up to 53%.", "year": 2021, "venue": "", "authors": "Baolin Li and Vijay Gadepally and Siddharth Samsi and Mark Veillette and Devesh Tiwari"}, {"title": "Compositional rl agents that follow language commands in temporal logic", "abstract": "We demonstrate how a reinforcement learning agent can use compositional recurrent neural networks to learn to carry out commands specified in linear temporal logic (LTL). Our approach takes as input an LTL formula, structures a deep network according to the parse of the formula, and determines satisfying actions. This compositional structure of the network enables zero-shot generalization to significantly more complex unseen formulas. We demonstrate this ability in multiple problem domains with both discrete and continuous state-action spaces. In a symbolic domain, the agent finds a sequence of letters that satisfy a specification. In a Minecraft-like environment, the agent finds a sequence of actions that conform to a formula. In the Fetch environment, the robot finds a sequence of arm configurations that move blocks on a table to fulfill the commands. While most prior work can learn to execute one formula reliably, we develop a novel form of multi-task learning for RL agents that allows them to learn from a diverse set of tasks and generalize to a new set of diverse tasks without any additional training. The compositional structures presented here are not specific to LTL, thus opening the path to RL agents that perform zero-shot generalization in other compositional domains.", "year": 2021, "venue": "", "authors": "Yen-Ling Kuo and Boris Katz and Andrei Barbu"}, {"title": "Self-supervised speckle reduction GAN for synthetic aperture radar", "abstract": "In this work, we present a novel generative adversarial network (GAN) for speckle reduction in synthetic aperture radar (SAR) imagery that requires only knowledge of the noise statistics. Speckle is ubiquitous in SAR and can cause problems both for human interpretability and for automated processing such as automated target recognition. The speckle reduction GAN presented in this paper does not require image pairs to train on. Instead we take the output of a smoothing CNN, subtract it from the noisy input and compared the residual noise with simulated speckle drawn using well known SAR noise characteristics. This directly encourage the smoothing CNN to remove those image features that match our simulated speckle. For smoothing and discrimination network, we utilize a simple CNN with a small number of residual network blocks, and unchanging size from layer to layer. We train and show good \u2026", "year": 2021, "venue": "", "authors": "Michael Newey and Prafull Sharma"}, {"title": "A polystore based database operating system (DBOS)", "abstract": "Current operating systems are complex systems that were designed before today\u2019s computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today\u2019s most \u2026", "year": 2021, "venue": "", "authors": "Michael Cafarella and David DeWitt and Vijay Gadepally and Jeremy Kepner and Christos Kozyrakis and Tim Kraska and Michael Stonebraker and Matei Zaharia"}, {"title": "On a novel application of wasserstein-procrustes for unsupervised cross-lingual learning", "abstract": "The emergence of unsupervised word embeddings, pre-trained on very large monolingual text corpora, is at the core of the ongoing neural revolution in Natural Language Processing (NLP). Initially introduced for English, such pre-trained word embeddings quickly emerged for a number of other languages. Subsequently, there have been a number of attempts to align the embedding spaces across languages, which could enable a number of cross-language NLP applications. Performing the alignment using unsupervised cross-lingual learning (UCL) is especially attractive as it requires little data and often rivals supervised and semi-supervised approaches. Here, we analyze popular methods for UCL and we find that often their objectives are, intrinsically, versions of the Wasserstein-Procrustes problem. Hence, we devise an approach to solve Wasserstein-Procrustes in a direct way, which can be used to refine and to improve popular UCL methods such as iterative closest point (ICP), multilingual unsupervised and supervised embeddings (MUSE) and supervised Procrustes methods. Our evaluation experiments on standard datasets show sizable improvements over these approaches. We believe that our rethinking of the Wasserstein-Procrustes problem could enable further research, thus helping to develop better algorithms for aligning word embeddings across languages. Our code and instructions to reproduce the experiments are available at https://github.com/guillemram97/wp-hungarian.", "year": 2020, "venue": "", "authors": "Guillem Ram\u00edrez and Rumen Dangovski and Preslav Nakov and Marin Solja\u010di\u0107"}, {"title": "Contextualizing enhances gradient based meta learning", "abstract": "Meta learning methods have found success when applied to few shot classification problems, in which they quickly adapt to a small number of labeled examples. Prototypical representations, each representing a particular class, have been of particular importance in this setting, as they provide a compact form to convey information learned from the labeled examples. However, these prototypes are just one method of representing this information, and they are narrow in their scope and ability to classify unseen examples. We propose the implementation of contextualizers, which are generalizable prototypes that adapt to given examples and play a larger role in classification for gradient-based models. We demonstrate how to equip meta learning methods with contextualizers and show that their use can significantly boost performance on a range of few shot learning datasets. We also present figures of merit demonstrating the potential benefits of contextualizers, along with analysis of how models make use of them. Our approach is particularly apt for low-data environments where it is difficult to update parameters without overfitting. Our implementation and instructions to reproduce the experiments are available at https://github.com/naveace/proto-context.", "year": 2020, "venue": "", "authors": "Evan Vogelbaum and Rumen Dangovski and Li Jing and Marin Solja\u010di\u0107"}, {"title": "Cilkmem: Algorithms for analyzing the memory high-water mark of fork-join parallel programs", "abstract": "Software engineers designing recursive fork-join programs destined to run on massively parallel computing systems must be cognizant of how their program's memory requirements scale in a many-processor execution. Although tools exist for measuring memory usage during one particular execution of a parallel program, such tools cannot bound the worst-case memory usage over all possible parallel executions.This paper introduces Cilkmem, a tool that analyzes the execution of a deterministic Cilk program to determine its p-processor memory high-water mark (MHWM), which is the worst-case memory usage of the program over all possible p-processor executions. Cilkmem employs two new algorithms for computing the p-processor MHWM. The first algorithm calculates the exact p-processor MHWM in O(T1 \u00b7p) time, where T1 is the total work of the program. The second algorithm solves, in O(T1) time, the \u2026", "year": 2020, "venue": "", "authors": "Tim Kaler and William Kuszmaul and Tao B Schardl and Daniele Vettorel"}, {"title": "Equivalence Classes in  for Three Families of Pattern-Replacement Relations", "abstract": "We study a family of equivalence relations on , the group of permutations on  letters, created in a manner similar to that of the Knuth relation and the forgotten relation. For our purposes, two permutations are in the same equivalence class if one can be reached from the other through a series of pattern-replacements using patterns whose order permutations are in the same part of a predetermined partition of . In particular, we are interested in the number of classes created in  by each relation and in characterizing these classes. Imposing the condition that the partition of  has one nontrivial part containing the cyclic shifts of a single permutation, we find enumerations for the number of nontrivial classes. When the permutation is the identity, we are able to compare the sizes of these classes and connect parts of the problem to Young tableaux and Catalan lattice paths. Imposing the condition that the partition has one nontrivial part containing all of the permutations in  beginning with 1, we both enumerate and characterize the classes in . We do the same for the partition that has two nontrivial parts, one containing all of the permutations in  beginning with 1, and one containing all of the permutations in  ending with 1.", "year": 2013, "venue": "", "authors": "William Kuszmaul and Ziling Zhou"}, {"title": "Message-Passing Monte Carlo: Generating low-discrepancy point sets via graph neural networks", "abstract": "Discrepancy is a well-known measure for the irregularity of the distribution of a point set. Point sets with small discrepancy are called low discrepancy and are known to efficiently fill the space in a uniform manner. Low-discrepancy points play a central role in many problems in science and engineering, including numerical integration, computer vision, machine perception, computer graphics, machine learning, and simulation. In this work, we present a machine learning approach to generate a new class of low-discrepancy point sets named Message-Passing Monte Carlo (MPMC) points. Motivated by the geometric nature of generating low-discrepancy point sets, we leverage tools from Geometric Deep Learning and base our model on graph neural networks. We further provide an extension of our framework to higher dimensions, which flexibly allows the generation of custom-made points that emphasize the \u2026", "year": 2024, "venue": "", "authors": "T Konstantin Rusch and Nathan Kirk and Michael M Bronstein and Christiane Lemieux and Daniela Rus"}, {"title": "Effects of phase angle and sensor properties on on-orbit debris detection using commercial star trackers", "abstract": "The recent proliferation of resident space objects (RSOs) in low Earth orbit (LEO) threatens the sustainability of space as a resource and requires persistent monitoring to avoid collisions involving valuable space assets. State-of-the-art ground-based space surveillance techniques, due to their susceptibility to atmosphere, weather, and lighting conditions, tend to focus on RSOs with characteristic length greater than 10 cm or 1 dm. Consequently, millions of smaller LEO RSOs remain untracked by ground-based methods, which reduces overall space situational awareness. Onboard satellite sensors offer a space-based method for tracking RSOs. Prior research has investigated the feasibility of using commercial star trackers (CSTs) \u2014 optical sensors prevalent on most active spacecraft \u2014 to observe, detect, and estimate the position and velocity of RSOs larger than 10 cm. In a recent effort, we expanded on these \u2026", "year": 2024, "venue": "", "authors": "Allan Shtofenmakher and Hamsa Balakrishnan"}, {"title": "Growing Q-networks: Solving continuous control tasks with adaptive control resolution", "abstract": "Recent reinforcement learning approaches have shown surprisingly strong capabilities of bang-bang policies for solving continuous control benchmarks. The underlying coarse action space discretizations often yield favorable exploration characteristics, while final performance does not visibly suffer in the absence of action penalization in line with optimal control theory. In robotics applications, smooth control signals are commonly preferred to reduce system wear and improve energy efficiency, while regularization via action costs can be detrimental to exploration. Our work aims to bridge this performance gap by growing discrete action spaces from coarse to fine control resolution. We take advantage of recent results in decoupled Q-learning to scale our approach to high-dimensional action spaces up to dim (A)= 38. Our work indicates that an adaptive control resolution in combination with value decomposition yields simple critic-only algorithms that enable surprisingly strong performance on continuous control tasks.", "year": 2024, "venue": "", "authors": "Tim Seyde and Peter Werner and Wilko Schwarting and Markus Wulfmeier and Daniela Rus"}, {"title": "Certifying bimanual rrt motion plans in a second", "abstract": "We present an efficient method for certifying non-collision for piecewise-polynomial motion plans in algebraic reparametrizations of configuration space. Such motion plans include those generated by popular randomized methods including RRTs and PRMs, as well as those generated by many methods in trajectory optimization. Based on Sums-of-Squares optimization, our method provides exact, rigorous certificates of non-collision; it can never falsely claim that a motion plan containing collisions is collision-free. We demonstrate that our formulation is practical for real world deployment, certifying the safety of a twelve degree of freedom motion plan in just over a second. Moreover, the method is capable of discriminating the safety or lack thereof of two motion plans which differ by only millimeters.", "year": 2024, "venue": "", "authors": "Alexandre Amice and Peter Werner and Russ Tedrake"}, {"title": "Transformer-based Atmospheric Density Forecasting", "abstract": "As the peak of the solar cycle approaches in 2025 and the ability of a single geomagnetic storm to significantly alter the orbit of Resident Space Objects (RSOs), techniques for atmospheric density forecasting are vital for space situational awareness. While linear data-driven methods, such as dynamic mode decomposition with control (DMDc), have been used previously for forecasting atmospheric density, deep learning-based forecasting has the ability to capture nonlinearities in data. By learning multiple layer weights from historical atmospheric density data, long-term dependencies in the dataset are captured in the mapping between the current atmospheric density state and control input to the atmospheric density state at the next timestep. This work improves upon previous linear propagation methods for atmospheric density forecasting, by developing a nonlinear transformer-based architecture for atmospheric density forecasting. Empirical NRLMSISE-00 and JB2008, as well as physics-based TIEGCM atmospheric density models are compared for forecasting with DMDc and with the transformer-based propagator.", "year": 2023, "venue": "", "authors": "Julia Briden and Peng Mun Siew and Victor Rodriguez-Fernandez and Richard Linares"}, {"title": "Innovating AI Leadership Education", "abstract": "This research to practice full paper explores a new educational framework for AI-informed leadership and evaluates its curriculum and pedagogical approach through a novel, tailored, research instrument. Artificial Intelligence continues to rapidly transform many aspects of markets, solutions, and organizational culture across companies, agencies, and institutions in the public and private sectors. Within complex organizations, AI tools, technologies, and applications inform how leaders engage in strategy-making, management, operations, human resources, and professional education. Non-technical managers and executives are increasingly expected to lead teams to implement responsible AI solutions with the promise to improve efficiency, effectiveness, productivity, profitability, and more. AI is rapidly transforming organizational culture, requiring non-technical leaders to develop AI literacy and essential skills to \u2026", "year": 2023, "venue": "", "authors": "Xiaoxue Du and Sharifa Alghowinem and Matthew Taylor and Kate Darling and Cynthia Breazeal"}, {"title": "Towards Cooperative Flight Control Using Visual-Attention", "abstract": "The cooperation of a human pilot with an autonomous agent during flight control realizes parallel autonomy. We propose an air-guardian system that facilitates cooperation between a pilot with eye tracking and a parallel end-to-end neural control system. Our vision-based air-guardian system combines a causal continuous-depth neural network model with a cooperation layer to enable parallel autonomy between a pilot and a control system based on perceived differences in their attention profiles. The attention profiles for neural networks are obtained by computing the networks' saliency maps (feature importance) through the VisualBackProp algorithm, while the attention profiles for humans are either obtained by eye tracking of human pilots or saliency maps of networks trained to imitate human pilots. When the attention profile of the pilot and guardian agents align, the pilot makes control decisions. Otherwise, the \u2026", "year": 2023, "venue": "", "authors": "Lianhao Yin and Makram Chahine and Tsun-Hsuan Wang and Tim Seyde and Chao Liu and Mathias Lechner and Ramin Hasani and Daniela Rus"}, {"title": "Deployment of real-time network traffic analysis using GraphBLAS hypersparse matrices and D4M associative arrays", "abstract": "Matrix/array analysis of networks can provide significant insight into their behavior and aid in their operation and protection. Prior work has demonstrated the analytic, performance, and compression capabilities of GraphBLAS (graph-blas.org) hypersparse matrices and D4M (d4m.mit.edu) associative arrays (a mathematical superset of matrices). Obtaining the benefits of these capabilities requires integrating them into operational systems, which comes with its own unique challenges. This paper describes two examples of real-time operational implementations. First, is an operational GraphBLAS implementation that constructs anonymized hypersparse matrices on a high-bandwidth network tap. Second, is an operational D4M implementation that analyzes daily cloud gateway logs. The architectures of these implementations are presented. Detailed measurements of the resources and the performance are collected \u2026", "year": 2023, "venue": "", "authors": "Michael Jones and Jeremy Kepner and Andrew Prout and Timothy Davis and William Arcand and David Bestor and William Bergeron and Chansup Byun and Vijay Gadepally and Micheal Houle and Matthew Hubbell and Hayden Jananthan and Anna Klein and Lauren Milechin and Guillermo Morales and Julie Mullen and Ritesh Patel and Sandeep Pisharody and Albert Reuther and Antonio Rosa and Siddharth Samsi and Charles Yee and Peter Michaleas"}, {"title": "Quantization-aware interval bound propagation for training certifiably robust quantized neural networks", "abstract": "We study the problem of training and certifying adversarially robust quantized neural networks (QNNs). Quantization is a technique for making neural networks more efficient by running them using low-bit integer arithmetic and is therefore commonly adopted in industry. Recent work has shown that floating-point neural networks that have been verified to be robust can become vulnerable to adversarial attacks after quantization, and certification of the quantized representation is necessary to guarantee robustness. In this work, we present quantization-aware interval bound propagation (QA-IBP), a novel method for training robust QNNs. Inspired by advances in robust learning of non-quantized networks, our training algorithm computes the gradient of an abstract representation of the actual network. Unlike existing approaches, our method can handle the discrete semantics of QNNs. Based on QA-IBP, we also develop a complete verification procedure for verifying the adversarial robustness of QNNs, which is guaranteed to terminate and produce a correct answer. Compared to existing approaches, the key advantage of our verification procedure is that it runs entirely on GPU or other accelerator devices. We demonstrate experimentally that our approach significantly outperforms existing methods and establish the new state-of-the-art for training and certifying the robustness of QNNs.", "year": 2023, "venue": "", "authors": "Mathias Lechner and \u0110or\u0111e \u017dikeli\u0107 and Krishnendu Chatterjee and Thomas A Henzinger and Daniela Rus"}, {"title": "Learning stability attention in vision-based end-to-end driving policies", "abstract": "Today\u2019s end-to-end learning systems can learn to explicitly infer control from perception. However, it is difficult to guarantee stability and robustness for these systems since they are often exposed to unstructured, high-dimensional, and complex observation spaces (eg, autonomous driving from a stream of pixel inputs). We propose to leverage control Lyapunov functions (CLFs) to equip end-to-end vision-based policies with stability properties and introduce stability attention in CLFs (att-CLFs) to tackle environmental changes and improve learning flexibility. We also present an uncertainty propagation technique that is tightly integrated into att-CLFs. We demonstrate the effectiveness of att-CLFs via comparison with classical CLFs, model predictive control, and vanilla end-to-end learning in a photo-realistic simulator and on a real full-scale autonomous vehicle.", "year": 2023, "venue": "", "authors": "Tsun-Hsuan Wang and Wei Xiao and Makram Chahine and Alexander Amini and Ramin Hasani and Daniela Rus"}, {"title": "On neural architectures for deep learning-based source separation of co-channel OFDM signals", "abstract": "We study the single-channel source separation problem involving orthogonal frequency-division multiplexing (OFDM) signals, which are ubiquitous in many modern-day digital communication systems. Related efforts have been pursued in monaural source separation, where state-of-the-art neural architectures have been adopted to train an end-to-end separator for audio signals (as 1-dimensional time series). In this work, through a prototype problem based on the OFDM source model, we assess\u2014and question\u2014the efficacy of using audio-oriented neural architectures in separating signals based on features pertinent to communication waveforms. Perhaps surprisingly, we demonstrate that in some configurations, where perfect separation is theoretically attainable, these audio-oriented neural architectures perform poorly in separating co-channel OFDM waveforms. Yet, we propose critical domain-informed \u2026", "year": 2023, "venue": "", "authors": "Gary CF Lee and Amir Weiss and Alejandro Lancho and Yury Polyanskiy and Gregory W Wornell"}, {"title": "Feasibility Analysis of On-Orbit Debris Detection Using Commercial Star Trackers", "abstract": "The U.S. Space Surveillance Network (SSN) currently tracks over 23,000 resident space objects (RSOs) in low- earth orbit (LEO). The SSN uses ground-based radar and optical methods, which are susceptible to variations in atmosphere, weather, and lighting conditions. These barriers limit the surveillance capabilities to objects with characteristic length greater than 10 cm. Consequently, hundreds of thousands of smaller RSOs in LEO remain untracked, reducing overall space situational awareness. Prior research has demonstrated the feasibility of using space-based commercial star trackers (CSTs) to detect and track objects larger than 10 cm in characteristic length. The analysis we present in this paper shows that CSTs can also be used to detect debris particles below 10 cm in size. We model particles as Lambertian spheres with zero phase angle and ten percent reflectivity. The apparent visual magnitude of debris particles is expressed as a function of particle size and RSO-CST distance and compared against the sensitivity levels of a variety of CSTs. We find that, when properly illuminated, debris of characteristic length between 1 cm and 10 cm can be detected by some CSTs even at distances of tens of kilometers. More sensitive CSTs can characterize RSOs at the larger end of this scale (i.e., 10 cm) hundreds of kilometers away; alternatively, they can track objects smaller than 1 cm at closer distances.", "year": 2023, "venue": "", "authors": "Allan Shtofenmakher and Hamsa Balakrishnan"}, {"title": "Stochastic optimal control via local occupation measures", "abstract": "Viewing stochastic processes through the lens of occupation measures has proved to be a powerful angle of attack for the theoretical and computational analysis of stochastic optimal control problems. We present a simple modification of the traditional occupation measure framework derived from resolving the occupation measures locally on a partition of the control problem's space-time domain. This notion of local occupation measures provides fine-grained control over the construction of structured semidefinite programming relaxations for a rich class of stochastic optimal control problems with embedded diffusion and jump processes via the moment-sum-of-squares hierarchy. As such, it bridges the gap between discretization-based approximations to the Hamilton-Jacobi-Bellmann equations and occupation measure relaxations. We demonstrate with examples that this approach enables the computation of high quality bounds for the optimal value of a large class of stochastic optimal control problems with significant performance gains relative to the traditional occupation measure framework.", "year": 2022, "venue": "", "authors": "Flemming Holtorf and Alan Edelman and Christopher Rackauckas"}, {"title": "Enabling transformers to understand low-level programs", "abstract": "Unlike prior approaches to machine learning, Transformer models can first be trained on a large corpus of unlabeled data with a generic objective and then on a smaller task-specific dataset. This versatility has led to both larger models and datasets. Consequently, Transformers have led to breakthroughs in the field of natural language processing. Generic program optimization presently operates on low-level programs such as LLVM. Unlike the high-level languages (e.g. C, Python, Java), which have seen initial success in machine-learning analyses, lower-level languages tend to be more verbose and repetitive to precisely specify program behavior, provide more details about microarchitecture, and derive properties necessary for optimization, all of which makes it difficult for machine learning. In this work, we apply transfer learning to low-level (LLVM) programs and study how low-level programs can be made \u2026", "year": 2022, "venue": "", "authors": "Zifan Carl Guo and William S Moses"}, {"title": "Parallelizing explicit and implicit extrapolation methods for ordinary differential equations", "abstract": "Numerically solving ordinary differential equations (ODEs) is a naturally serial process and as a result the vast majority of ODE solver software are serial. In this manuscript we developed a set of parallelized ODE solvers using extrapolation methods which exploit \u201cparallelism within the method\u201d so that arbitrary user ODEs can be parallelized. We describe the specific choices made in the implementation of the explicit and implicit extrapolation methods which allow for generating low overhead static schedules to then exploit with optimized multi-threaded implementations. We demonstrate that while the multi-threading gives a noticeable acceleration on both explicit and implicit problems, the explicit parallel extrapolation methods gave no significant improvement over state-of-the-art even with a multi-threading advantage against current optimized high order Runge- Kutta tableaus. However, we demonstrate that the \u2026", "year": 2022, "venue": "", "authors": "Chris Elrod and Yingbo Ma and Konstantin Althaus and Christopher Rackauckas"}, {"title": "Exploring perceptual straightness in learned visual representations", "abstract": "Humans have been shown to use a ''straightened'' encoding to represent the natural visual world as it evolves in time (Henaff et al. 2019). In the context of discrete video sequences, ''straightened'' means that changes between frames follow a more linear path in representation space at progressively deeper levels of processing. While deep convolutional networks are often proposed as models of human visual processing, many do not straighten natural videos. In this paper, we explore the relationship between network architecture, differing types of robustness, biologically-inspired filtering mechanisms, and representational straightness in response to time-varying input; we identify strengths and limitations of straightness as a useful way of evaluating neural network representations. We find that (1) adversarial training leads to straighter representations in both CNN and transformer-based architectures but (2) this effect is task-dependent, not generalizing to tasks such as segmentation and frame-prediction, where straight representations are not favorable for predictions; and nor to other types of robustness. In addition, (3) straighter representations impart temporal stability to class predictions, even for out-of-distribution data. Finally, (4) biologically-inspired elements increase straightness in the early stages of a network, but do not guarantee increased straightness in downstream layers of CNNs. We show that straightness is an easily computed measure of representational robustness and stability, as well as a hallmark of human representations with benefits for computer vision models.", "year": 2022, "venue": "", "authors": "Anne Harrington and Vasha DuTell and Ayush Tewari and Mark Hamilton and Simon Stent and Ruth Rosenholtz and William T Freeman"}, {"title": "Spoken ObjectNet: A bias-controlled spoken caption dataset", "abstract": "Visually-grounded spoken language datasets can enable models to learn cross-modal correspondences with very weak supervision. However, modern audio-visual datasets contain biases that undermine the real-world performance of models trained on that data. We introduce Spoken ObjectNet, which is designed to remove some of these biases and provide a way to better evaluate how effectively models will perform in real-world scenarios. This dataset expands upon ObjectNet, which is a bias-controlled image dataset that features similar image classes to those present in ImageNet. We detail our data collection pipeline, which features several methods to improve caption quality, including automated language model checks. Lastly, we show baseline results on image retrieval and audio retrieval tasks. These results show that models trained on other datasets and then evaluated on Spoken ObjectNet tend to perform poorly due to biases in other datasets that the models have learned. We also show evidence that the performance decrease is due to the dataset controls, and not the transfer setting.", "year": 2021, "venue": "", "authors": "Ian Palmer and Andrew Rouditchenko and Andrei Barbu and Boris Katz and James Glass"}, {"title": "Technical report on data integration and preparation", "abstract": "AI application developers typically begin with a dataset of interest and a vision of the end analytic or insight they wish to gain from the data at hand. Although these are two very important components of an AI workflow, one often spends the first few weeks (sometimes months) in the phase we refer to as data conditioning. This step typically includes tasks such as figuring out how to prepare data for analytics, dealing with inconsistencies in the dataset, and determining which algorithm (or set of algorithms) will be best suited for the application. Larger, faster, and messier datasets such as those from Internet of Things sensors, medical devices or autonomous vehicles only amplify these issues. These challenges, often referred to as the three Vs (volume, velocity, variety) of Big Data, require low-level tools for data management, preparation and integration. In most applications, data can come from structured and/or unstructured sources and often includes inconsistencies, formatting differences, and a lack of ground-truth labels. In this report, we highlight a number of tools that can be used to simplify data integration and preparation steps. Specifically, we focus on data integration tools and techniques, a deep dive into an exemplar data integration tool, and a deep-dive in the evolving field of knowledge graphs. Finally, we provide readers with a list of practical steps and considerations that they can use to simplify the data integration challenge. The goal of this report is to provide readers with a view of state-of-the-art as well as practical tips that can be used by data creators that make data integration more seamless.", "year": 2021, "venue": "", "authors": "El Kindi Rezig and Michael Cafarella and Vijay Gadepally"}, {"title": "Beyond worst-case analysis of multicore caching strategies", "abstract": "Every processor with multiple cores sharing a cache needs to implement a cache-replacement algorithm. Previous work demonstrated that the competitive ratio of a large class of online algorithms, including Least-Recently-Used (LRU), grows with the length of the input. Furthermore, even offline algorithms like Furthest-In-Future, the optimal algorithm in single-core caching, cannot compete in the multicore setting. These negative results motivate a more in-depth comparison of multicore caching algorithms via alternative analysis measures. Specifically, the power of the adversary to adapt to online algorithms suggests the need for a direct comparison of online algorithms to each other.In this paper, we introduce cyclic analysis, a generalization of bijective analysis introduced by Angelopoulos et al. [SODA'07]. Cyclic analysis captures the advantages of bijective analysis while offering flexibility that makes it more \u2026", "year": 2021, "venue": "", "authors": "Shahin Kamali and Helen Xu"}, {"title": "Fast training of deep neural networks robust to adversarial perturbations", "abstract": "Despite their promising performance, deep neural networks have shown sensitivities to perturbations of their inputs (e.g., adversarial examples) and their learned feature representations are often difficult to interpret, raising concerns about their true capability and trustworthiness. Recent work in adversarial training, a form of robust optimization in which the model is optimized against adversarial examples, demonstrates the ability to improve performance sensitivities to perturbations and yield feature representations that are more interpretable. Adversarial training, however, comes with an increased computational cost over that of standard (i.e., nonrobust) training, rendering it impractical for use in large-scale problems. Recent work suggests that a fast approximation to adversarial training shows promise for reducing training time and maintaining robustness in the presence of perturbations bounded by the infinity \u2026", "year": 2020, "venue": "", "authors": "Justin Goodwin and Olivia Brown and Victoria Helus"}, {"title": "Work-efficient parallel algorithms for accurate floating-point prefix sums", "abstract": "Existing work-efficient parallel algorithms for floating-point prefix sums exhibit either good performance or good numerical accuracy, but not both. Consequently, prefix-sum algorithms cannot easily be used in scientific-computing applications that require both high performance and accuracy. We have designed and implemented two new algorithms, called CAST_BLK and PAIR_BLK, whose accuracy is significantly higher than that of the high-performing prefix-sum algorithm from the Problem Based Benchmark Suite, while running with comparable performance on modern multicore machines. Specifically, the root mean squared error of the PBBS code on a large array of uniformly distributed 64-bit floating-point numbers is 8 times higher than that of CAST_BLK and 5.8 times higher than that of PAIR_BLK. These two codes employ the PBBS three-stage strategy for performance, but they are designed to achieve high \u2026", "year": 2020, "venue": "", "authors": "Sean Fraser and Helen Xu and Charles E Leiserson"}, {"title": "Memoryless worker-task assignment with polylogarithmic switching cost", "abstract": "We study the basic problem of assigning memoryless workers to tasks with dynamically changing demands. Given a set of  workers and a multiset  of  tasks, a memoryless worker-task assignment function is any function  that assigns the workers  to the tasks  based only on the current value of . The assignment function  is said to have switching cost at most  if, for every task multiset , changing the contents of  by one task changes  by at most  worker assignments. The goal of memoryless worker task assignment is to construct an assignment function with the smallest possible switching cost. In past work, the problem of determining the optimal switching cost has been posed as an open question. There are no known sub-linear upper bounds, and after considerable effort, the best known lower bound remains 4 (ICALP 2020). We show that it is possible to achieve polylogarithmic switching cost. We give a construction via the probabilistic method that achieves switching cost  and an explicit construction that achieves switching cost . We also prove a super-constant lower bound on switching cost: we show that for any value of , there exists a value of  for which the optimal switching cost is . Thus it is not possible to achieve a switching cost that is sublinear strictly as a function of . Finally, we present an application of the worker-task assignment problem to a metric embeddings problem. In particular, we use our results to give the first low-distortion embedding from sparse binary vectors into low-dimensional Hamming space.", "year": 2020, "venue": "", "authors": "Aaron Berger and William Kuszmaul and Adam Polak and Jonathan Tidor and Nicole Wein"}, {"title": "SyFER-MLIR: Integrating Fully Homomorphic Encryption Into the MLIR Compiler Framework", "abstract": "Fully homomorphic encryption opens up the possibility of secure computation on private data. However, fully homomorphic encryption is limited by its speed and the fact that arbitrary computations must be represented by combinations of primitive operations, such as addition, multiplication, and binary gates. Integrating FHE into the MLIR compiler infrastructure allows it to be automatically optimized at many different levels and will allow any program which compiles into MLIR to be modified to be encrypted by simply passing another flag into the compiler. The process of compiling into an intermediate representation and dynamically generating the encrypted program, rather than calling functions from a library, also allows for optimizations across multiple operations, such as rewriting a DAG of operations to run faster and removing unnecessary operations.", "year": 2020, "venue": "", "authors": "Sanath Govindarajan and William S Moses"}, {"title": "John Holodnak, et al. Ai-enabling workloads on large-scale gpu-accelerated system: Characterization, opportunities, and implications", "abstract": "", "year": "", "venue": "", "authors": "Baolin Li and Rohin Arora and Siddharth Samsi and Tirthak Patel and William Arcand and David Bestor and Chansup Byun and Rohan Basu Roy and Bill Bergeron"}, {"title": "Quanta: Efficient high-rank fine-tuning of llms with quantum-informed tensor adaptation", "abstract": "We propose Quantum-informed Tensor Adaptation (QuanTA), a novel, easy-to-implement, fine-tuning method with no inference overhead for large-scale pre-trained language models. By leveraging quantum-inspired methods derived from quantum circuit structures, QuanTA enables efficient high-rank fine-tuning, surpassing the limitations of Low-Rank Adaptation (LoRA)---low-rank approximation may fail for complicated downstream tasks. Our approach is theoretically supported by the universality theorem and the rank representation theorem to achieve efficient high-rank adaptations. Experiments demonstrate that QuanTA significantly enhances commonsense reasoning, arithmetic reasoning, and scalability compared to traditional methods. Furthermore, QuanTA shows superior performance with fewer trainable parameters compared to other approaches and can be designed to integrate with existing fine-tuning algorithms for further improvement, providing a scalable and efficient solution for fine-tuning large language models and advancing state-of-the-art in natural language processing.", "year": 2024, "venue": "", "authors": "Zhuo Chen and Rumen Dangovski and Charlotte Loh and Owen Dugan and Di Luo and Marin Soljacic"}, {"title": "Modern hashing made simple", "abstract": "Modern work on hashing has led to hash tables with extraordinary guarantees. However, these data structures are too complex to be taught in (even an advanced) data structures course. In this paper, we show that this need not be the case: using standard machinery that we already teach, one can construct a simple hash table that offers guarantees much stronger than what are classically taught:\u2022 Operations are O(1)-time with high probability;\u2022 The hash table stores n k-bit items in nk + O(n log log n) bits of space;\u2022 The hash table is dynamically resized, so the space bound holds with respect to the current size n at each time step.", "year": 2024, "venue": "", "authors": "Michael A Bender and Mart\u00edn Farach-Colton and John Kuszmaul and William Kuszmaul"}, {"title": "AutoCoreset: An automatic practical coreset construction framework", "abstract": "A coreset is a small weighted subset of an input set that approximates its loss function, for a given set of queries. Coresets became prevalent in machine learning as they have shown to be advantageous for many applications. Unfortunately, coresets are constructed in a problem-dependent manner, where for each problem, a new coreset construction algorithm is suggested, taking years to prove its correctness. Even the generic frameworks require additional (problem-dependent) computations or proofs to be done by the user. Besides, many problems do not have (provable) small coresets, limiting their applicability. To this end, we suggest an automatic practical framework for constructing coresets, which requires (only) the input data and the desired cost function from the user, without the need for any other task-related computation to be done by the user. To do so, we reduce the problem of approximating a loss function to an instance of vector summation approximation, where the vectors we aim to sum are loss vectors of a specific subset of the queries, such that we aim to approximate the image of the function on this subset. We show that while this set is limited, the coreset is quite general. An extensive experimental study on various machine learning applications is also conducted. Finally, we provide a \u201cplug and play\" style implementation, proposing a user-friendly system that can be easily used to apply coresets for many problems. We believe that these contributions enable future research and easier use and applications of coresets.", "year": 2023, "venue": "", "authors": "Alaa Maalouf and Murad Tukan and Vladimir Braverman and Daniela Rus"}, {"title": "Locally regularized neural differential equations: Some black boxes were meant to remain closed!", "abstract": "Neural Differential Equations have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. Controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time-points to guide the training towards learning a dynamical system that is easier to integrate. We \u201cclose the blackbox\u201d and allow the use of our method with any sensitivity method. We perform experimental studies to compare our method to global regularization to show that we attain similar performance numbers without compromising on the flexibility of implementation. We develop two sampling strategies to trade-off between performance and training time. Our method reduces the number of function evaluations to 0.556 x-0.733 x and accelerates predictions by 1.3 x-2x.", "year": 2023, "venue": "", "authors": "Avik Pal and Alan Edelman and Christopher Vincent Rackauckas"}, {"title": "Transparent checkpointing for automatic differentiation of program loops through expression transformations", "abstract": "Automatic differentiation (AutoDiff) in machine learning is largely restricted to expressions used for neural networks (NN), with the depth rarely exceeding a few tens of layers. Compared to NN, numerical simulations typically involve iterative algorithms like time steppers that lead to millions of iterations. Even for modest-sized models, this may yield infeasible memory requirements when applying the adjoint method, also called backpropagation, to time-dependent problems. In this situation, checkpointing algorithms provide a trade-off between recomputation and storage. This paper presents the package Checkpointing.jl that leverages expression transformations in the programming language Julia and the package ChainRules.jl to automatically and transparently transform loop iterations into differentiated loops. The user may choose between various checkpointing algorithm schemes and storage devices. We \u2026", "year": 2023, "venue": "", "authors": "Michel Schanen and Sri Hari Krishna Narayanan and Sarah Williamson and Valentin Churavy and William S Moses and Ludger Paehler"}, {"title": "On the generalization error of meta learning for the Gibbs algorithm", "abstract": "We analyze the generalization ability of joint-training meta learning algorithms via the Gibbs algorithm. Our exact characterization of the expected meta generalization error for the meta Gibbs algorithm is based on symmetrized KL information, which measures the dependence between all meta-training datasets and the output parameters, including task-specific and meta parameters. Additionally, we derive an exact characterization of the meta generalization error for the super-task Gibbs algorithm, in terms of conditional symmetrized KL information within the super-sample and super-task framework introduced in [1] and [2], respectively. Our results also enable us to provide novel distribution-free generalization error upper bounds for these Gibbs algorithms applicable to meta learning.", "year": 2023, "venue": "", "authors": "Yuheng Bu and Harsha Vardhan Tetali and Gholamali Aminian and Miguel Rodrigues and Gregory Wornell"}, {"title": "On the advantages of asynchrony in the unsourced MAC", "abstract": "In this work we demonstrate how a lack of synchronization can in fact be advantageous in the problem of random access. Specifically, we consider a multiple-access problem over a frame-asynchronous 2-user binary-input adder channel in the unsourced setup (2-UBAC). Previous work has shown that under perfect synchronization the per-user rates achievable with linear codes over the 2-UBAC are limited by 0.5 bit per channel use (compared to the capacity of 0.75). In this paper, we first demonstrate that arbitrary small (even single-bit) shift between the user\u2019s frames enables (random) linear codes to attain full capacity of 0.75 bit/user. Furthermore, we derive density evolution equations for irregular LDPC codes, and prove (via concentration arguments) that they correctly track the asymptotic bit-error rate of a BP decoder. Optimizing the degree distributions we construct LDPC codes achieving per-user rates of 0.73 \u2026", "year": 2023, "venue": "", "authors": "Alexander Fengler and Alejandro Lancho and Krishna Narayanan and Yury Polyanskiy"}, {"title": "Infoshape: Task-based neural data shaping via mutual information", "abstract": "The use of mutual information as a tool in private data sharing has remained an open challenge due to the difficulty of its estimation in practice. In this paper, we propose InfoShape, a task-based encoder that aims to remove unnecessary sensitive information from training data while maintaining enough relevant information for a particular ML training task. We achieve this goal by utilizing mutual information estimators that are based on neural networks, in order to measure two performance metrics, privacy and utility. Using these together in a Lagrangian optimization, we train a separate neural network as a lossy encoder. We empirically show that InfoShape is capable of shaping the encoded samples to be informative for a specific downstream task while eliminating unnecessary sensitive information. Moreover, we demonstrate that the classification accuracy of downstream models has a meaningful connection with \u2026", "year": 2023, "venue": "", "authors": "Homa Esfahanizadeh and William Wu and Manya Ghobadi and Regina Barzilay and Muriel M\u00e9dard"}, {"title": "Channel Comparison Methods and Statistical Problems on Graphs", "abstract": "Initially driven by channel coding, information theory has developed a large collection of tools for measuring and comparing effectiveness of information channels. These tools have found applications in various fields such as statistics, probability, and theoretical computer science. This thesis explores several applications of these tools to statistical problems related to graphs.", "year": 2023, "venue": "", "authors": "Yuzhou Gu"}, {"title": "Hypersparse network flow analysis of packets with graphblas", "abstract": "Internet analysis is a major challenge due to the volume and rate of network traffic. In lieu of analyzing traffic as raw packets, network analysts often rely on compressed network flows (netflows) that contain the start time, stop time, source, destination, and number of packets in each direction. However, many traffic analyses benefit from temporal aggregation of multiple simultaneous netflows, which can be computationally challenging. To alleviate this concern, a novel netflow compression and resampling method has been developed leveraging GraphBLAS hyperspace traffic matrices that preserve anonymization while enabling subrange analysis. Standard multi-temporal spatial analyses are then performed on each sub range to generate detailed statistical aggregates of the source packets, source fan-out, unique links, destination fan-in, and destination packets of each subrange which can then be used for \u2026", "year": 2022, "venue": "", "authors": "Tyler Trigg and Chad Meiners and Sandeep Pisharody and Hayden Jananthan and Michael Jones and Adam Michaleas and Timothy Davis and Erik Welch and William Arcand and David Bestor and William Bergeron and Chansup Byun and Vijay Gadepally and Micheal Houle and Matthew Hubbell and Anna Klein and Peter Michaleas and Lauren Milechin and Julie Mullen and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Doug Stetson and Charles Yee and Jeremy Kepner"}, {"title": "Approximating dynamic time warping distance between run-length encoded strings", "abstract": "Dynamic Time Warping (DTW) is a widely used similarity measure for comparing strings that encode time series data, with applications to areas including bioinformatics, signature verification, and speech recognition. The standard dynamic-programming algorithm for DTW takes $O(n^2)$ time, and there are conditional lower bounds showing that no algorithm can do substantially better. In many applications, however, the strings $x$ and $y$ may contain long runs of repeated letters, meaning that they can be compressed using run-length encoding. A natural question is whether the DTW-distance between these compressed strings can be computed efficiently in terms of the lengths $k$ and $\\ell$ of the compressed strings. Recent work has shown how to achieve $O(k\\ell^2 + \\ell k^2)$ time, leaving open the question of whether a near-quadratic $\\tilde{O}(k\\ell)$-time algorithm might exist. We show that, if a small approximation loss is permitted, then a near-quadratic time algorithm is indeed possible: our algorithm computes a $(1 + \\epsilon)$-approximation for $DTW(x, y)$ in $\\tilde{O}(k\\ell / \\epsilon^3)$ time, where $k$ and $\\ell$ are the number of runs in $x$ and $y$. Our algorithm allows for $DTW$ to be computed over any metric space $(\\Sigma, \\delta)$ in which distances are $O(log(n))$-bit integers. Surprisingly, the algorithm also works even if $\\delta$ does not induce a metric space on $\\Sigma$ (e.g., $\\delta$ need not satisfy the triangle inequality).", "year": 2022, "venue": "", "authors": "Zoe Xi and William Kuszmaul"}, {"title": "Controlling the focus of pretrained language generation models", "abstract": "The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model's focus. This work aims to develop a control mechanism by which a user can select spans of context as \"highlights\" for the model to focus on, and generate relevant output. To achieve this goal, we augment a pretrained model with trainable \"focus vectors\" that are directly applied to the model's embeddings, while the model itself is kept fixed. These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. We also collect evaluation data where the highlight-generation pairs are annotated by humans. Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.", "year": 2022, "venue": "", "authors": "Jiabao Ji and Yoon Kim and James Glass and Tianxing He"}, {"title": "Non-clairvoyant scheduling with predictions revisited", "abstract": "One of the central objectives in the design of intelligent systems is the provision of anytime capabilities. In particular, applications such as medical diagnostic systems and motion planning require that the system outputs a reasonably efficient solution given the unavoidable constraints on computation time. Anytime algorithms [6] offer such a tradeoff between computation time and quality of the output. Russell and Zilberstein [10] introduced a useful distinction between two different types of anytime algorithms. On the one hand, there is the class of contract algorithms, which are given the amount of allowable computation time (ie, the intended query time) as part of the input. However, if the algorithm is interrupted at any point before this \u201ccontract time\u201d expires, the algorithm may output a result that is meaningless. On the other hand, the class of interruptible algorithms consists of algorithms whose allowable running time is not known in advance, and thus can be interrupted (queried) at any given point throughout their execution. Although less flexible than interruptible algorithms, contract algorithms are often easier to implement and maintain [5]. Hence a natural question arises: how can one convert a contract algorithm to an interruptible equivalent, and at which cost? This question can be addressed using a simple technique that consists of repeated executions of the contract algorithm with increasing runtimes (also called lengths). For example, consider a schedule of executions of the contract algorithm in which the i-th execution has length 2i. Assuming that an interruption occurs at time t, then the above schedule guarantees the completion of a \u2026", "year": 2022, "venue": "", "authors": "Alexander Lindermayr and Nicole Megow"}, {"title": "Maneuver identification challenge", "abstract": "AI algorithms that identify maneuvers from trajectory data could play an important role in improving flight safety and pilot training. AI challenges allow diverse teams to work together to solve hard problems and are an effective tool for developing AI solutions. AI challenges are also a key driver of AI computational requirements. The Maneuver Identification Challenge hosted at maneuver-id.mit.edu provides thousands of trajectories collected from pilots practicing in flight simulators, descriptions of maneuvers, and examples of these maneuvers performed by experienced pilots. Each trajectory consists of positions, velocities, and aircraft orientations normalized to a common coordinate system. Construction of the data set required significant data architecture to transform flight simulator logs into AI ready data, which included using a supercomputer for deduplication and data conditioning. There are three proposed \u2026", "year": 2021, "venue": "", "authors": "Kaira Samuel and Vijay Gadepally and David Jacobs and Michael Jones and Kyle McAlpin and Kyle Palko and Ben Paulk and Sid Samsi and Ho Chit Siu and Charles Yee and Jeremy Kepner"}, {"title": "Vertical, temporal, and horizontal scaling of hierarchical hypersparse graphblas matrices", "abstract": "Hypersparse matrices are a powerful enabler for a variety of network, health, finance, and social applications. Hierarchical hypersparse GraphBLAS matrices enable rapid streaming updates while preserving algebraic analytic power and convenience. In many contexts, the rate of these updates sets the bounds on performance. This paper explores hierarchical hypersparse update performance on a variety of hardware with identical software configurations. The high-level language bindings of the GraphBLAS readily enable performance experiments on simultaneous diverse hardware. The best single process performance measured was 4,000,000 updates per second. The best single node performance measured was 170,000,000 updates per second. The hardware used spans nearly a decade and allows a direct comparison of hardware improvements for this computation over this time range; showing a 2x \u2026", "year": 2021, "venue": "", "authors": "Jeremy Kepner and Tim Davis and Chansup Byun and William Arcand and David Bestor and William Bergeron and Vijay Gadepally and Michael Houle and Matthew Hubbell and Michael Jones and Anna Klein and Lauren Milechin and Julie Mullen and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Charles Yee and Peter Michaleas"}, {"title": "Efficient access history for race detection", "abstract": "While there has been extensive research on race-detection algorithms for task-parallel programs, most of this research has focused on optimizing a particular component, namely, reachability analysis, which checks whether two instructions are logically in parallel. Little attention has been paid to the other important component, the access history, which stores all memory locations previous instructions have accessed. In theory, the access-history component adds no asymptotic overhead; however, in practice, it is often the most expensive component of race detection since it is queried and (possibly) updated at each memory access. We optimize this component based on the observation that, typically, strands within parallel programs access contiguous blocks of memory. Therefore, instead of maintaining the access history at the granularity of individual memory locations, we maintain it at the granularity of these \u2026", "year": 2021, "venue": "", "authors": "Yifan Xu and Anchengcheng Zhou and Grace Q Yin and Kunal Agrawal and I-Ting Angelina Lee and Tao B Schardl"}, {"title": "Linear probing revisited: Tombstones mark the death of primary clustering", "abstract": "First introduced in 1954, linear probing is one of the oldest data structures in computer science, and due to its unrivaled data locality, it continues to be one of the fastest hash tables in practice. It is widely believed and taught, however, that linear probing should never be used at high load factors; this is because primary-clustering effects cause insertions at load factor  to take expected time  (rather than the ideal ). The dangers of primary clustering, first discovered by Knuth in 1963, have been taught to generations of computer scientists, and have influenced the design of some of many widely used hash tables. We show that primary clustering is not a foregone conclusion. We demonstrate that small design decisions in how deletions are implemented have dramatic effects on the asymptotic performance of insertions, so that, even if a hash table operates continuously at a load factor , the expected amortized cost per operation is . This is because tombstones created by deletions actually cause an anti-clustering effect that combats primary clustering. We also present a new variant of linear probing (which we call graveyard hashing) that completely eliminates primary clustering on \\emph{any} sequence of operations: if, when an operation is performed, the current load factor is  for some , then the expected cost of the operation is . One corollary is that, in the external-memory model with a data blocks of size , graveyard hashing offers the following remarkable guarantee: at any load factor  satisfying , graveyard hashing achieves  expected block transfers per operation. Past external-memory hash tables have \u2026", "year": 2021, "venue": "", "authors": "Michael A Bender and Bradley C Kuszmaul and William Kuszmaul"}, {"title": "A metastudy of algorithm lower bounds", "abstract": "Algorithms are essential to the field of computer science, and algorithm designers are always searching for the mathematically optimal algorithms. Sherry and Thompson found that improvements to algorithm upper bounds have been steadily decreasing since the 1970s. In this work we aim to discover whether this could be because researchers have already found the optimal versions of many algorithms. In order to get a better sense of the picture, we compiled lower bounds on the algorithm families studied by Sherry and Thompson. We find that, while a few problems still have large gaps between upper and lower bounds where improvement is possible, over threequarters of these problems are already very close to being optimal! The \u201cslowing progress\u201d may in fact prove to be a triumph in disguise, as it is an indicator that many problems have achieved optimal solutions.", "year": 2021, "venue": "", "authors": "Emily Liu"}, {"title": "Floors and Ceilings in Divide-and-Conquer Recurrences\u2217", "abstract": "The master theorem is a core tool for algorithm analysis. Many applications use the discrete version of the theorem, in which floors and ceilings may appear within the recursion. Several of the known proofs of the discrete master theorem include substantial errors, however, and other known proofs employ sophisticated mathematics. We present an elementary and approachable proof that applies generally to Akra-Bazzi-style recurrences.", "year": 2021, "venue": "", "authors": "William Kuszmaul and Charles E Leiserson"}, {"title": "Cache-Efficient Parallel-Partition Algorithms Using Exclusive-Read-and-Write Memory", "abstract": "We present an in-place algorithm for the parallel-partition problem with linear work and polylogarithmic span. The algorithm uses only exclusive read/write shared variables and can be implemented using parallel-for-loops without any additional concurrency considerations (i.e., the algorithm is EREW). A key feature of the algorithm is that it exhibits provably optimal cache behavior up to small-order factors.We also present a second in-place EREW algorithm with work O(n) and span O(log n \u00b7 log log n), which is within an O(log log n) factor of the optimal span. By using this low-span algorithm as a subroutine within the cache-friendly algorithm, we obtain a single EREW algorithm that combines their theoretical guarantees: the algorithm achieves span O(log n \u00b7 log log n) and exhibits optimal cache behavior. As an immediate consequence, we also get an in-place EREW Quicksort algorithm with work O(n log n) and \u2026", "year": 2020, "venue": "", "authors": "William Kuszmaul and Alek Westover"}, {"title": "Multicore paging algorithms cannot be competitive", "abstract": "Every processor with multiple cores sharing a cache needs to implement a page-replacement algorithm. Lopez-Ortiz and Salinger [ITCS 2012] demonstrated that competitive ratio of canonical paging algorithms such as Least-Recently-Used (LRU) and Furthest-In-Future (FIF) grows with the length of the input. In this paper, we answer an open question about the existence of competitive multicore paging algorithms in the negative. Specifically, we show that all lazy algorithms, which include all practical algorithms, cannot be competitive against the optimal offline algorithm.", "year": 2020, "venue": "", "authors": "Shahin Kamali and Helen Xu"}, {"title": "Computing included and excluded sums using parallel prefix", "abstract": "Many scientific computing applications involve reducing elements in overlapping subregions of a multidimensional array. For example, the integral image problem from image processing requires finding the sum of elements in arbitrary axis-aligned subregions of an image. Furthermore, the fast multipole method, a widely used kernel in particle simulations, relies on reducing regions outside of a bounding box in a multidimensional array to a representative multipole expansion for certain interactions. We abstract away the application domains and define the underlying included and excluded sums problems of reducing regions inside and outside (respectively) of an axis-aligned bounding box in a multidimensional array. In this thesis, we present the dimension-reduction excluded-sums (DRES) algorithm, an asymptotically improved algorithm for the excluded sums problem in arbitrary dimensions and compare it with the state-of-the-art algorithm by Demaine et al.", "year": 2020, "venue": "", "authors": "Sean Sean Cameron Burrows Fraser"}, {"title": "Brief announcement: Fast concurrent cuckoo kick-out eviction schemes for high-density tables", "abstract": "Cuckoo hashing guarantees constant-time lookups regardless of table density, making it a viable candidate for high-density tables. Cuckoo hashing insertions perform poorly at high table densities, however. In this paper, we mitigate this problem through the introduction of novel kick-out eviction algorithms. Experimentally, our algorithms reduce the number of bins viewed per insertion for high-density tables by as much as a factor of ten. We also implement an optimistic concurrency scheme for serializable multi-writer cuckoo hash tables (not using hardware transactional memory). For delete-light loads, one of our kick-out schemes avoids all competition between insertions with high probability, and significantly reduces transaction-abort frequency. This result is extended to arbitrary workloads using a new mechanism called a claim flag.", "year": 2016, "venue": "", "authors": "William Kuszmaul"}, {"title": "A new upper bound for the growth factor in Gaussian elimination with complete pivoting", "abstract": "The growth factor in Gaussian elimination measures how large the entries of an LU factorization can be relative to the entries of the original matrix. It is a key parameter in error estimates, and one of the most fundamental topics in numerical analysis. We produce an upper bound of n0.2079lnn+0.91$n^{0.2079 \\ln n +0.91}$ for the growth factor in Gaussian elimination with complete pivoting \u2014 the first improvement upon Wilkinson's original 1961 bound of 2n0.25lnn+0.5$2 \\, n ^{0.25\\ln n +0.5}$.", "year": 2025, "venue": "", "authors": "Ankit Bisain and Alan Edelman and John Urschel"}, {"title": "Sprout: Green generative AI with carbon-efficient LLM inference", "abstract": "The rapid advancement of generative AI has heightened environmental concerns, particularly regarding carbon emissions. Our framework, Sprout, addresses these challenges by reducing the carbon footprint of inference in large language models (LLMs). Sprout introduces \u201cgeneration directives\u201d to guide the autoregressive generation process, achieving a balance between ecological sustainability and high-quality outputs. By employing a strategic optimizer for directive assignment and a novel offline quality evaluator, Sprout reduces the carbon footprint of generative LLM inference by over 40% in real-world evaluations, using the Llama model and global electricity grid data. This work is crucial as the rising interest in inference time compute scaling laws amplifies environmental concerns, emphasizing the need for eco-friendly AI solutions.", "year": 2024, "venue": "", "authors": "Baolin Li and Yankai Jiang and Vijay Gadepally and Devesh Tiwari"}, {"title": "Population Transformer: Learning population-level representations of neural activity", "abstract": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scal. We address two key challenges in scaling models with neural time-series data: sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained representations and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight and more interpretable, while still retaining competitive performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we \u2026", "year": 2024, "venue": "", "authors": "Geeling Chau and Christopher Wang and Sabera Talukder and Vighnesh Subramaniam and Saraswati Soedarmadji and Yisong Yue and Boris Katz and Andrei Barbu"}, {"title": "Building HPC Learning Pathways: Understanding our Community", "abstract": " Demand for specialist High Performance Computing (HPC) skills is growing rapidly. To support this demand, we need to improve the opportunities for skills development. Existing approaches to HPC training often involve growing skills and experience over a number of years of on-the-job training. However, we already face a skills shortage that can\u2019t be addressed with current training provisions. Clear and well-structured \u201ctraining pathways\u201d can offer an important means of helping learners to understand what they need to learn, in what order, so that they can develop the skills that they need. Development of such pathways requires a clearer understanding of the learning aims of members of the HPC community. In this paper, we present information gathered at a BoF session run during the SC23 supercomputing conference. This information provides insight into the current aims and perspectives of community \u2026", "year": 2024, "venue": "", "authors": "Weronika Filinger and Julie Mullen and Jeremy Cohen and Samantha Wittke and Ann Backhaus"}, {"title": "Disciplined geodesically convex programming", "abstract": "Convex programming plays a fundamental role in machine learning, data science, and engineering. Testing convexity structure in nonlinear programs relies on verifying the convexity of objectives and constraints. \\citet{grant2006disciplined} introduced a framework, Disciplined Convex Programming (DCP), for automating this verification task for a wide range of convex functions that can be decomposed into basic convex functions (atoms) using convexity-preserving compositions and transformations (rules). However, the restriction to Euclidean convexity concepts can limit the applicability of the framework. For instance, many notable instances of statistical estimators and matrix-valued (sub)routines in machine learning applications are Euclidean non-convex, but exhibit geodesic convexity through a more general Riemannian lens. In this work, we extend disciplined programming to this setting by introducing Disciplined Geodesically Convex Programming (DGCP). We determine convexity-preserving compositions and transformations for geodesically convex functions on general Cartan-Hadamard manifolds, as well as for the special case of symmetric positive definite matrices, a common setting in matrix-valued optimization. For the latter, we also define a basic set of atoms. Our paper is accompanied by a Julia package SymbolicAnalysis.jl, which provides functionality for testing and certifying DGCP-compliant expressions. Our library interfaces with manifold optimization software, which allows for directly solving verified geodesically convex programs.", "year": 2024, "venue": "", "authors": "Andrew Cheng and Vaibhav Dixit and Melanie Weber"}, {"title": "Securellm: Using compositionality to build provably secure language models for private, sensitive, and secret data", "abstract": "Traditional security mechanisms isolate resources from users who should not access them. We reflect the compositional nature of such security mechanisms back into the structure of LLMs to build a provably secure LLM; that we term SecureLLM. Other approaches to LLM safety attempt to protect against bad actors or bad outcomes, but can only do so to an extent making them inappropriate for sensitive data. SecureLLM blends access security with fine-tuning methods. Each data silo has associated with it a separate fine-tuning and a user has access only to the collection of fine-tunings that they have permission for. The model must then perform on compositional tasks at the intersection of those data silos with the combination of those individual fine-tunings. While applicable to any task like document QA or making API calls, in this work we concern ourselves with models that learn the layouts of new SQL databases to provide natural-language-to-SQL translation capabilities. Existing fine-tuning composition methods fail in this challenging environment, as they are not well-equipped for handling compositional tasks. Compositionality remains a challenge for LLMs. We contribute both a difficult new compositional natural-language-to-SQL translation task and a new perspective on LLM security that allows models to be deployed to secure environments today.", "year": 2024, "venue": "", "authors": "Abdulrahman Alabdulkareem and Christian M Arnold and Yerim Lee and Pieter M Feenstra and Boris Katz and Andrei Barbu"}, {"title": "A modified aeromagnetic compensation method robust to in-cabin obe interferences", "abstract": "Aeromagnetic compensation plays a vital role in geomagnetic navigation and has received considerable attention throughout last few decades. Classical aeromagnetic compensation methods based on the Tolles\u2013Lawson (TL) model are mainly aimed at permanent, induced, and eddy-current magnetic interferences of aircraft platform, which ignores other stray magnetic field interference on the platform including the interferences caused by on-board electronic (OBE) systems. In order to cooperate with TL model, magnetometers are usually required to be installed on the extension rod outside the cabin, which is widely applied to geophysical magnetic survey. In order to ensure safety and reduce the cost of platform modification in geomagnetic navigation, it\u2019s necessary to place magnetometers inside the cabin. It also further exacerbates the magnetic interferences and improves the difficulty of magnetic compensation. In this paper, a modified aeromagnetic compensation method is proposed, and the in-cabin OBE interferences are respectively modelled to be proportional to the currents and their temporal variations of different electronic devices. To ensure that modified model adapts to strong OBE interference in the cabin, a cut-off frequency determination method based on curvature calculation and a feature selection method based on correlation calculation are proposed. The cutoff frequency determination method helps to select passband filter which suitable for in-cabin OBE interference. The feature selection method can help to effectively select current and voltage inputs for modified model. In addition, principal component analysis (PCA) is \u2026", "year": 2024, "venue": "", "authors": "Yuxin Liu and Wen Li and Dongyan Wei and Ge Shen"}, {"title": "Scalable deep learning for pilot performance analysis using multimodal physiological time series", "abstract": "Sensors used to collect human physiological data often necessitate the processing and classification of time series data, which can quickly become intractable with very lengthy inputs or many time series features. In this study we compared the performance of two methods of time series feature extraction and dimensionality reduction, Minimally Random Convolutional Kernel Transform (MiniRocket) and statistical feature engi-neering using TSFresh, to determine the optimal hardware configurations and associated performance-accuracy trade-offs between model speed and complexity. Our results showed that MiniRocket scales extremely well with only linear complexity while the scaling of TSFresh is dependent on the set of features selected for computation. Further, MiniRocket outperformed the TSFresh model accuracy for all configurations except the most comprehensive (but slowest) feature extraction set thereby \u2026", "year": 2023, "venue": "", "authors": "Noah Lee and Patrick W Moore and Laura J Brattain"}, {"title": "Mapping of Internet \u201cCoastlines\u201d via Large Scale Anonymized Network Source Correlations", "abstract": "Expanding the scientific tools available to protect computer networks can be aided by a deeper understanding of the underlying statistical distributions of network traffic and their potential geometric interpretations. Analyses of large scale network observations provide a unique window into studying those underlying statistics. Newly developed GraphBLAS hypersparse matrices and D4M associative array technologies enable the efficient anonymized analysis of network traffic on the scale of trillions of events. This work analyzes over 100,000,000,000 anonymized packets from the largest Internet telescope (CAIDA) and over 10,000,000 anonymized sources from the largest commercial honeyfarm (GreyNoise). Neither CAIDA nor GreyNoise actively emit Internet traffic and provide distinct observations of unsolicited Internet traffic (primarily botnets and scanners). Analysis of these observations confirms the previously \u2026", "year": 2023, "venue": "", "authors": "Hayden Jananthan and Jeremy Kepner and Michael Jones and William Arcand and David Bestor and William Bergeron and Chansup Byun and Timothy Davis and Vijay Gadepally and Daniel Grant and Michael Houle and Matthew Hubbell and Anna Klein and Lauren Milechin and Guillermo Morales and Andrew Morris and Julie Mullen and Ritesh Patel and Alex Pentl and Sandeep Pisharody and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Tyler Trigg and Gabriel Wachman and Charles Yee and Peter Michaleas"}, {"title": "Supplementary Information: Mitigating Confirmation Bias in Semi-supervised Learning via Efficient Bayesian Model Averaging", "abstract": "State-of-the-art (SOTA) semi-supervised learning (SSL) methods have been highly successful in leveraging a mix of labeled and unlabeled data, often via self-training or pseudo-labeling. During pseudo-labeling, the model's predictions on unlabeled data are used for training and may result in confirmation bias where the model reinforces its own mistakes. In this work, we show that SOTA SSL methods often suffer from confirmation bias and demonstrate that this is often a result of using a poorly calibrated classifier for pseudo labeling. We introduce BaM-SSL, an efficient Bayesian Model averaging technique that improves uncertainty quantification in SSL methods with limited computational or memory overhead. We demonstrate that BaM-SSL mitigates confirmation bias in SOTA SSL methods across standard vision benchmarks of CIFAR-10, CIFAR-100, giving up to 16% improvement in test accuracy on the CIFAR-100 with 400 labels benchmark. Furthermore, we also demonstrate their effectiveness in additional realistic and challenging problems, such as class-imbalanced datasets and in photonics science.", "year": 2023, "venue": "", "authors": "Charlotte Loh and Rumen Dangovski"}, {"title": "Performance Bounds for Quantum Control", "abstract": "Quantum feedback controllers often lack performance targets and optimality certificates. We combine quantum filtering theory and moment-sum-of-squares techniques to construct a hierarchy of convex optimization problems that furnish monotonically improving, computable bounds on the best attainable performance for a large class of quantum feedback control problems. We prove convergence of the bounds under technical assumptions and demonstrate the practical utility of our approach by designing certifiably near-optimal controllers for a qubit in a cavity subjected to continuous photon counting and homodyne detection measurements.", "year": 2023, "venue": "", "authors": "Flemming Holtorf and Frank Sch\u00e4fer and Julian Arnold and Christopher Rackauckas and Alan Edelman"}, {"title": "CorBit: Leveraging Correlations for Compressing Bitmap Indexes.", "abstract": "A bitmap index is a secondary index structure that supports equality and range predicates. In its simplest form, a bitmap index stores one bitmap per unique column value indicating qualifying tuples. To use such indexes in large-scale data warehousing, they need to be space efficient. Existing schemes such as Roaring can compress individual bitmaps but do not consider cross-column compression.In this paper, we introduce CorBit, a technique that leverages column correlations to compress bitmap indexes on a given table. The high-level idea is to only store the bits that need to be flipped (the diff) when encoding the bitmaps of a column that correlates with another column that already has a bitmap index in place. CorBit automatically determines which columns to store an index for and which column bitmaps to diff-encode, minimizing the overall size of the index. Compared to Roaring, CorBit consumes 9.1% less space on the DMV dataset while incurring a 12.6% runtime overhead.", "year": 2023, "venue": "", "authors": "Xi Lyu and Andreas Kipf and Pascal Pfeil and Dominik Horn and Jana Giceva and Tim Kraska"}, {"title": "Interpreting neural policies with disentangled tree representations", "abstract": "The advancement of robots, particularly those functioning in complex human-centric environments, relies on control solutions that are driven by machine learning. Understanding how learning-based controllers make decisions is crucial since robots are often safety-critical systems. This urges a formal and quantitative understanding of the explanatory factors in the interpretability of robot learning. In this paper, we aim to study interpretability of compact neural policies through the lens of disentangled representation. We leverage decision trees to obtain factors of variation [1] for disentanglement in robot learning; these encapsulate skills, behaviors, or strategies toward solving tasks. To assess how well networks uncover the underlying task dynamics, we introduce interpretability metrics that measure disentanglement of learned neural dynamics from a concentration of decisions, mutual information and modularity perspective. We showcase the effectiveness of the connection between interpretability and disentanglement consistently across extensive experimental analysis.", "year": 2022, "venue": "", "authors": "Tsun-Hsuan Wang and Wei Xiao and Tim Seyde and Ramin Hasani and Daniela Rus"}, {"title": "Are all vision models created equal? a study of the open-loop to closed-loop causality gap", "abstract": "There is an ever-growing zoo of modern neural network models that can efficiently learn end-to-end control from visual observations. These advanced deep models, ranging from convolutional to patch-based networks, have been extensively tested on offline image classification and regression tasks. In this paper, we study these vision architectures with respect to the open-loop to closed-loop causality gap, i.e., offline training followed by an online closed-loop deployment. This causality gap typically emerges in robotics applications such as autonomous driving, where a network is trained to imitate the control commands of a human. In this setting, two situations arise: 1) Closed-loop testing in-distribution, where the test environment shares properties with those of offline training data. 2) Closed-loop testing under distribution shifts and out-of-distribution. Contrary to recently reported results, we show that under proper training guidelines, all vision models perform indistinguishably well on in-distribution deployment, resolving the causality gap. In situation 2, We observe that the causality gap disrupts performance regardless of the choice of the model architecture. Our results imply that the causality gap can be solved in situation one with our proposed training guideline with any modern network architecture, whereas achieving out-of-distribution generalization (situation two) requires further investigations, for instance, on data diversity rather than the model architecture.", "year": 2022, "venue": "", "authors": "Mathias Lechner and Ramin Hasani and Alexander Amini and Tsun-Hsuan Wang and Thomas A Henzinger and Daniela Rus"}, {"title": "Developing a series of ai challenges for the united states department of the air force", "abstract": "American leadership in AI. These broad strategy documents have influenced organizations such as the United States Department of the Air Force (DAF). The DAF-MIT AI Accelerator is an initiative between the DAF and MIT to bridge the gap between AI researchers and DAF mission requirements. Several projects supported by the DAF-MIT AI Accelerator are developing public challenge problems that address numerous Federal AI research priorities. These challenges target priorities by making large, AI-ready datasets publicly available, incentivizing open-source solutions, and creating a demand signal for dual use technologies that can stimulate further research. In this article, we describe these public challenges being developed and how their application contributes to scientific advances.", "year": 2022, "venue": "", "authors": "Vijay Gadepally and Gregory Angelides and Andrei Barbu and Andrew Bowne and Laura J Brattain and Tamara Broderick and Armando Cabrera and Glenn Carl and Ronisha Carter and Miriam Cha and Emilie Cowen and Jesse Cummings and Bill Freeman and James Glass and Sam Goldberg and Mark Hamilton and Thomas Heldt and Kuan Wei Huang and Phillip Isola and Boris Katz and Jamie Koerner and Yen-Chen Lin and David Mayo and Kyle McAlpin and Taylor Perron and Jean Piou and Hrishikesh M Rao and Hayley Reynolds and Kaira Samuel and Siddharth Samsi and Morgan Schmidt and Leslie Shing and Olga Simek and Brandon Swenson and Vivienne Sze and Jonathan Taylor and Paul Tylkin and Mark Veillette and Matthew L Weiss and Allan Wollaber and Sophia Yuditskaya and Jeremy Kepner"}, {"title": "Entangled residual mappings", "abstract": "Residual mappings have been shown to perform representation learning in the first layers and iterative feature refinement in higher layers. This interplay, combined with their stabilizing effect on the gradient norms, enables them to train very deep networks. In this paper, we take a step further and introduce entangled residual mappings to generalize the structure of the residual connections and evaluate their role in iterative learning representations. An entangled residual mapping replaces the identity skip connections with specialized entangled mappings such as orthogonal, sparse, and structural correlation matrices that share key attributes (eigenvalues, structure, and Jacobian norm) with identity mappings. We show that while entangled mappings can preserve the iterative refinement of features across various deep models, they influence the representation learning process in convolutional networks differently than attention-based models and recurrent neural networks. In general, we find that for CNNs and Vision Transformers entangled sparse mapping can help generalization while orthogonal mappings hurt performance. For recurrent networks, orthogonal residual mappings form an inductive bias for time-variant sequences, which degrades accuracy on time-invariant tasks.", "year": 2022, "venue": "", "authors": "Mathias Lechner and Ramin Hasani and Zahra Babaiee and Radu Grosu and Daniela Rus and Thomas A Henzinger and Sepp Hochreiter"}, {"title": "The mit supercloud workload classification challenge", "abstract": "High-Performance Computing (HPC) centers and cloud providers support an increasingly diverse set of applications on heterogenous hardware. As Artificial Intelligence (AI) and Machine Learning (ML) workloads have become an increasingly larger share of the compute workloads, new approaches to optimized resource usage, allocation, and deployment of new AI frameworks are needed. By identifying compute workloads and their utilization characteristics, HPC systems may be able to better match available resources with the application demand. By leveraging datacenter instrumentation, it may be possible to develop AI-based approaches that can identify workloads and provide feedback to researchers and datacenter operators for improving operational efficiency. To enable this research, we released the MIT Supercloud Dataset, which provides de-tailed monitoring logs from the MIT Supercloud cluster. This \u2026", "year": 2022, "venue": "", "authors": "Benny J Tang and Qiqi Chen and Matthew L Weiss and Nathan C Frey and Joseph McDonald and David Bestor and Charles Yee and William Arcand and William Bergeron and Chansup Byun and Daniel Edelman and Michael Houle and Matthew Hubbell and Michael Jones and Jeremy Kepner and Anna Klein and Adam Michaleas and Peter Michaleas and Lauren Milechin and Julia Mullen and Andrew Prout and Albert Reuther and Antonio Rosa and Andrew Bowne and Lindsey McEvoy and Baolin Li and Devesh Tiwari and Jiay Gadepally and Siddharth Samsi"}, {"title": "Autonomous flight arcade challenge: Single-and multi-agent learning environments for aerial vehicles", "abstract": "The Autonomous Flight Arcade (AFA) is a novel suite of singleand multi-agent learning environments for control of aerial vehicles. These environments incorporate realistic physics using the Unity game engine with diverse objectives and levels of decisionmaking sophistication. In addition to the environments themselves, we introduce an interface for interacting with them, including the ability to vary key parameters, thereby both changing the difficulty and the core challenges. We also introduce a pipeline for collecting human gameplay within the environments. We demonstrate the performance of artificial agents in these environments trained using deep reinforcement learning, and also motivate these environments as a benchmark for designing non-learned classical control policies and agents trained using imitation learning from human demonstrations. Finally, we motivate the use of AFA environments as a testbed for training artificial agents capable of cooperative human-AI decision making, including parallel autonomy.", "year": 2022, "venue": "", "authors": "Paul Tylkin and Tsun-Hsuan Wang and Tim Seyde and Kyle Palko and Ross Allen and Alexander Amini and Daniela Rus"}, {"title": "Cislunar space situational awareness sensor tasking using deep reinforcement learning agents", "abstract": "Cislunar space is gaining popularity with numerous missions being planned for the near future. However, operating in the cislunar space domain poses additional risk to satellites due to the lack of space situational awareness in the regime. Without maintaining a proper catalog of resident space objects in cislunar space, active space assets are susceptible to catastrophic collisions with untracked resident space objects (RSOs). The cislunar orbital regime is unique in that propagation of orbits is not easily predicted nor learned due to the complex three-body dynamics. In this work, we explored the usage of a deep reinforcement learning agent to optimally task a narrow field of view ground-based optical telescope for cislunar space situational awareness. The performance of our trained agent is compared to two random policies; a policy that randomly select a direction to observe and a policy that randomly select a RSO within the field of regard to observe.", "year": 2022, "venue": "", "authors": "Peng Mun Siew and Daniel Jang and Thomas G Roberts and Richard Linares and Justin Fletcher"}, {"title": "Airline disruption management with delay ledgers", "abstract": "The impact of disruptions may result in reduced capacities at airports, forcing airlines to revise schedules and delay flights. However, due to myriad factors (eg, passengers who may miss their connections, remaining flights to be performed by an aircraft, high-valued passengers with elite statuses), a delayed flight may be more or less costly to an airline, even when compared to another similarly delayed flight. Currently, identifying optimal slot swaps between airlines requires sharing the airline-specific delay cost of each flight. However, this is not amenable as sharing these private delay costs could reveal sensitive business practices. We propose the use of a procedure called the Delay Ledger (DELED) which enables airlines to identify a set of beneficial slot swaps across a network of airports which guarantees improvements in terms of private delay costs while ensuring that no private flight-specific valuations are shared. DELED is guaranteed to lower airline delay costs, incentivizes truthful airline participation, and supports flexible airline privacy preferences. We evaluate DELED across 30 days with 8 major US airlines, resulting in average reductions in private delay costs of 8-22% per day compared to current approaches.", "year": 2022, "venue": "", "authors": "Hamsa Balakrishnan"}, {"title": "Meta-learning and self-supervised pretraining for real world image translation", "abstract": "Recent advances in deep learning, in particular enabled by hardware advances and big data, have provided impressive results across a wide range of computational problems such as computer vision, natural language, or reinforcement learning. Many of these improvements are however constrained to problems with large-scale curated data-sets which require a lot of human labor to gather. Additionally, these models tend to generalize poorly under both slight distributional shifts and low-data regimes. In recent years, emerging fields such as meta-learning or self-supervised learning have been closing the gap between proof-of-concept results and real-life applications of machine learning by extending deep-learning to the semi-supervised and few-shot domains. We follow this line of work and explore spatio-temporal structure in a recently introduced image-to-image translation problem in order to: i) formulate a novel multi-task few-shot image generation benchmark and ii) explore data augmentations in contrastive pre-training for image translation downstream tasks. We present several baselines for the few-shot problem and discuss trade-offs between different approaches. Our code is available at https://github.com/irugina/meta-image-translation.", "year": 2021, "venue": "", "authors": "Ileana Rugina and Rumen Dangovski and Mark Veillette and Pooya Khorrami and Brian Cheung and Olga Simek and Marin Solja\u010di\u0107"}, {"title": "Incremental edge orientation in forests", "abstract": "For any forest  it is possible to orient the edges  so that no vertex in  has out-degree greater than . This paper considers the incremental edge-orientation problem, in which the edges  arrive over time and the algorithm must maintain a low-out-degree edge orientation at all times. We give an algorithm that maintains a maximum out-degree of  while flipping at most  edge orientations per edge insertion, with high probability in . The algorithm requires worst-case time  per insertion, and takes amortized time . The previous state of the art required up to  edge flips per insertion. We then apply our edge-orientation results to the problem of dynamic Cuckoo hashing. The problem of designing simple families  of hash functions that are compatible with Cuckoo hashing has received extensive attention. These families  are known to satisfy \\emph{static guarantees}, but do not come typically with \\emph{dynamic guarantees} for the running time of inserts and deletes. We show how to transform static guarantees (for -associativity) into near-state-of-the-art dynamic guarantees (for -associativity) in a black-box fashion. Rather than relying on the family  to supply randomness, as in past work, we instead rely on randomness within our table-maintenance algorithm.", "year": 2021, "venue": "", "authors": "Michael A Bender and Tsvi Kopelowitz and William Kuszmaul and Ely Porat and Clifford Stein"}, {"title": "Air force crew scheduling: An integer optimization approach", "abstract": "Air Force flight, training, and crew scheduling is a labor-intensive and largely manual process across all flying squadrons. Complex training requirements and dependencies, operational constraints, numerous qualifications, and unforeseen missions confound the schedule development process. We develop multiple optimization formulations for the Air Force crew scheduling problem. Furthermore, we present multiple objective functions aiming at mimicking reality to account for pilot qualification upgrades and their ability to stay current and mission ready. To compare candidate schedules, we identify numerous metrics that show the impact of the different objective functions. Finally, we briefly discuss how to incorporate scheduler preferences and focus on creating human-interpretable schedules so that the scheduler can select the most desired schedule for the squadrons' current needs.", "year": 2021, "venue": "", "authors": "Matthew J Koch"}, {"title": "Interpretable neuroevolutionary models for learning non-differentiable functions and programs", "abstract": "A key factor in the modern success of deep learning is the astonishing expressive power of neural networks. However, this comes at the cost of complex, black-boxed models that are unable to extrapolate beyond the domain of the training dataset, conflicting with goals of expressing physical laws or building human-readable programs. In this paper, we introduce OccamNet, a neural network model that can find interpretable, compact and sparse solutions for fitting data, \u00e0 la Occam\u2019s razor. Our model defines a probability distribution over a non-differentiable function space, and we introduce an optimization method that samples functions and updates the weights based on cross-entropy matching in an evolutionary strategy: we train by biasing the probability mass towards better fitting solutions. We demonstrate that we can fit a variety of algorithms, ranging from simple analytic functions through recursive programs to even simple image classification. Our method takes minimal memory footprint, does not require AI accelerators for efficient training, fits complicated functions in minutes of training on a single CPU, and demonstrates significant performance gains when scaled on GPU. Our implementation, demonstrations and instructions for reproducing the experiments are available at https://github. com/AllanSCosta/occam-net.", "year": 2020, "venue": "", "authors": "Allan Costa and Rumen Dangovski and Samuel Kim and Pawan Goyal and M Solja\u010di\u0107 and Joseph Jacobson"}, {"title": "Programming technologies for engineering quality multicore software", "abstract": "This thesis is concerned with the development of programming technologies that reduce the complexity of parallel programming to make it easier for average programmers to exploit the capabilities of multicore hardware. I contend that realizing the full potential of the multicore revolution requires the development of programming technologies that make it easier to write quality code -- code that has a simple understandable structure and performs well in practice. These programming technologies broadly include parallel algorithms, data structures, optimization techniques, profiling tools, and system design principles. Along these ends, this thesis presents seven intellectual artifacts from the domains of parallel algorithms, multicore-centric systems for scientific computing, and programming tools that make it easier to write quality code by simplifying the design, analysis, and performance engineering of multicore software: --", "year": 2020, "venue": "", "authors": "Tim Tim FS Kaler"}, {"title": "Temporal fingerprints: Identity matching across fully encrypted domain", "abstract": "Technological advancements have significantly transformed communication patterns, introducing a diverse array of online platforms, thereby prompting individuals to use multiple profiles for different domains and objectives. Enhancing the understanding of cross domain identity matching capabilities is essential, not only for practical applications such as commercial strategies and cybersecurity measures, but also for theoretical insights into the privacy implications of data disclosure. In this study, we demonstrate that individual temporal data, in the form of inter-event times distribution, constitutes an individual temporal fingerprint, allowing for matching profiles across different domains back to their associated real-world entity. We evaluate our methodology on encrypted digital trading platforms within the Ethereum Blockchain and present impressing results in matching identities across these privacy-preserving domains, while outperforming previously suggested models. Our findings indicate that simply knowing when an individual is active, even if information about who they talk to and what they discuss is lacking, poses risks to users' privacy, highlighting the inherent challenges in preserving privacy in today's digital landscape.", "year": 2024, "venue": "", "authors": "Shahar Somin and Keeley Erhardt and Alex'Sandy' Pentland"}, {"title": "Mallet: SQL Dialect Translation with LLM Rule Generation", "abstract": "Translating between the SQL dialects of different systems is important for migration and federated query processing. Existing approaches rely on hand-crafted translation rules, which tend to be incomplete and hard to maintain, especially as the number of dialects to translate increases. Thus, dialect translation remains a largely unsolved problem.To address this issue, we introduce Mallet, a system that leverages Large Language Models (LLMs) to automate the generation of SQL-to-SQL translation rules, namely schema conversion, automated UDF generation, extension selection, and expression composition. Once the rules are generated, they are infinitely reusable on new workloads without putting the LLM on the critical path of query execution. Mallet enhances the accuracy of the LLMs by (1) performing retrieval augmented generation (RAG) over system documentation and human expertise, (2) subjecting the \u2026", "year": 2024, "venue": "", "authors": "Amadou Latyr Ngom and Tim Kraska"}, {"title": "Space lower bounds for dynamic filters and value-dynamic retrieval", "abstract": "A filter is a data structure that answers approximate-membership queries on a set S of n elements, with a false-positive rate of \u0454. A filter is said to be dynamic if it supports insertions/deletions to the set S, subject to a capacity constraint of n.   This paper considers the space requirement of filters, regardless of running time.  It has been known for decades that static filters have optimal space n log\u0454\u22121 + O(1) expected bits, and that dynamic filters can be implemented in space n log\u0454\u22121 + \u0398(n) bits. We prove that this \u0398(n)-bit gap is fundamental: any dynamic filter must use n log\u0454\u22121 + \u03a9(n) bits, no matter the choice of \u0454.   Extending our techniques, we are also able to obtain a lower bound for the value-dynamic retrieval problem. Here again, we show that there is a \u0398(n)-bit gap between the optimal static and (value-)dynamic solutions.", "year": 2024, "venue": "", "authors": "William Kuszmaul and Stefan Walzer"}, {"title": "H2G2-Net: A hierarchical heterogeneous graph generative network framework for discovery of multi-modal physiological responses", "abstract": "Discovering human cognitive and emotional states using multi-modal physiological signals draws attention across various research applications. Physiological responses of the human body are influenced by human cognition and commonly used to analyze cognitive states. From a network science perspective, the interactions of these heterogeneous physiological modalities in a graph structure may provide insightful information to support prediction of cognitive states. However, there is no clue to derive exact connectivity between heterogeneous modalities and there exists a hierarchical structure of sub-modalities. Existing graph neural networks are designed to learn on non-hierarchical homogeneous graphs with pre-defined graph structures; they failed to learn from hierarchical, multi-modal physiological data without a pre-defined graph structure. To this end, we propose a hierarchical heterogeneous graph generative network (H2G2-Net) that automatically learns a graph structure without domain knowledge, as well as a powerful representation on the hierarchical heterogeneous graph in an end-to-end fashion. We validate the proposed method on the CogPilot dataset that consists of multi-modal physiological signals. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.", "year": 2024, "venue": "", "authors": "Haidong Gu and Nathan Gaw and Yinan Wang and Chancellor Johnstone and Christine Beauchene and Sophia Yuditskaya and Hrishikesh Rao and Chun-An Chou"}, {"title": "Carbon in Motion: Characterizing Open-Sora on the Sustainability of Generative AI for Video Generation", "abstract": "The rapid rise of generative AI (GenAI) technologies has brought innovative video generation models like OpenAI\u2019s Sora to the forefront, but these advancements come with significant sustainability challenges due to their high carbon footprint. This paper presents a carbon-centric case study on video generation, providing the first systematic investigation into the environmental impact of this technology. By analyzing Open-Sora, an open-source text-to-video model inspired by OpenAI Sora, we identify the iterative diffusion denoising process as the primary source of carbon emissions. Our findings reveal that video generation applications are significantly more carbon-demanding than text-based GenAI models and that their carbon footprint is largely dictated by denoising step number, video resolution, and duration. To promote sustainability, we propose integrating carbon-aware credit systems and encouraging offline generation during high carbon intensity periods, offering a foundation for environmentally friendly practices in GenAI.", "year": 2024, "venue": "", "authors": "Baolin Li and Yankai Jiang and Devesh Tiwari"}, {"title": "Generalized multiagent reinforcement learning for coverage path planning in unknown, dynamic, and hazardous environments", "abstract": "The efficiency and efficacy of many coverage path problems can be critical to applications such as search and rescue. While improved coverage efficiency can be achieved with the use of multiple cooperative agents, the problem of determining the optimal joint action can be an exponentially complex optimization problem. Such complexity lends itself to a multiagent reinforcement learning (MARL) approach. Previous approaches, however, often train to specific, known, static environments; generating policies that do not port well to previously unseen and dynamic environments. Here we present an approach to the MARL coverage path problem which allows for a single policy to be applied to environments with varying numbers of disjoint, nonconvex areas of interest, to which the agents may have never been exposed and which may be changing. We do this while considering other priorities such as hazards to the \u2026", "year": 2024, "venue": "", "authors": "Andrea Henshall and Sertac Karaman"}, {"title": "Kernel-based tests for likelihood-free hypothesis testing", "abstract": "Given  observations from two balanced classes, consider the task of labeling an additional  inputs that are known to all belong to\\emph {one} of the two classes. Special cases of this problem are well-known: with completeknowledge of class distributions () theproblem is solved optimally by the likelihood-ratio test; when  it corresponds to binary classification; and when  it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-offbetween  and : increasing the data sample  reduces the amount  of training/simulationdata needed. In this work we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes--a case often encountered in practice;(b) study the minimax sample complexity for non-parametric classes of densities under\\textit {maximum meandiscrepancy}(MMD) separation; and (c) investigate the empirical performance of kernels parameterized by neural networks on two tasks: detectionof the Higgs boson and detection of planted DDPM generated images amidstCIFAR-10 images. For both problems we confirm the existence of the theoretically predicted asymmetric  vs  trade-off.", "year": 2023, "venue": "", "authors": "Patrik R\u00f3bert Gerber and Tianze Jiang and Yury Polyanskiy and Rui Sun"}, {"title": "Scalable Multi-Agent Sensor Tasking Using Deep Reinforcement Learning", "abstract": "Satellite launches have seen a dramatic increase in recent years, driven by the growth of commercial and government constellations for a range of applications, including communication, navigation, and Earth observation. While this trend provides numerous benefits, it also puts pressure on existing narrow Field of View (FOV) ground-based sensor networks to keep pace with the growing volume of objects. Also known as the curse of dimensionality, wherein the complexity of the object-tracking problem grows exponentially as the number of targets and length of the observation window grows linearly, this complexity grows even further when incorporating multiple agents acting within the same environment. Current methods for allocating sensors to specific tasks are often manual, time-consuming, and prone to human error. As a result, there is a need for more robust and sophisticated methods for monitoring and managing the space environment to ensure the safe and efficient use of this valuable resource. In this paper, we propose a Multiagent Reinforcement Learning (MARL) approach to model the relationships between Space Objects (SOs) and sensors toward a method that improves performance over previous techniques, as well as affords scalability and adaptation to novel scenarios.", "year": 2023, "venue": "", "authors": "Peng Mun Siew and Tory Smith and Ravi Ponmalai and Richard Linares"}, {"title": "Minimax optimal testing by classification", "abstract": "This paper considers an ML inspired approach to hypothesis testing known as classifier/classification-accuracy testing (CAT). In CAT, one first trains a classifier by feeding it labeled synthetic samples generated by the null and alternative distributions, which is then used to predict labels of the actual data samples. This method is widely used in practice when the null and alternative are only specified via simulators (as in many scientific experiments). We study goodness-of-fit, two-sample (TS) and likelihood-free hypothesis testing (LFHT), and show that CAT achieves (near-) minimax optimal sample complexity in both the dependence on the total-variation (TV) separation \u03b5 and the probability of error \u03b4 in a variety of non-parametric settings, including discrete distributions, d-dimensional distributions with a smooth density, and the Gaussian sequence model. In particular, we close the high probability sample complexity of LFHT for each class. As another highlight, we recover the minimax optimal complexity of TS over discrete distributions, which was recently established by Diakonikolas et al.(2021). The corresponding CAT simply compares empirical frequencies in the first half of the data, and rejects the null when the classification accuracy on the second half is better than random.", "year": 2023, "venue": "", "authors": "Patrik R Gerber and Yanjun Han and Yury Polyanskiy"}, {"title": "Interventions to Reduce AI Energy Requirements", "abstract": "The ever-growing computational requirements of AI and its associated development and deployment costs are a widely understood trend [1]\u2013[3]. This increasing computational demand naturally translates into increased energy usage and, in most cases, increased carbon emissions from datacenters where these models are developed and deployed. For example, particular Natural Language Processing (NLP) models can consume as much CO2 emission as the lifetime emission of 5 cars [4]. In particular, the widespread adoption, proliferation, and development of large neural networks has made it increasingly important for AI practitioners to account for the environmental and climate impacts of AI development. Creating power and energy efficient methods to train neural networks could reduce the carbon footprint of these models and thus lessen the environmental impact of AI. While there are numerous examples of research into efficient machine learning models [5],[6], the focus of our paper is on easyto-implement interventions that can be readily applied by ML practitioners without significant modifications to their code. Further, some of these interventions seem to provide energy efficiency gains almost \u201cfor free\u201d in that they do not affect the accuracy or precision of the trained model and may incur minimal changes in computational performance. To illustrate the potential impact of these interventions, we highlight selected results via a popular neural network architecture on a common computer vision benchmark, to quantify improvements in energy efficiency and corresponding changes to model accuracy with relatively simple, straightforward \u2026", "year": 2023, "venue": "", "authors": "Daniel Edelman and Joseph McDonald and David Bestor and Michael Jones and Baolin Li and Devesh Tiwari and Dan Zhao and Siddharth Samsi and Vijay Gadepally"}, {"title": "Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?", "abstract": "The rise of Generative Artificial Intelligence systems (\" Al systems\") parallels the Greek myth of Pandora who was overwhelmed with curiosity and opened the Box,\"[r] eleasing curses upon mankind.\"'However, Pandora's Box is not solely about evil or curses as the artifact-looking Box included Elpis, the personified spirit of", "year": 2023, "venue": "", "authors": "Dimitrios Ioannidis and Jeremy Kepner and Andrew Bowne and Harriet S Bryant"}, {"title": "Learning to optimize quasi-Newton methods", "abstract": "Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse Hessian in noisy loss landscapes and is capable of representing a wide range of inverse Hessians. We experimentally verify that our algorithm can optimize in noisy settings, and show that simpler alternatives for representing the inverse Hessians worsen performance. Lastly, we use our optimizer to train a semi-realistic deep neural network with 95k parameters at speeds comparable to those of standard neural network optimizers.", "year": 2022, "venue": "", "authors": "Isaac Liao and Rumen R Dangovski and Jakob N Foerster and Marin Solja\u010di\u0107"}, {"title": "Python implementation of the dynamic distributed dimensional data model", "abstract": "Python has become a standard scientific computing language with fast-growing support of machine learning and data analysis modules, as well as an increasing usage of big data. The Dynamic Distributed Dimensional Data Model (D4M) offers a highly composable, unified data model with strong performance built to handle big data fast and efficiently. In this work we present an implementation of D4M in Python. D4M.py implements all foundational functionality of D4M and includes Accumulo and SQL database support via Graphulo. We describe the mathematical background and motivation, an explanation of the approaches made for its fundamen-tal functions and building blocks, and performance results which compare D4M.py's performance to D4M-MATLAB and D4M.jl.", "year": 2022, "venue": "", "authors": "Hayden Jananthan and Lauren Milechin and Michael Jones and William Arcand and William Bergeron and David Bestor and Chansup Byun and Michael Houle and Matthew Hubbell and Vijay Gadepally and Anna Klein and Peter Michnlons and Guillermo Morales and Julie Mullen and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Charles Yee and Jeremy Kepner"}, {"title": "When Are Cache-Oblivious Algorithms Cache Adaptive? A Case Study of Matrix Multiplication and Sorting", "abstract": "Cache-adaptive algorithms are a class of algorithms that achieve optimal utilization of dynamically changing memory. These memory fluctuations are the norm in today\u2019s multi-threaded shared-memory machines and time-sharing caches.Bender et al.[8] proved that many cache-oblivious algorithms are optimally cache-adaptive, but that some cache-oblivious algorithms can be relatively far from optimally cache-adaptive on worstcase memory fluctuations. This worst-case gap between cache obliviousness and cache adaptivity depends on a highly-structured, adversarial memory profile. Existing cache-adaptive analysis does not predict the relative performance of cache-oblivious and cache-adaptive algorithms on non-adversarial profiles. Does the worst-case gap appear in practice, or is it an artifact of an unrealistically powerful adversary?", "year": 2022, "venue": "", "authors": "Arghya Bhattacharya and Abiyaz Chowdhury and Helen Xu and Rathish Das and Rezaul Alam Chowdhury and Rob Johnson and Rishab Nithyanand and Michael A Bender"}, {"title": "Graphulo: Linear algebra graph kernels", "abstract": "The Big Data and the Internet of Things era continue to challenge computational systems. These challenges, often referred to as the 3V\u2019s of big data, are compounded by heterogeneous data structures and sources. While several technologies, such as NoSQL and NewSQL databases, have been developed to address some of these challenges, these databases often support different underlying data representations and are largely designed to perform the same sets of operations. In order to generate meaningful results from large datasets, analysts often use a graph representation which provides an intuitive way to work with the data. In some cases, graph vertices can represent users and events, and edges can represent the relationship between these users and events. Graph algorithms are used to extract meaningful information from these very large graphs. The MIT Graphulo initiative (http://graphulo. mit. edu/) is \u2026", "year": 2022, "venue": "", "authors": "Lauren Milechin and Shana Hutchison and Hayden Jananthan and Jeremy Kepner and Benjamin A Miller and Andrew Prout and Siddharth Samsi and Chuck Yee and Vijay Gadepally"}, {"title": "The relationship between COVID-19 severity and computer science MOOC learner achievement: a preliminary analysis", "abstract": "Online education, and MOOCs in particular, experienced a dramatic rise during the COVID-19 lockdown. Many had extra time to start learning new topics, while a significant fraction of the population experienced disruptions in areas such as healthcare, childcare, and potential loss of livelihood, among others. In this work we analyze learner data from multiple instances of two introductory Python MOOCs, offered before and during the COVID-19 pandemic, to understand how the pandemic affected learner progress and outcomes in these courses. We explore multiple measures of COVID-19 severity, and find a strong correlation between a measure of severity and relative change in certification rate. Specifically, we find that the change in certification rate relative to pre-pandemic baselines showed a negative correlation as COVID-19 severity increased.", "year": 2022, "venue": "", "authors": "Michael Yee and Anindya Roy and Julius Stein and Meghan Perdue and Ana Bell and Ronisha Carter and Shigeru Miyagawa"}, {"title": "Loss curve approximations for fast neural architecture ranking & training elasticity estimation", "abstract": "Two key questions for any deep learning task involve questions around its optimization. First, when should we stop training or, alternatively, how long should we train for before the gains are not worth the continued training (i.e. early or optimal stopping)? Secondly, what is the \u201cright\u201d or best model: what training settings, hyper-parameters, and model architecture are best for the task at hand to maximize performance (i.e. architecture search)? Though essential, these questions are arguably also the most expensive parts of deep learning experimentation and the most unclear. Moreover, these expensive, exhaustive searches require large computational budgets that can carry large environmental footprints and significant energy expenditure. In this paper, we introduce a new method we call the Loss Curve Gradient Approximation (LCGA) that ranks model performance with minimal training. Using a wide variety of \u2026", "year": 2022, "venue": "", "authors": "Dan Zhao and Nathan C Frey and Vijay Gadepally and Siddharth Samsi"}, {"title": "Optimal time-backlog tradeoffs for the variable-processor cup game", "abstract": "The \\emph{-processor cup game} is a classic and widely studied scheduling problem that captures the setting in which a -processor machine must assign tasks to processors over time in order to ensure that no individual task ever falls too far behind. The problem is formalized as a multi-round game in which two players, a filler (who assigns work to tasks) and an emptier (who schedules tasks) compete. The emptier's goal is to minimize backlog, which is the maximum amount of outstanding work for any task. Recently, Kuszmaul and Westover (ITCS, 2021) proposed the \\emph{variable-processor cup game}, which considers the same problem, except that the amount of resources available to the players (i.e., the number  of processors) fluctuates between rounds of the game. They showed that this seemingly small modification fundamentally changes the dynamics of the game: whereas the optimal backlog in the fixed -processor game is , independent of , the optimal backlog in the variable-processor game is . The latter result was only known to apply to games with \\emph{exponentially many} rounds, however, and it has remained an open question what the optimal tradeoff between time and backlog is for shorter games. This paper establishes a tight trade-off curve between time and backlog in the variable-processor cup game. Importantly, we prove that for a game consisting of  rounds, the optimal backlog is  if and only if . Our techniques also allow for us to resolve several other open questions concerning how the variable-processor cup game behaves in beyond-worst-case-analysis settings.", "year": 2022, "venue": "", "authors": "William Kuszmaul and Shyam Narayanan"}, {"title": "Building heterogeneous cloud system for machine learning inference", "abstract": "Online inference is becoming a key service product for many businesses, deployed in cloud platforms to meet customer demands. Despite their revenue-generation capability, these services need to operate under tight Quality-of-Service (QoS) and cost budget constraints. This paper introduces KAIROS, a novel runtime framework that maximizes the query throughput while meeting QoS target and a cost budget. KAIROS designs and implements novel techniques to build a pool of heterogeneous compute hardware without online exploration overhead, and distribute inference queries optimally at runtime. Our evaluation using industry-grade deep learning (DL) models shows that KAIROS yields up to 2X the throughput of an optimal homogeneous solution, and outperforms state-of-the-art schemes by up to 70%, despite advantageous implementations of the competing schemes to ignore their exploration overhead.", "year": 2022, "venue": "", "authors": "Baolin Li and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "Cooperative flight control using visual-attention\u2013air-guardian", "abstract": "", "year": 2022, "venue": "", "authors": "Lianhao Yin and Tsun-Hsuan Wang and Makram Chahine and Tim Seyde and Mathias Lechner and Ramin Hasani and Daniela Rus"}, {"title": "Attention-Based Learning for Combinatorial Optimization", "abstract": "Combinatorial optimization problems, such as the Traveling Salesman Problem (TSP), have been studied for decades. However, with the rise of reinforcement learning in recent years, many of these problems are being revisited as a way to gauge these new models in different environments. In this thesis, we explore the use of a new type of model, the Decision Transformer, which is a Self-Attention Transformer architecture that was recently developed for training on reinforcement learning problems. To analyze the model, we structure the Traveling Salesman problem as a reinforcement learning problem and, by continuously varying parameters of the environment, measure its generalizability and success in this environment. This thesis aims to conduct an initial study of applying Decision Transformers to combinatorial optimization problems.\u00b9", "year": 2022, "venue": "", "authors": "Carson Smith"}, {"title": "Network requirements for distributed machine learning training in the cloud", "abstract": "In this thesis, I characterize the impact of network bandwidth on distributed machine learning training. I test four popular machine learning models (ResNet, DenseNet, VGG, and BERT) on an Nvidia A-100 cluster to determine the impact of bursty and non-bursty cross traffic (such as web-search traffic and long-lived flows) on the iteration time and throughput of distributed training. By varying the cross traffic load, I measure the impact of network congestion on training iteration times. I observe that with heavy web-search cross traffic (80% of link capacity), on average training iteration time is increased by up to 4 to 8\u00d7, for ResNet and BERT models, respectively. Further, I establish that the ring-all reduce communication collective is negatively impacted by network congestion even if the congestion is only affecting part of the ring. I also develop empirical models for the behavior of machine learning training in the presence of each type of cross traffic deployed. These results provide the motivation for developing novel congestion control protocols that are tailored for distributed training environments.", "year": 2022, "venue": "", "authors": "James Salamy"}, {"title": "The Locality-First Strategy for Developing Efficient Multicore Algorithm", "abstract": "To scale applications on multicores up to bigger problems, software systems must be optimized both for parallelism to take full advantage of the multiple cores and for locality to exploit the memory system for cache-friendliness. Parallelization alone does not suffice to reach peak performance due to the processor-memory gap: the increasing divergence of processor and memory speeds. Locality and parallelism are difficult to optimize for independently \u2014 and even more challenging to combine \u2014 because they tend to conflict with each other.  I advocate that algorithm developers employ a locality-first strategy for developing efficient parallel and cache-friendly algorithms for multicores. That is, they should first understand and exploit locality as much as possible before introducing parallelism. I argue that an algorithm developer can achieve high-performing code more easily with the locality-first strategy than with either a parallelism-first strategy or a strategy of trying to optimize both simultaneously.  I present ten artifacts that leverage the locality-first strategy to create fast multicore algorithms that are simple to describe and implement. For example, locality-first data structure design in graph processing achieves about 2\u00d7 speedup over the state of the art. Additionally, I prove mathematically that multicore cache-replacement algorithms that take advantage of locality outperform all other online algorithms. The other eight artifacts make similar contributions in their respective domains. Together, these artifacts demonstrate that the locality-first strategy provides an effective roadmap for algorithm developers to design and implement theoretically and \u2026", "year": 2022, "venue": "", "authors": "Helen Jiang Xu"}, {"title": "Detection of anomalous zigbee transmissions using machine learning", "abstract": "Effective spectrum awareness is critical to a large number of wireless communication systems. Malicious actors increasingly use the spectrum for their own purposes, such as to disrupt systems via jamming and/or spoofing. Radio anomaly detection approaches have been leveraged somewhat in wireless sensor networks, but most of these prior works have focused on detecting changes in sensor data (e.g., temperature and pressure), or in expert features rather than on anomalies occurring in the physical layer. This paper is focused on the detection of anomalous Zigbee transmissions using features extracted from the in-phase and quadrature components and network traffic data. We evaluated the performance of five supervised machine learning algorithms (i.e., Random Forest, J48, JRip, Naive Bayes, and PART) for anomalous RF detection and identified the best learner. Furthermore, we experimented with \u2026", "year": 2021, "venue": "", "authors": "Jarilyn M Hern\u00e1ndez Jim\u00e9nez and Hope Hong and Patrick Seipel"}, {"title": "Mathematics of Digital Hyperspace", "abstract": "Social media, e-commerce, streaming video, e-mail, cloud documents, web pages, traffic flows, and network packets fill vast digital lakes, rivers, and oceans that we each navigate daily. This digital hyperspace is an amorphous flow of data supported by continuous streams that stretch standard concepts of type and dimension. The unstructured data of digital hyperspace can be elegantly represented, traversed, and transformed via the mathematics of hypergraphs, hypersparse matrices, and associative array algebra. This paper explores a novel mathematical concept, the semilink, that combines pairs of semirings to provide the essential operations for graph analytics, database operations, and machine learning. The GraphBLAS standard currently supports hypergraphs, hypersparse matrices, the mathematics required for semilinks, and seamlessly performs graph, network, and matrix operations. With the addition of \u2026", "year": 2021, "venue": "", "authors": "Jeremy Kepner and Timothy Davis and Vijay Gadepally and Hayden Jananthan and Lauren Milechin"}, {"title": "Addressing climate change through community organizing and machine learning", "abstract": "Climate change is the challenge of our time. It is global, slow-moving, and impersonal \u2013 yet it has already impacted everyone from California to Kuwait. Humans are ill-adapted to this type of problem, as the scale \u2013 both in time and space \u2013 is too large to spur action. But to ignore the difficult choices ahead poses a catastrophic threat to humanity. To address the climate crisis \u2013 to mobilize societal change \u2013 we must make it meaningful to both decision-makers and the public.   In this thesis, I investigate two aims. Aim one is developing EarthDNA Ambassadors, a community that \u201cripens\u201d the issue of climate change by connecting leaders, empowering students, and engaging the world on climate. Aim two is developing the Earth Intelligence Engine, which uses AI to generate satellite images of the future, bridging the gap between AI experts, climate models, and decision-makers \u2013 starting with floods, the most frequent disaster in the US.  I developed and deployed training materials with dozens of young people to address the need for climate leaders. Surveys show the Ambassadors training program not only improves participants\u2019 negotiation and communication skills, but also improves their mindset for learning leadership. Furthermore, EarthDNA\u2019s Climate 101 workshop improves climate literacy and climate behaviors, and although long-lasting change is unlikely after one session, Climate 101 creates a privileged moment in which participants are more likely to increase their involvement in climate activism.   Working with a team, we also developed an initial framework for the Earth Intelligence Engine (EIE) to  generate satellite imagery of future floods \u2026", "year": 2021, "venue": "", "authors": "Brandon Leshchinskiy"}, {"title": "Multidimensional included and excluded sums", "abstract": "This paper presents algorithms for the included-sums and excluded-sums problems used by scientific computing applications such as the fast multipole method. These problems are defined in terms of a d-dimensional array of N elements and a binary associative operator \u2295 on the elements. The included-sum problem requires that the elements within overlapping boxes cornered at each element within the array be reduced using \u2295. The excluded-sum problem reduces the elements outside each box. The weak versions of these problems assume that the operator \u2295 has an inverse \u2296, whereas the strong versions do not require this assumption. In addition to studying existing algorithms to solve these problems, we introduce three new algorithms.The bidirectional box-sum (BDBS) algorithm solves the strong included-sums problem in \u0398(dN) time, asymptotically beating the classical summed-area table (SAT \u2026", "year": 2021, "venue": "", "authors": "Helen Xu and Sean Fraser and Charles E Leiserson"}, {"title": "High-throughput image alignment for connectomics using frugal snap judgments", "abstract": "The accuracy and computational efficiency of image alignment directly affects the advancement of connectomics, a field which seeks to understand the structure of the brain through electron microscopy. We introduce the algorithms Quilter and Stacker that are designed to perform 2D and 3D alignment respectively on petabyte-scale data sets from connectomics. Quilter and Stacker are efficient, scalable, and can run on hardware ranging from a researcher's laptop to a large computing cluster. On a single 18-core cloud machine each algorithm achieves throughputs of more than 1 TB/hr; when combined the algorithms produce an end-to-end alignment pipeline that processes data at a rate of 0.82 TB/hr - an over 10x improvement from previous systems. This efficiency comes from both traditional optimizations and from the use of \u201cFrugal Snap Judgments\u201d to judiciously exploit performance-accuracy trade-offs. A high \u2026", "year": 2020, "venue": "", "authors": "Tim Kaler and Brian Wheatman and Sarah Wooders"}, {"title": "New results on families of pattern-replacement equivalences", "abstract": "We study pattern-replacement equivalences of a similar nature to those studied by Linton, Propp, Roby, and West. Given any partition P of S c, we consider two equivalence relations acting on S n associated with P, loose P-equivalence and tight P-equivalence. We develop a general framework for studying tight pattern-replacement equivalences. Using our machinery, we count equivalence classes in S n for three tight equivalences posed as open problems by Linton, Propp, Roby, and West, and we find systems of equivalence-class representatives for two others, also previously posed as open problems. In addition, we study several infinite families of tight equivalences. In particular, we characterize the equivalence classes in S n under the most general tight equivalence, tight S c-equivalence. Moreover, we extend past work on loose P-equivalences (for various P) by characterizing equivalence classes under three \u2026", "year": 2020, "venue": "", "authors": "William Kuszmaul and Ziling Zhou"}, {"title": "Fast concurrent cuckoo kick-out eviction schemes for high-density tables", "abstract": "Cuckoo hashing guarantees constant-time lookups regardless of table density, making it a viable candidate for high-density tables. Cuckoo hashing insertions perform poorly at high table densities, however. In this paper, we mitigate this problem through the introduction of novel kick-out eviction algorithms. Experimentally, our algorithms reduce the number of bins viewed per insertion for high-density tables by as much as a factor of ten. We also introduce an optimistic concurrency scheme for transactional multi-writer cuckoo hash tables (not using hardware transactional memory). For delete-light workloads, one of our kick-out algorithms avoids all competition between insertions with high probability, and significantly reduces transaction-abort frequency. This result is extended to arbitrary workloads using a new synchronization mechanism called a claim flag.", "year": 2016, "venue": "", "authors": "William Kuszmaul"}, {"title": "New results on doubly adjacent pattern-replacement equivalences", "abstract": "In this paper, we consider the family of pattern-replacement equivalence relations referred to as the \"indices and values adjacent\" case. Each such equivalence is determined by a partition  of a subset of  for some . In 2010, Linton, Propp, Roby, and West posed a number of open problems in the area of pattern-replacement equivalences. Five, in particular, have remained unsolved until now, the enumeration of equivalence classes under the -equivalence, under the -equivalence, under the  equivalence, and under the -equivalence. We find formulas for three of the five equivalences and systems of representatives for the equivalence classes of the other two. We generalize our results to hold for all replacement partitions of , as well as for an infinite family of other replacement partitions. In addition, we characterize the equivalence classes in  under the -equivalence, finding a generalization of Stanley's results on the -equivalence. To do this, we introduce a notion of confluence that often allows one to find a representative element in each equivalence class under a given equivalence relation. Using an inclusion-exclusion argument, we are able to use this to count the equivalence classes under equivalence relations satisfying certain conditions.", "year": 2014, "venue": "", "authors": "William Kuszmaul"}, {"title": "Efficient d-ary Cuckoo Hashing at High Load Factors by Bubbling Up", "abstract": "A d-ary cuckoo hash table is an open-addressed hash table that stores each key x in one of d random positions h1 (x ), h 2(x ),\u2026, hd(x ). In the offline setting, where all items are given and keys need only be matched to locations, it is possible to support a load factor of 1 \u2014 \u03f5 while using  hashes. The online setting, where keys are moved as new keys arrive sequentially, has the additional challenge of the time to insert new keys, and it has not been known whether one can use d = O (ln \u03f5-1) hashes to support poly(\u03f5-1) expected-time insertions.In this paper, we introduce bubble-up cuckoo hashing, an implementation of d-ary cuckoo hashing that achieves all of the following properties simultaneously:\u2022 uses  hash locations per item for an arbitrarily small positive constant a.\u2022 achieves expected insertion time O (\u03b4-1) for any insertion taking place at load factor 1 \u2014 \u03b4 \u2264 1 \u2014 \u03f5.\u2022 achieves expected positive query time O \u2026", "year": 2025, "venue": "", "authors": "William Kuszmaul and Michael Mitzenmacher"}, {"title": "Long-horizon planning for multi-agent robots in partially observable environments", "abstract": "The ability of Language Models (LMs) to understand natural language makes them a powerful tool for parsing human instructions into task plans for autonomous robots. Unlike traditional planning methods that rely on domain-specific knowledge and handcrafted rules, LMs generalize from diverse data and adapt to various tasks with minimal tuning, acting as a compressed knowledge base. However, LMs in their standard form face challenges with long-horizon tasks, particularly in partially observable multi-agent settings. We propose an LM-based Long-Horizon Planner for Multi-Agent Robotics (LLaMAR), a cognitive architecture for planning that achieves state-of-the-art results in long-horizon tasks within partially observable environments. LLaMAR employs a plan-act-correct-verify framework, allowing self-correction from action execution feedback without relying on oracles or simulators. Additionally, we present MAP-THOR, a comprehensive test suite encompassing household tasks of varying complexity within the AI2-THOR environment. Experiments show that LLaMAR achieves a 30\\% higher success rate than other state-of-the-art LM-based multi-agent planners in MAP-THOR and Search\\& Rescue tasks. Code can be found at https://github. com/nsidn98/LLaMAR", "year": 2024, "venue": "", "authors": "Sid Nayak and Adelmo Morrison Orozco and Marina Have and Jackson Zhang and Vittal Thirumalai and Darren Chen and Aditya Kapoor and Eric Robinson and Karthik Gopalakrishnan and James Harrison and Anuj Mahajan and Brian Ichter and Hamsa Balakrishnan"}, {"title": "Batched sparse and mixed-precision linear algebra interface for efficient use of GPU hardware accelerators in scientific applications", "abstract": "Batched Sparse Linear Algebra has become an emergent processing mode on modern hardware accelerators based on Graphics Processing Units (GPUs) developed over the years to serve as the main compute devices in the largest computing clusters and supercomputers. We propose a set solver interface designs for batched sparse numerical solvers on these hardware accelerators. We motivate our specific designs by both their use in scientific applications of national importance and also by the possibility of implementing them in an efficient and portable manner with multiple options for vendor-specific optimizations. We present the C language interface calls for the linker-agnostic interchange of functional entry points. We also show how using C++ for the batched solvers simplifies the interface design while giving the user much broader set of opportunities for customization, testing, and debugging. We also \u2026", "year": 2024, "venue": "", "authors": "Piotr Luszczek and Ahmad Abdelfattah and Hartwig Anzt and Atsushi Suzuki and Stanimire Tomov"}, {"title": "Tight Bounds for Classical Open Addressing", "abstract": "We introduce a classical open-addressed hash table, called rainbow hashing, that supports a load factor of up to 1 , while also supporting  expected-time queries, and  expected-time insertions and deletions. We further prove that this tradeoff curve is optimal: any classical open-addressed hash table that supports load factor  must incur  expected time per operation. Finally, we extend rainbow hashing to the setting where the hash table is dynamically resized over time. Surprisingly, the addition of dynamic resizing does not come at any time cost-even while maintaining a load factor of  at all times, we can support  queries and  updates. Prior to our work, achieving any time bounds of the form  for all of insertions, deletions, and queries simultaneously remained an open Question.", "year": 2024, "venue": "", "authors": "Michael A Bender and William Kuszmaul and Renfei Zhou"}, {"title": "Blueprinting the Cloud: Unifying and Automatically Optimizing Cloud Data Infrastructures with BRAD--Extended Version", "abstract": "Modern organizations manage their data with a wide variety of specialized cloud database engines (e.g., Aurora, BigQuery, etc.). However, designing and managing such infrastructures is hard. Developers must consider many possible designs with non-obvious performance consequences; moreover, current software abstractions tightly couple applications to specific systems (e.g., with engine-specific clients), making it difficult to change after initial deployment. A better solution would virtualize cloud data management, allowing developers to declaratively specify their workload requirements and rely on automated solutions to design and manage the physical realization. In this paper, we present a technique called blueprint planning that achieves this vision. The key idea is to project data infrastructure design decisions into a unified design space (blueprints). We then systematically search over candidate blueprints using cost-based optimization, leveraging learned models to predict the utility of a blueprint on the workload. We use this technique to build BRAD, the first cloud data virtualization system. BRAD users issue queries to a single SQL interface that can be backed by multiple cloud database services. BRAD automatically selects the most suitable engine for each query, provisions and manages resources to minimize costs, and evolves the infrastructure to adapt to workload shifts. Our evaluation shows that BRAD meet user-defined performance targets and improve cost-savings by 1.6-13x compared to serverless auto-scaling or HTAP systems.", "year": 2024, "venue": "", "authors": "Geoffrey X Yu and Ziniu Wu and Ferdi Kossmann and Tianyu Li and Markos Markakis and Amadou Ngom and Samuel Madden and Tim Kraska"}, {"title": "Developing HPC Learning Pathways: Challenges and Recommendations", "abstract": " Recent growth in computational research, touching all disciplines and all aspects of research and discovery, has resulted in increased demand for High Performance Computing (HPC) skills among members of the research community. Providing adequate training for practitioners coming from a wide range of technical and research background is not straightforward. Concurrently, learners now have a range of informal options for learning, not all of which provide accurate or recommended content. Curated learning paths are one of the strategies that can help address this challenge. Starting with data collected from a set of HPC practitioners we take a closer look at the diverse set of learning goals and concerns within the \"Computational Researcher\" persona. The range of responses highlights the challenges of building HPC learning paths for computational researchers wishing to develop and improve their HPC \u2026", "year": 2024, "venue": "", "authors": "Weronika Filinger and Julie Mullen and Jeremy Cohen"}, {"title": "ABNet: Attention BarrierNet for Safe and Scalable Robot Learning", "abstract": "Safe learning is central to AI-enabled robots where a single failure may lead to catastrophic results. Barrier-based method is one of the dominant approaches for safe robot learning. However, this method is not scalable, hard to train, and tends to generate unstable signals under noisy inputs that are challenging to be deployed for robots. To address these challenges, we propose a novel Attention BarrierNet (ABNet) that is scalable to build larger foundational safe models in an incremental manner. Each head of BarrierNet in the ABNet could learn safe robot control policies from different features and focus on specific part of the observation. In this way, we do not need to one-shotly construct a large model for complex tasks, which significantly facilitates the training of the model while ensuring its stable output. Most importantly, we can still formally prove the safety guarantees of the ABNet. We demonstrate the strength of ABNet in 2D robot obstacle avoidance, safe robot manipulation, and vision-based end-to-end autonomous driving, with results showing much better robustness and guarantees over existing models.", "year": 2024, "venue": "", "authors": "Wei Xiao and Tsun-Hsuan Wang and Daniela Rus"}, {"title": "Scheduling jobs with work-inefficient parallel solutions", "abstract": "This paper introduces the serial-parallel decision problem. Consider an online scheduler that receives a series of tasks, where each task has both a parallel and a serial implementation. The parallel implementation has the advantage that it can make progress concurrently on multiple processors, but the disadvantage that it is (potentially) work-inefficient. As tasks arrive, the scheduler must decide for each task which implementation to use.  We begin by studying total awake time. We give a simple decide-on-arrival scheduler that achieves a competitive ratio of 3 for total awake time---this scheduler makes serial/parallel decisions immediately when jobs arrive. Our second result is an parallel-work-oblivious scheduler that achieves a competitive ratio of 6 for total awake time---this scheduler makes all of its decisions based only on the size of each serial job and without needing to know anything about the parallel \u2026", "year": 2024, "venue": "", "authors": "William Kuszmaul and Alek Westover"}, {"title": "Mosaic pages: big TLB reach with small pages", "abstract": "This article introduces mosaic pages, which increase translation lookaside buffer (TLB) reach by compressing multiple, discrete translations into one TLB entry. Mosaic leverages virtual contiguity for locality, but does not use physical contiguity. Mosaic relies on recent advances in hashing theory to constrain memory mappings, in order to realize this physical address compression without reducing memory utilization or increasing swapping. Mosaic reduces TLB misses in several workloads by 6%\u201381%. Our results show that Mosaic \u2019s constraints on memory mappings do not harm performance, we never see conflicts before memory is 98% full in our experiments\u2014at which point a traditional design would also likely swap. Timing and area analyses on a commercial 28-nm CMOS process indicate that the hashing required on the critical path can run at a maximum frequency of 4 GHz, indicating that a Mosaic TLB is \u2026", "year": 2024, "venue": "", "authors": "Jaehyun Han and Krishnan Gosakan and William Kuszmaul and Ibrahim N Mubarek and Nirjhar Mukherjee and Karthik Sriram and Guido Tagliavini and Evan West and Michael A Bender and Abhishek Bhattacharjee and Alex Conway and Mart\u00edn Farach-Colton and Jayneel Gandhi and Rob Johnson and Sudarsun Kannan and Donald E Porter"}, {"title": "Speedcode: Software Performance Engineering Education via the Coding of Didactic Exercises", "abstract": "This paper introduces Speedcode, an online programming platform that aims to improve the accessibility of software performance-engineering education. At its core, Speedcode provides a platform that lets users gain hands-on experience in software performance engineering and parallel programming by completing short programming exercises. Speedcode challenges users to develop fast multicore solutions for short programming problems and evaluates their code's performance and scalability in a quiesced cloud environment. Speedcode supports parallel programming using OpenCilk, a task-parallel computing platform that is open-source and easy to program, teach and use for research. Speedcode aims to reduce barriers to learning and teaching software performance engineering. It allows users to run and evaluate their code on modern multicore machines from their own computer without installing any \u2026", "year": 2024, "venue": "", "authors": "Tim Kaler and Xuhao Chen and Brian Wheatman and Dorothy Curtis and Bruce Hoppe and Tao B Schardl and Charles E Leiserson"}, {"title": "History-Independent Dynamic Partitioning: Operation-Order Privacy in Ordered Data Structures", "abstract": "A data structure is history independent if its internal representation reveals nothing about the history of operations beyond what can be determined from the current contents of the data structure. History independence is typically viewed as a security or privacy guarantee, with the intent being to minimize risks incurred by a security breach or audit. Despite widespread advances in history independence, there is an important data-structural primitive that previous work has been unable to replace with an equivalent history-independent alternative---dynamic partitioning. In dynamic partitioning, we are given a dynamic set S of ordered elements and a size-parameter B, and the objective is to maintain a partition of S into ordered groups, each of size \u0398(B). Dynamic partitioning is important throughout computer science, with applications to B-tree rebalancing, write-optimized dictionaries, log-structured merge trees, other \u2026", "year": 2024, "venue": "", "authors": "Michael A Bender and Mart\u00edn Farach-Colton and Michael T Goodrich and Hanna Koml\u00f3s"}, {"title": "Classification-Based Transfer Learning for Blind Adaptive Receiver Beamforming", "abstract": "Adaptive receiver beamforming processors typically require expert design and can be limited by their convergence rate in data-starved applications. In this paper, we present a new type of machine learning beamformer using classification-based transfer learning (CBTL) to alleviate these limitations. The architecture consists of a pre-trained signal classifier, in our case a convolutional neural network, prepended by a beamforming layer. Narrowband beamforming weights are optimized by minimizing the classification loss, in turn nulling interference and amplifying a signal of interest (SOI). There are no requirements for calibration of the array, synchronization to the SOI, or training data modulated by the SOI. We describe the CBTL beamformer and demonstrate its effectiveness using several modulated signals. Simulated performance was compared to two well-established methods for blind source separation, and we \u2026", "year": 2024, "venue": "", "authors": "Michael Wentz and Jack Capper and Binoy Kurien and Keith Forsythe and Kaushik Chowdhury"}, {"title": "Towards an Analysis of Quadratic Probing", "abstract": "Since 1968, one of the simplest open questions in the theory of hash tables has been to prove anything nontrivial about the correctness of quadratic probing. We make the first tangible progress towards this goal, showing that there exists a positive-constant load factor at which quadratic probing is a constant-expected-time hash table. Our analysis applies more generally to any fixed-offset open-addressing hash table, and extends to higher load factors in the case where the hash table examines blocks of some size B= \u03c9 (1).", "year": 2024, "venue": "", "authors": "William Kuszmaul and Zoe Xi"}, {"title": "Flexible Multiagent Coverage Path Planning for Disjoint Areas of Interest", "abstract": "Time-critical coverage applications including search and rescue are among the most important considered for autonomous, multiagent path-planning. Utilizing multiple self- directed robots to investigate multiple, disjoint areas of interest in potentially hazardous environments can dramatically reduce the time necessary to achieve a shared goal, however, determining optimal joint actions while avoiding collisions is algorithmically challenging. Previous mixed integer approaches have focused on developing paths for agents through a single, contiguous area using time-indexed decision variables and fixed time horizons without considering additional factors such as threats to the agents. Here we present the following contributions: 1) a more flexible algorithm which takes into account factors other than coverage by utilizing a constraint inspired by the Miller-Tucker-Zemlin formulation; 2) the capability to consider \u2026", "year": 2024, "venue": "", "authors": "Andrea Henshall and Ezra Tal and Sertac Karaman"}, {"title": "Hypersparse traffic matrix construction using GraphBLAS on a DPU", "abstract": "Low-power small form factor data processing units (DPUs) enable offloading and acceleration of a broad range of networking and security services. DPUs have accelerated the transition to programmable networking by enabling the replacement of FPGAs/ASICs in a wide range of network oriented devices. The GraphBLAS sparse matrix graph open standard math library is well-suited for constructing anonymized hypersparse traffic matrices of network traffic which can enable a wide range of network analytics. This paper measures the performance of the GraphBLAS on an ARM based NVIDIA DPU (BlueField 2) and, to the best of our knowledge, represents the first reported GraphBLAS results on a DPU and/or ARM based system. Anonymized hypersparse traffic matrices were constructed at a rate of over 18 million packets per second.", "year": 2023, "venue": "", "authors": "William Bergeron and Michael Jones and Chase Barber and Kale DeYoung and George Amariucai and Kaleb Ernst and Nathan Fleming and Peter Michaleas and Sandeep Pisharody and Nathan Wells and Antonio Rosa and Eugene Vasserman and Jeremy Kepner"}, {"title": "Efficient deep learning on wearable physiological sensor data for pilot flight performance analysis", "abstract": "With the proliferation of wearable sensors for physiological and cognitive monitoring, a large amount of time series data needs to be processed and analyzed in a timely fashion. While deep learning has shown to be useful for the analysis, the majority of the deep learning methods are computing resource intensive. This paper demonstrates an efficient deep learning approach by adapting MINIROCKET to eye tracking and electrodermal activity data for flight performance assessment. The model was trained on 35 subjects using leave-one-subject-out cross validation and further evaluated on an independent data set of 8 subjects. We performed dimensionality reduction on each time series observation, reducing the size by 99.7% while still achieving averaged Area Under the Curve of 0.912 and average equal error rate of 0.181, thus enabling fast and accurate inference on edge devices. The approach presented \u2026", "year": 2023, "venue": "", "authors": "Patrick W Moore and Hrishikesh M Rao and Christine Beauchene and Emilie Cowen and Sophia Yuditskaya and Thomas Heldt and Laura J Brattain"}, {"title": "From bits to insights: Exploring network traffic, traffic matrices, and heavy-tailed data", "abstract": "With the Internet a central component of modern society, entire industries and fields have developed both in support and against cybersecurity. For cyber operators to best understand their networks, they must conduct detailed traffic analyses. A growing recognition is the ubiquity of heavy-tailed characteristics in network traffic. However, a thorough analysis of cybersecurity programs suggests little statistics educational background, worsened by the observation that college-level statistics courses largely lack heavy-tailed content, meaning cyber operators are both ill-equipped to appropriately analyze their network traffic and unable to easily access resources that could help. In response, we developed an accessible Jupyter Notebook module that guides individuals\u2014regardless of statistical background\u2014through traffic matrix creation, heavy-tailed data identification, data visualization, and distribution fitting. Such \u2026", "year": 2023, "venue": "", "authors": "Christopher Howard and Hayden Jananthan and Jeremy Kepner"}, {"title": "Focusing and calibration of large scale network sensors using graphblas anonymized hypersparse matrices", "abstract": "Defending community-owned cyber space requires community-based efforts. Large-scale network observations that uphold the highest regard for privacy are key to protecting our shared cyberspace. Deployment of the necessary network sensors requires careful sensor placement, focusing, and calibration with significant volumes of network observations. This paper demonstrates novel focusing and calibration procedures on a multi-billion packet dataset using high-performance GraphBLAS anonymized hypersparse matrices. The run-time performance on a real-world data set confirms previously observed real-time processing rates for high-bandwidth links while achieving significant data compression. The output of the analysis demonstrates the effectiveness of these procedures at focusing the traffic matrix and revealing the underlying stable heavy-tail statistical distributions that are necessary for anomaly \u2026", "year": 2023, "venue": "", "authors": "Jeremy Kepner and Michael Jones and Phil Dykstra and Chansup Byun and Timothy Davis and Hayden Jananthan and William Arcand and David Bestor and William Bergeron and Vijay Gadepally and Micheal Houle and Matthew Hubbell and Anna Klein and Lauren Milechin and Guillermo Morales and Julie Mullen and Ritesh Patel and Alex Pentland and Sandeep Pisharody and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Tyler Trigg and Charles Yee and Peter Michaleas"}, {"title": "Meta-learning and self-supervised pretraining for storm event imagery translation", "abstract": "Recent advances in deep learning have provided impressive results across a wide range of computational problems such as computer vision, natural language, or reinforcement learning. However, many of these improvements are constrained to problems with large-scale curated datasets which require a lot of human labor to gather. Additionally, these models tend to generalize poorly under both slight distributional shifts and low-data regimes. In recent years, emerging fields such as meta-learning and self-supervised learning have been closing the gap between proof-of-concept results and real-life applications of machine learning by extending deep learning to the semi-supervised and few-shot domains. We follow this line of work and explore spatiotemporal structure in a recently introduced image-to-image translation problem for storm event imagery in order to: i) formulate a novel multi-task few-shot image \u2026", "year": 2023, "venue": "", "authors": "Ileana Rugina and Rumen Dangovski and Mark Veillette and Pooya Khorrami and Brian Cheung and Olga Simek and Marin Solja\u010di\u0107"}, {"title": "2D-Radar Imaging With Deep Convolutional Neural Networks", "abstract": "Millimeter-wave (mmWave) frequency-modulated continuous-waveform (FMCW) radar technology has become widely used for advanced driver assistance systems (ADAS) because of its ability to operate in harsh environmental conditions and provide direct measurements of range and velocity. However, the spatial resolution of an FMCW radar system is limited by the number of individual radar elements in it. While many algorithms have been developed to increase sensor array resolution for sparsely populated scenes with simplistic priors, many real-world scenes have neither the required level of sparsity nor easily described priors. In this work, we propose a system that uses deep convolutional neural networks (DCNN) to produce high-resolution radar images of realistic driving scenes. Our proposed system is able to generate radar point clouds that are five times as dense as traditional algorithms such as \u2026", "year": 2023, "venue": "", "authors": "Mumin Jin and Atulya Yellepeddi and Gregory Wornell"}, {"title": "Coded orthogonal modulation for the multi-antenna MAC", "abstract": "This study focuses on (traditional and unsourced) multiple-access communication over a single transmit and multiple (M) receive antennas. We assume full or partial channel state information (CSI) at the receiver. It is known that to fully achieve the fundamental limits (even asymptotically) the decoder needs to jointly estimate all user codewords, doing which directly is computationally infeasible. We propose a low-complexity solution, termed coded orthogonal modulation multiple-access (COMMA), in which users first encode their messages via a long (multi-user interference aware) outer code operating over a q-ary alphabet. These symbols are modulated onto q orthogonal waveforms. At the decoder a multiple-measurement vector approximate message passing (MMV-AMP) algorithm estimates several candidates (out of q) for each user, with the remaining uncertainty resolved by the single-user outer decoders \u2026", "year": 2023, "venue": "", "authors": "Alexander Fengler and Alejandro Lancho and Yury Polyanskiy"}, {"title": "Dronescape: Designing an educational escape room for adult ai literacy", "abstract": "Escape rooms have become increasingly popular as a form of entertainment, in addition to being adopted by educators for their effectiveness in improving student engagement and learning. While they have been introduced in various educational contexts, from nursing to mathematics, and for different age groups, including K-12 and university students, little research has been conducted on the benefits of escape rooms for adult learning of artificial intelligence (AI). Furthermore, most escape room implementations lack relevance to real-world situations and challenges with using AI systems in the wild. This study explores the effectiveness of an escape room, DRONEscape, as a tool for teaching AI concepts to Air Force participants. The results suggest that escape rooms can most effectively facilitate engagement and collaboration, and have positive effects on learning AI concepts. This paper also provides \u2026", "year": 2023, "venue": "", "authors": "Daniella DiPaola and Jocelyn Shen and Rachelle Hu and Sharifa Alghowinem and Cynthia Breazeal"}, {"title": "Fatigue assessment from facial videos using deep neural networks and engineered features informed by domain knowledge", "abstract": "Fatigue impairs cognitive and motor function, potentially leading to mishaps in high-pressure occupations such as aviation and emergency medical services. The current approach is primarily based on self-assessment, which is subjective and error-prone. An objective method is needed to detect severe and likely dangerous levels of fatigue quickly and accurately. Here, we present a quantitative evaluation tool that uses less than two minutes of facial video, captured using an iPad, to assess fatigue vs. alertness. The tool is fast, easy to use, and scalable since it uses cameras readily available on consumer-electronic devices. We compared the classification performance between a Long Short-Term Memory (LSTM) deep neural network and a Random Forest (RF) classifier applied to engineered features informed by domain knowledge. The preliminary results on an 11-subject dataset show that RF outperforms LSTM \u2026", "year": 2023, "venue": "", "authors": "Luke Kenworthy and Patrick Moore and Hrishikesh M Rao and Laura J Brattain and Kevin James and Thomas Heldt"}, {"title": "Coded Orthogonal Modulation for the Multi-Antenna Multiple-Access Channel", "abstract": "This study focuses on (traditional and unsourced) multiple-access communication over a single transmit and multiple () receive antennas. We assume full or partial channel state information (CSI) at the receiver. It is known that to fully achieve the fundamental limits (even asymptotically) the decoder needs to jointly estimate all user codewords, doing which directly is computationally infeasible. We propose a low-complexity solution, termed coded orthogonal modulation multiple-access (COMMA), in which users first encode their messages via a long (multi-user interference aware) outer code operating over a -ary alphabet. These symbols are modulated onto  orthogonal waveforms. At the decoder a multiple-measurement vector approximate message passing (MMV-AMP) algorithm estimates several candidates (out of ) for each user, with the remaining uncertainty resolved by the single-user outer decoders. Numerically, we show that COMMA outperforms a standard solution based on linear multiuser detection (MUD) with Gaussian signaling. Theoretically, we derive bounds and scaling laws for , the number of users , SNR, and , allowing to quantify the trade-off between receive antennas and spectral efficiency. The orthogonal signaling scheme is applicable to unsourced random access and, with chirp sequences as basis, allows for low-complexity fast Fourier transform (FFT) based receivers that are resilient to frequency and timing offsets.", "year": 2023, "venue": "", "authors": "Alexander Fengler and Alejandro Lancho and Yury Polyanskiy"}, {"title": "Zero-shot linear combinations of grounded social interactions with Linear Social MDPs", "abstract": "Humans and animals engage in rich social interactions. It is often theorized that a relatively small number of basic social interactions give rise to the full range of behavior observed. But no computational theory explaining how social interactions combine together has been proposed before. We do so here. We take a model, the Social MDP, which is able to express a range of social interactions, and extend it to represent linear combinations of social interactions. Practically for robotics applications, such models are now able to not just express that an agent should help another agent, but to express goal-centric social interactions. Perhaps an agent is helping someone get dressed, but preventing them from falling, and is happy to exchange stories in the meantime. How an agent responds socially, should depend on what it thinks the other agent is doing at that point in time. To encode this notion, we take linear combinations of social interactions as defined in Social MDPs, and compute the weights on those combinations on the fly depending on the estimated goals of other agents. This new model, the Linear Social MDP, enables zero-shot reasoning about complex social interactions, provides a mathematical basis for the long-standing intuition that social interactions should compose, and leads to interesting new behaviors that we validate using human observers. Complex social interactions are part of the future of intelligent agents, and having principled mathematical models built on a foundation like MDPs will make it possible to bring social interactions to every robotic application.", "year": 2023, "venue": "", "authors": "Ravi Tejwani and Yen-Ling Kuo and Tianmin Shu and Bennett Stankovits and Dan Gutfreund and Joshua B Tenenbaum and Boris Katz and Andrei Barbu"}, {"title": "Increment-and-freeze: Every cache, everywhere, all of the time", "abstract": "One of the most basic algorithmic problems concerning caches is to compute the LRU hit-rate curve on a given trace. Unfortunately, the known algorithms exhibit poor data locality and fail to scale to large caches. It is widely believed that the LRU hit-rate curve cannot be computed efficiently enough to be used in online production settings. This has led to a large literature on heuristics that aim to approximate the curve efficiently.In this paper, we show that the poor data locality of past algorithms can be avoided. We introduce a new algorithm, called Increment-and-Freeze, for computing exact LRU hit-rate curves. The algorithm achieves RAM-model complexity O(n log n), external-memory complexity O(n over B log n), and parallelism \u0398(log n). We also present two theoretical extensions of Increment-and-Freeze, one that achieves SORT complexity in the external-memory model, and one that achieves a parallel span of \u2026", "year": 2023, "venue": "", "authors": "Michael A Bender and Daniel DeLayo and Bradley C Kuszmaul and William Kuszmaul and Evan West"}, {"title": "Learned Risk Metric Maps for Kinodynamic Systems", "abstract": "We present Learned Risk Metric Maps (LRMM) for real-time estimation of coherent risk metrics of high-dimensional dynamical systems operating in unstructured, partially observed environments. LRMM models are simple to design and train-requiring only procedural generation of obstacle sets, state and control sampling, and supervised training of a function approximator-which makes them broadly applicable to arbitrary system dynamics and obstacle sets. In a parallel autonomy setting, we demonstrate the model's ability to rapidly infer collision probabilities of a fast-moving car-like robot driving recklessly in an obstructed environment; allowing the LRMM agent to intervene, take control of the vehicle, and avoid collisions. In this time-critical scenario, we show that LRMMs can evaluate risk metrics 20-100x times faster than alternative safety algorithms based on control barrier functions (CBFs) and Hamilton-Jacobi \u2026", "year": 2023, "venue": "", "authors": "Ross E Allen and Wei Xiao and Daniela Rus"}, {"title": "Individualized Tracking of Neurocognitive-State-Dependent Eye-Movement Features Using Mobile Devices", "abstract": "With current clinical techniques, it is difficult to assess a patient's neurodegenerative disease (e.g., Alzheimer's) state accurately and frequently. The most widely used tests are qualitative or only performed intermittently, motivating the need for quantitative, accurate, and unobtrusive metrics to track disease progression. Clinical studies have shown that saccade latency (an eye movement measure of reaction time) and error rate (the proportion of eye movements in the wrong direction) may be significantly affected by neurocognitive diseases. Nevertheless, how these features change over time as a disease progresses is underdeveloped due to the constrained recording setup.In this work, our goal is to first understand how these features change over time in healthy individuals. To do so, we used a mobile app to frequently and accurately measure these features outside of the clinical environment from 80 healthy \u2026", "year": 2023, "venue": "", "authors": "Hsin-Yu Lai and Charles G Sodini and Vivienne Sze and Thomas Heldt"}, {"title": "Randomized Data Structures: New Perspectives and Hidden Surprises", "abstract": "This thesis revisits some of the oldest and most basic questions in the theory of randomized data structures\u2014questions such as: How efficient is a linear probing hash table? How fast can you maintain a sorted array of numbers? How big does a pointer have to be? With the help of new techniques, along with a willingness to look beyond conventional wisdom, we are able to achieve much stronger bounds for each of these questions than were previously thought to be possible.  Our results also come with a powerful set of tools that span a wide range of problems and settings. Perhaps the most surprising of these tools is a new paradigm for designing efficient dynamic data structures, in which, by \u2018tying our hands behind our back\u2019 (i.e., by artificially restricting ourselves to a special class of privacy-preserving data structures), we are able to circumvent decades-old barriers in time/space efficiency. This technique appears three (completely separate) times in the thesis.   Combined, our results overturn a 60-year-old myth on linear-probing hash tables; refute a 30-year-old conjecture and solve a 40-year-old open problem on dynamic sorting; resolve a 20-year-old open problem on dynamic load balancing; settle some of the most basic and fundamental questions from the theory of space-efficient data structures; and answer a 20-year-old question on memory allocation that was left as the central open problem in the first paper on history independence.", "year": 2023, "venue": "", "authors": "William Kuszmaul"}, {"title": "A Short Review of Automatic Differentiation Pitfalls in Scientific Computing", "abstract": "Automatic differentiation, also known as backpropagation, AD, autodiff, or algorithmic differentiation, is a popular technique for computing derivatives of computer programs. While AD has been successfully used in countless engineering, science and machine learning applications, it can sometimes nevertheless produce surprising results. In this paper we categorize problematic usages of AD and illustrate each category with examples such as chaos, time-averages, discretizations, fixed-point loops, lookup tables, linear solvers, and probabilistic programs, in the hope that readers may more easily avoid or detect such pitfalls.", "year": 2023, "venue": "", "authors": "Jan Hueckelheim and Harshitha Menon and William S Moses and Bruce Christianson and Paul Hovland and Laurent Hascoet"}, {"title": "Deep Learning Emulators for Accessible Climate Projections", "abstract": "Climate change has shifted from a purely scientific topic to a deeply politicized issue. To combat climate change we need to create mutual understanding on the links between policies, global warming, and city-scale impacts. Climate models have been incredibly helpful in generating this causal understanding, but running them requires supercomputers and is only accessible to the minority of researchers.", "year": 2023, "venue": "", "authors": "Bj\u00f6rn L\u00fctjens"}, {"title": "Python-based tools for characterizing geosynchronous satellite behavior and evaluating maneuver prediction techniques", "abstract": "Geosynchronous (GEO) satellites maneuver frequently to maintain their Earth-relative position despite drift incurred from natural perturbations, but quantifying their diverse maneuver patterns can be challenging. Even for individual satellites, between one station-keeping cycle and the next, the frequency, magnitude, and direction of maneuvers can change. Additionally, there is very little accountability among operators to disclose detailed mission objectives and precise orbital data or to adhere to operational guidelines. This complicates the process of characterizing station-keeping control objectives, predicting maneuvers, and recognizing the early signs of a shift in a satellite\u2019s pattern of life (PoL). Characterizing PoLs for a diverse range of GEO satellites can help to contextualize historic on-orbit behaviors and behavior patterns, cultivate generalized maneuver prediction on a large scale, and help future behaviors to \u2026", "year": 2023, "venue": "", "authors": "Haley Elizabeth Solera"}, {"title": "AI enabled maneuver identification via the maneuver identification challenge", "abstract": "Artificial intelligence (AI) has enormous potential to improve Air Force pilot training by providing actionable feedback to pilot trainees on the quality of their maneuvers and enabling instructor-less flying familiarization for early-stage trainees in low-cost simulators. Historically, AI challenges consisting of data, problem descriptions, and example code have been critical to fueling AI breakthroughs. The Department of the Air Force-Massachusetts Institute of Technology AI Accelerator (DAF-MIT AI Accelerator) developed such an AI challenge using real-world Air Force flight simulator data. The Maneuver ID challenge assembled thousands of virtual reality simulator flight recordings collected by actual Air Force student pilots at Pilot Training Next (PTN). This dataset has been publicly released at Maneuver-ID.mit.edu and represents the first of its kind public release of USAF flight training data. Using this dataset, we have applied a variety of AI methods to separate \"good\" vs \"bad\" simulator data and categorize and characterize maneuvers. These data, algorithms, and software are being released as baselines of model performance for others to build upon to enable the AI ecosystem for flight simulator training.", "year": 2022, "venue": "", "authors": "Kaira Samuel and Matthew LaRosa and Kyle McAlpin and Morgan Schaefer and Brandon Swenson and Devin Wasilefsky and Yan Wu and Dan Zhao and Jeremy Kepner"}, {"title": "DASH: Scheduling deep learning workloads on multi-generational GPU-accelerated clusters", "abstract": "Two notable characteristics of modern GPU-accelerated HPC clusters are: (1) they increasingly run deep learning (DL) model-training workloads, and (2) they consist of multiple generations of GPUs, i.e., they are heterogeneous. However, existing works in GPU cluster scheduling for DL workloads have not addressed the GPU multi-generation problem. We propose DASH, a GPU cluster scheduler designed to optimally make a match between different DL workloads and GPU types in a multi-generational GPU environment. By lever-aging execution characteristics of co-scheduled DL workloads, DASH can improve the average job runtime by 17% and the average job completion time by 14 % compared to the traditional heterogeneity-unaware job scheduler.", "year": 2022, "venue": "", "authors": "Baolin Li and Tirthak Patel and Vijay Gadepally and Karen Gettings and Siddharth Samsi and Devesh Tiwari"}, {"title": "An Evaluation of Low Overhead Time Series Preprocessing Techniques for Downstream Machine Learning", "abstract": "In this paper we address the application of pre-processing techniques to multi-channel time series data with varying lengths, which we refer to as the alignment problem, for downstream machine learning. The misalignment of multi-channel time series data may occur for a variety of reasons, such as missing data, varying sampling rates, or inconsistent collection times. We consider multi-channel time series data collected from the MIT SuperCloud High Performance Computing (HPC) center, where different job start times and varying run times of HPC jobs result in misaligned data. This misalignment makes it challenging to build AI/ML approaches for tasks such as compute workload classification. Building on previous supervised classification work with the MIT SuperCloud Dataset, we address the alignment problem via three broad, low overhead approaches: sampling a fixed subset from a full time series, performing \u2026", "year": 2022, "venue": "", "authors": "Matthew L Weiss and Joseph McDonald and David Bestor and Charles Yee and Daniel Edelman and Michael Jones and Andrew Prout and Andrew Bowne and Lindsey McEvoy and Vijay Gadepally and Siddharth Samsi"}, {"title": "A Survey of Data Challenges Across a Modernizing Bureaucracy: A New Perspective on Examining Old Government Problems", "abstract": "The introduction and increasing popularity of artificial intelligence (AI) and machine learning (ML) technologies allow organizations to gain valuable insights from their copious amounts of data. However, legacy organizations often struggle to overcome outdated data management practices and unleash the potential of AI and ML on their data. There is simply too much data to sift through manually. Therefore, a data science tool is required to locate relevant information effectively within an organizations\u2019 data lake. This paper presents a survey of challenges government organizations face related to this data discovery issue. The challenges are ubiquitous across mission sets, covering human resources and personnel management, logistics and supply chains, fraud and predatory business detection, government procurement, and civil litigation. This paper introduces the Data Discovery by Example (DICE) system to \u2026", "year": 2022, "venue": "", "authors": "Andrew Bowne and Lindsey McEvoy and Dhruv Gupta and Cameron Brown and Vijay Gadepally and El Kindi Rezig"}, {"title": "Keynote talk: large scale parallel sparse matrix streaming graph/network analysis", "abstract": "Groundbreaking work analyzing early Internet data revealed novel phenomena that became the basis of a new endeavor: Network Science. This exciting new field has revealed fundamental properties about communication, social, and biological networks. Simultaneously, the Internet has expanded enormously and is now a domain of activity as important to civilization as land, sea, air, and space. The initial Internet observations that nurtured network science have ballooned and become the largest dynamic streaming data sets availability; creating fresh opportunities to examine the foundations of network science in previously unimagined detail. The analysis of streaming networks with trillions of events have stimulated the development of novel mathematics (e.g., associative array algebra), algorithms (e.g., hypersparse neural networks), software (e.g., GraphBLAS.org), and hardware. All of these capabilities are \u2026", "year": 2022, "venue": "", "authors": "Jeremy Kepner"}, {"title": "What Does Dynamic Optimality Mean in External Memory?", "abstract": "In this paper, we revisit the question of how the dynamic optimality of search trees should be defined in external memory. A defining characteristic of external-memory data structures is that there is a stark asymmetry between queries and inserts/updates/deletes: by making the former slightly asymptotically slower, one can make the latter significantly asymptotically faster (even allowing for operations with sub-constant amortized I/Os). This asymmetry makes it so that rotation-based search trees are not optimal (or even close to optimal) in insert/update/delete-heavy external-memory workloads. To study dynamic optimality for such workloads, one must consider a different class of data structures. The natural class of data structures to consider are what we call buffered-propagation trees. Such trees can adapt dynamically to the locality properties of an input sequence in order to optimize the interactions between different inserts/updates/deletes and queries. We also present a new form of beyond-worst-case analysis that allows for us to formally study a continuum between static and dynamic optimality. Finally, we give a novel data structure, called the \\jellotree, that is statically optimal and that achieves dynamic optimality for a large natural class of inputs defined by our beyond-worst-case analysis.", "year": 2022, "venue": "", "authors": "Michael A Bender and Mart\u00edn Farach-Colton and William Kuszmaul"}, {"title": "Polynomial structure in semidefinite relaxations and non-convex formulations", "abstract": "Semidefinite relaxation is a powerful tool used to approximate otherwise intractable non-convex problems, but tend to run into scalability issues in large-scale instances. The goal of this thesis is to explore the power of semidefinite relaxations and address the scalability issues, for special classes of problems with polynomial structure.  In the first part of this thesis, we consider semidefinite relaxations of functions on quadratic maps, with applications to approximating permanents of positive semidefinite (PSD) matrices, product of quadratic forms, and can be interpreted as a generalization of MaxCut. The optimization problems and their convex relaxations have a product structure which is crucial in the analysis of approximation quality. We show that these problems are all connected with a unified analysis which recovers tight approximation factors. This leads to better approximation bounds on the permanent of PSD matrices, intermediate relaxations trading off accuracy with computational power, and constant factor approximation bounds for maximizing concave objectives on the image of quadratic maps.  In the second part, we study the global landscape of low-rank sum of squares problems, using a non-convex Burer-Monteiro formulation to decrease the computational cost but with the risk of getting stuck in local minima. We show that in the univariate case where the SDP solution is guaranteed to be rank-2, this formulation does not have spurious local minima. This is in contrast to previous work showing that for general SDPs, in addition to genericity conditions, the rank has to be roughly the square root of the degree of the polynomial for there to \u2026", "year": 2022, "venue": "", "authors": "Chenyang Yuan"}, {"title": "Transfer learning for space traffic management", "abstract": "", "year": 2022, "venue": "", "authors": "Sydney Dolan and Siddharth Nayak and Hamsa Balakrishnan"}, {"title": "Bio-signal analysis for personalized pilot training", "abstract": "Quantitative data on pilot performance can help increase efficiency in pilot training programs by informing when students are prepared take on more difficult challenges. Previous research indicates attentional and cognitive load differences in novice pilots versus more experienced pilots. These differences may manifest as changes in certain physiological signals as pilots of differing experience levels are over, under, or adequately challenged in a given task. This thesis analyzes the effectiveness of measured elecrodermal activity (EDA) and electromyography (EMG) signals to indicate trends in pilot experience, task performance, and challenge difficulty across N = 29 subjects.  EMG and EDA features, as well as accelerometry and joystick data, are considered over the entire task and shorter windows in at the beginning and end of each task. Significant differences, with a p-value less than 0.05, are seen in EMG features based on difficulty, experience, and trial attempt and in EDA features based on experience and trial attempt.  Using these features, tasks by a subject are classified using a logistic regression model with forward step-wise feature selection. The performance of the model in classifying the easiest against the hardest difficulty level reaches an AUC of 0.99, reduced to 0.83 with the dominant joystick feature removed. Classifying a run as performed by an expert against a novice (using a cutoff of 30 flight hours) in the hardest difficulty level reaches an AUC of 0.89, reduced to 0.67 with the dominant joystick data removed. Lastly, the model performance when classifying the first against the last attempt of a subject at a given difficulty level \u2026", "year": 2022, "venue": "", "authors": "Stuart D Powell"}, {"title": "Quantifying the Emergence of Symbolic Communication.", "abstract": "We quantitatively study the emergence of symbolic communication in humans with a communication game that attempts to recapitulate an essential step in the development of human language: the emergence of shared signs. In our experiment, a teacher must communicate a first order logic formula to a student through a narrow channel deprived of common shared signs: subjects cannot communicate with each other with the sole exception of car motions in a computer game. Subjects spontaneously develop a shared vocabulary of car motions including indices, icons, and symbols, spanning both task-specific and task-agnostic concepts such as \u201csquare\u201d and \u201cunderstand\u201d. We characterize the conditions under which indices, icons, and symbols arise, finding that symbols are harder to establish than icons and indices. We observe the dominant sign category being developed transition from indices to icons to symbols, and identify communicating in ambiguous game environments as a pressure for icon and symbol development.", "year": 2022, "venue": "", "authors": "Emily Cheng and Yen-Ling Kuo and Josefina Correa and Boris Katz and Ignacio Cases and Andrei Barbu"}, {"title": "GraphBLAS on the Edge: High Performance Streaming of Network Traffic.", "abstract": "", "year": 2022, "venue": "", "authors": "Michael Jones and Jeremy Kepner and Daniel Andersen and Aydin Bulu\u00e7 and Chansup Byun and K Claffy and Timothy Davis and William Arcand and Jonathan Bernays and David Bestor and William Bergeron and Vijay Gadepally and Micheal Houle and Matthew Hubbell and Hayden Jananthan and Anna Klein and C Meiners and Lauren Milechin and Julie Mullen and Sandeep Pisharody and Andrew Prout and Albert Reuther and Antonio Rosa and Siddharth Samsi and Jon Sreekanth and Doug Stetson and Charles Yee and Peter Michaleas"}, {"title": "Beyond Faithfulness: A Framework to Characterize and Compare Saliency Methods", "abstract": "Saliency methods calculate how important each input feature is to a machine learning model\u2019s prediction, and are commonly used to understand model reasoning. \u201cFaithfulness,\u201d or how fully and accurately the saliency output reflects the underlying model, is an oft-cited desideratum for these methods. However, explanation methods must necessarily sacrifice certain information in service of user-oriented goals such as simplicity. To that end, and akin to performance metrics, we frame saliency methods as abstractions: individual tools that provide insight into specific aspects of model behavior and entail tradeoffs. Using this framing, we describe a framework of nine dimensions to characterize and compare the properties of saliency methods. We group these dimensions into three categories that map to different phases of the interpretation process: methodology, or how the saliency is calculated; sensitivity, or relationships between the saliency result and the underlying model or input; and, perceptibility, or how a user interprets the result. As we show, these dimensions give us a granular vocabulary for describing and comparing saliency methods \u2014 for instance, allowing us to develop \u201csaliency cards\u201d as a form of documentation, or helping downstream users understand tradeoffs and choose a method for a particular use case. Moreover, by situating existing saliency methods within this framework, we identify opportunities for future work, including filling gaps in the landscape and developing new evaluation metrics.", "year": 2022, "venue": "", "authors": "Angie Boggust and Harini Suresh and Hendrik Strobelt and John Guttag and Arvind Satyanarayan"}, {"title": "Improved compression for word embeddings by scaling principal components", "abstract": "Word embeddings have been adopted as a fundamental component of many natural language processing applications for their ability to capture meaningful semantic relationships. However they often present a significant computational bottleneck due to memory requirements. In this article we present a postprocessing technique for embeddings, based on modifying their principal components, that enables compression while maintaining comparable if not better performance relative to the original embedding. Specifically, our technique can reduce the overall memory footprint of popular embeddings such as GloVe and word2vec by 50% while maintaining the same performance on different metrics, including commonly used similarity and analogy tasks as well as on end-to-end tasks such as text classification. Compared to the original embeddings and previous postprocessing methods, this approach improves \u2026", "year": 2021, "venue": "", "authors": "Joseph McDonald and Siddharth Samsi and Daniel Edelman and Chansup Byun and Jeremy Kepner and Vijay Gadepally"}, {"title": "Overcoming Data Scarcity in Deep Learning of Scientific Problems", "abstract": "Data-driven approaches such as machine learning have been increasingly applied to the natural sciences, e.g. for property prediction and optimization or material discovery. An essential criteria to ensure the success of such methods is the need for extensive amounts of labeled data, making it unfeasible for data-scarce problems where labeled data generation is computationally expensive, or labour and time intensive. Here, I introduce surrogate and invariance- boosted contrastive learning (SIB-CL), a deep learning framework which overcomes data-scarcity by incorporating three \u201cinexpensive\" and easily obtainable auxiliary information. Specifically, these are: 1) abundant unlabeled data, 2) prior knowledge of known symmetries or invariances of the problem and 3) a surrogate dataset obtained at near-zero cost either from simplification or approximation. I demonstrate the effectiveness and generality of SIB-CL on various scientific problems, for example, the prediction of the density-of-states of 2D photonic crystals and solving the time-independent Schr\u00f6dinger equation of 3D random potentials. SIB-CL is shown to provide orders of magnitude savings on the amount of labeled data needed when compared to conventional deep learning techniques, offering opportunities to apply data-driven methods even to data-scarce problems.", "year": 2021, "venue": "", "authors": "Charlotte Chang Le Loh"}, {"title": "On frank-wolfe adversarial training", "abstract": "We develop a theoretical framework for adversarial training (AT) with FW optimization (FW-AT) that reveals a geometric connection between the loss landscape and the distortion of  FW attacks (the attack's  norm). Specifically, we show that high distortion of FW attacks is equivalent to low variation along the attack path. It is then experimentally demonstrated on various deep neural network architectures that  attacks against robust models achieve near maximal  distortion. To demonstrate the utility of our theoretical framework we develop FW-Adapt, a novel adversarial training algorithm which uses simple distortion measure to adapt the number of attack steps during training. FW-Adapt provides strong robustness against white- and black-box attacks at lower training times than PGD-AT.", "year": 2021, "venue": "", "authors": "Theodoros Tsiligkaridis and Jay Roberts"}, {"title": "Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks", "abstract": "Attention mechanisms play a crucial role in the neural revolution of Natural Language Processing (NLP). With the growth of attention-based models, several pruning techniques have been developed to identify and exploit sparseness, making these models more efficient. Most efforts focus on hard-coding attention patterns or pruning attention weights based on training data. We propose Attention Pruning (AP), a framework that observes attention patterns in a fixed dataset and generates a global sparseness mask. AP saves 90% of attention computation for language modeling and about 50% for machine translation and GLUE tasks, maintaining result quality. Our method reveals important distinctions between self- and cross-attention patterns, guiding future NLP research. Our framework can reduce both latency and memory requirements for any attention-based model, aiding in the development of improved models for existing or new NLP applications. We have demonstrated this with encoder and autoregressive transformer models using Triton GPU kernels and make our code publicly available at https://github.com/irugina/AP.", "year": 2020, "venue": "", "authors": "Ileana Rugina and Rumen Dangovski and Li Jing and Preslav Nakov and Marin Solja\u010di\u0107"}, {"title": "Avoiding tree saturation in the face of many hotspots with few buffers", "abstract": "In a multistage network, hotspots induce tree saturation. The known solutions employ a variety of techniques, including combining (which works only for certain kinds of messages), feedback damping (which appears to provide low utilization in the absence of hot spots), and large numbers of buffers. In practice, the approach used today is to provide large numbers of buffers: in a P-processor system, the rule of thumb appears to be to provide 10P buffers, but 10P buffers maybe too expensive for systems containing 105 or more processors. Even employing Omega(P) buffers does not appear to provide any guarantees, however. We show that by organizing the switches so that the messages addressed to a particular processor can use only certain of the buffers, many hotspots can be tolerated with few buffers. For example, a switch with O(log P) buffers can tolerate a single hotspot with probability 1, and allows the first \u2026", "year": 2014, "venue": "", "authors": "Bradley C Kuszmaul and William H Kuszmaul"}, {"title": "A New Approach to Enumerating Statistics Modulo ", "abstract": "We find a new approach to computing the remainder of a polynomial modulo ; such a computation is called modular enumeration. Given a polynomial with coefficients from a commutative -algebra, our first main result constructs the remainder simply from the coefficients of residues of the polynomial modulo  for each . Since such residues can often be found to have nice values, this simplifies a number of modular enumeration problems; indeed in some cases, such residues are already known while the related modular enumeration problem has remained unsolved. We list six such cases which our technique makes easy to solve. Our second main result is a formula for the unique polynomial  such that  and  for each proper divisor  of . We find a formula for remainders of -multinomial coefficients and for remainders of -Catalan numbers modulo , reducing each problem to a finite number of cases for any fixed . In the prior case, we solve an open problem posed by Hartke and Radcliffe. In considering -Catalan numbers modulo , we discover a cyclic group operation on certain lattice paths which behaves predictably with regard to major index. We also make progress on a problem in modular enumeration on subset sums posed by Kitchloo and Pachter.", "year": 2014, "venue": "", "authors": "William Kuszmaul"}, {"title": "Robustaugmix: Joint optimization of natural and adversarial robustness", "abstract": "Machine learning models often suffer performance degradation when faced with corrupted data. In this work, we explore a technique that combines a data augmentation strategy (AugMix) with adversarial training, in order to increase robustness to both natural and adversarial forms of data corruption.", "year": "", "venue": "", "authors": "Josue Martinez-Martinez and Olivia Brown"}, {"title": "Optimal Non-Oblivious Open Addressing", "abstract": "A hash table is said to be open-addressed (or non-obliviously open-addressed) if it stores elements (and free slots) in an array with no additional metadata. Intuitively, open-addressed hash tables must incur a space-time tradeoff: The higher the load factor at which the hash table operates, the longer insertions/deletions/queries should take. In this paper, we show that no such tradeoff exists: It is possible to construct an open-addressed hash table that supports constant-time operations even when the hash table is entirely full. In fact, it is even possible to construct a version of this data structure that: (1) is dynamically resized so that the number of slots in memory that it uses, at any given moment, is the same as the number of elements it contains; (2) supports -time operations, not just in expectation, but with high probability; and (3) requires external access to just  hash functions that are each just -wise independent. Our results complement a recent lower bound by Bender, Kuszmaul, and Zhou showing that oblivious open-addressed hash tables must incur -time operations. The hash tables in this paper are non-oblivious, which is why they are able to bypass the previous lower bound.", "year": 2025, "venue": "", "authors": "Michael A Bender and William Kuszmaul and Renfei Zhou"}, {"title": "A Simple and Combinatorial Approach to Proving Chernoff Bounds and Their Generalizations", "abstract": "The Chernoff bound is one of the most widely used tools in theoretical computer science. It\u2019s rare to find a randomized algorithm that doesn\u2019t employ a Chernoff bound in its analysis.The standard proofs of Chernoff bounds are beautiful but in some ways not very intuitive. In this paper,I\u2019ll show you a different proof that has four features:\u2022 the proof offers a strong intuition for why Chernoff bounds look the way that they do;\u2022 the proof is user-friendly and (almost) algebra-free;\u2022 the proof comes with matching lower bounds, up to constant factors in the exponent;\u2022 the proof extends to establish generalizations of Chernoff bounds in other settings.The ultimate goal is that, once you know this proof (and with a bit of practice), you should be able to confidently reason about Chernoff-style bounds in your head, extending them to other settings, and convincing yourself that the bounds you\u2019re obtaining are tight (up to constant \u2026", "year": 2025, "venue": "", "authors": "William Kuszmaul"}, {"title": "Tight Bounds and Phase Transitions for Incremental and Dynamic Retrieval", "abstract": "Retrieval data structures are data structures that answer key-value queries without paying the space overhead of explicitly storing keys. The problem can be formulated in four settings (static, value-dynamic, incremental, or dynamic), each of which offers different levels of dynamism to the user. In this paper, we establish optimal bounds for the final two settings (incremental and dynamic) in the case of a polynomial universe. Our results complete a line of work that has spanned more than two decades, and also come with a surprise: the incremental setting, which has long been viewed as essentially equivalent to the dynamic one, actually has a phase transition, in which, as the value size v approaches log n, the optimal space redundancy actually begins to shrink, going from roughly n log log n (which has long been thought to be optimal) all the way down to \u0398(n ) (which is the optimal bound even for the seemingly \u2026", "year": 2025, "venue": "", "authors": "William Kuszmaul and Aaron Putterman and Tingqiang Xu and Hangrui Zhou and Renfei Zhou"}, {"title": "OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step", "abstract": "Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. Language model systems often enable LLMs to generate code for arithmetic operations to achieve accurate calculations. However, this approach compromises speed and security, and fine-tuning risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in* a single autoregressive step*, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of a LLM to control a symbolic architecture that performs arithmetic. Our implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama) achieves 100\\% accuracy on single arithmetic operations (), outperforming GPT 4o with and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o with and without a code interpreter on average across a range of mathematical problem solving benchmarks, demonstrating that OccamLLMs can excel in arithmetic tasks, even surpassing much larger models. Code is available at https://github. com/druidowm/OccamLLM.", "year": 2024, "venue": "", "authors": "Owen Dugan and Donato Jim\u00e9nez-Benet\u00f3 and Charlotte Loh and Zhuo Chen and Rumen Dangovski and Marin Soljacic"}, {"title": "Optimal Bounds for Open Addressing Without Reordering", "abstract": "In this paper, we revisit one of the simplest problems in data structures: the task of inserting elements into an open-addressed hash table so that elements can later be retrieved with as few probes as possible. We show that, even without reordering elements over time, it is possible to construct a hash table that achieves far better expected search complexities (both amortized and worst-case) than were previously thought possible. Along the way, we disprove the central conjecture left by Yao in his seminal paper \u201cUniform Hashing is Optimal\u201d.", "year": 2024, "venue": "", "authors": "Martin Farach-Colton and Andrew Krapivin and William Kuszmaul"}, {"title": "Tight Analyses of Ordered and Unordered Linear Probing", "abstract": "Linear-probing hash tables have been classically believed to support insertions in time , where  is the load factor of the hash table. Recent work by Bender, Kuszmaul, and Kuszmaul (FOCS'21), however, has added a new twist to this story: in some versions of linear probing, if the maximum load factor is at most , then the amortized expected time per insertion will never exceed  (even in workloads that operate continuously at a load factor of ). Determining the exact asymptotic value for the amortized insertion time remains open. In this paper, we settle the amortized complexity with matching upper and lower bounds of . Along the way, we also obtain tight bounds for the so-called path surplus problem, a problem in combinatorial geometry that has been shown to be closely related to linear probing. We also show how to extend Bender et al.'s bounds to say something not just \u2026", "year": 2024, "venue": "", "authors": "Mark Braverman and William Kuszmaul"}, {"title": "Nearly Optimal List Labeling", "abstract": "The list-labeling problem captures the basic task of storing a dynamically changing set of up to  elements in sorted order in an array of size  \u2022 The goal is to support insertions and deletions while moving around elements within the array as little as possible. Until recently, the best known upper bound stood at  amortized cost. This bound, which was first established in 1981, was finally improved two years ago, when a randomized  expected-cost algorithm was discovered. The best randomized lower bound for this problem remains , and closing this gap is considered to be a major open problem in data structures. In this paper, we present the See-Saw Algorithm, a randomized list-labeling solution that achieves a nearly optimal bound of  amortized expected cost. This bound is achieved despite at least three lower bounds showing that this type of result is \u2026", "year": 2024, "venue": "", "authors": "Michael A Bender and Alex Conway and Mart\u00edn Farach-Colton and Hanna Koml\u00f3s and Michal Kouck\u00fd and William Kuszmaul and Michael Saks"}, {"title": "Anonymized Network Sensing Graph Challenge", "abstract": "The MIT/IEEE/Amazon GraphChallenge encourages community approaches to developing new solutions for analyzing graphs and sparse data derived from social media, sensor feeds, and scientific data to discover relationships between events as they unfold in the field. The anonymized network sensing Graph Challenge seeks to enable large, open, community-based approaches to protecting networks. Many large-scale networking problems can only be solved with community access to very broad data sets with the highest regard for privacy and strong community buy-in. Such approaches often require community-based data sharing. In the broader networking community (commercial, federal, and academia) anonymized source-to-destination traffic matrices with standard data sharing agreements have emerged as a data product that can meet many of these requirements. This challenge provides an opportunity to highlight novel approaches for optimizing the construction and analysis of anonymized traffic matrices using over 100 billion network packets derived from the largest Internet telescope in the world (CAIDA). This challenge specifies the anonymization, construction, and analysis of these traffic matrices. A GraphBLAS reference implementation is provided, but the use of GraphBLAS is not required in this Graph Challenge. As with prior Graph Challenges the goal is to provide a well-defined context for demonstrating innovation. Graph Challenge participants are free to select (with accompanying explanation) the Graph Challenge elements that are appropriate for highlighting their innovations.", "year": 2024, "venue": "", "authors": "Hayden Jananthan and Michael Jones and William Arcand and David Bestor and William Bergeron and Daniel Burrill and Aydin Buluc and Chansup Byun and Timothy Davis and Vijay Gadepally and Daniel Grant and Michael Houle and Matthew Hubbell and Piotr Luszczek and Peter Michaleas and Lauren Milechin and Chasen Milner and Guillermo Morales and Andrew Morris and Julie Mullen and Ritesh Patel and Alex Pentland and Sandeep Pisharody and Andrew Prout and Albert Reuther and Antonio Rosa and Gabriel Wachman and Charles Yee and Jeremy Kepner"}, {"title": "What is Normal? A Big Data Observational Science Model of Anonymized Internet Traffic", "abstract": "Understanding what is normal is a key aspect of protecting a domain. Other domains invest heavily in observational science to develop models of normal behavior to better detect anomalies. Recent advances in high performance graph libraries, such as the GraphBLAS, coupled with supercomputers enables processing of the trillions of observations required. We leverage this approach to synthesize low-parameter observational models of anonymized Internet traffic with a high regard for privacy.", "year": 2024, "venue": "", "authors": "Jeremy Kepner and Hayden Jananthan and Michael Jones and William Arcand and David Bestor and William Bergeron and Daniel Burrill and Aydin Buluc and Chansup Byun and Timothy Davis and Vijay Gadepally and Daniel Grant and Michael Houle and Matthew Hubbell and Piotr Luszczek and Lauren Milechin and Chasen Milner and Guillermo Morales and Andrew Morris and Julie Mullen and Ritesh Patel and Alex Pentland and Sandeep Pisharody and Andrew Prout and Albert Reuther and Antonio Rosa and Gabriel Wachman and Charles Yee and Peter Michaleas"}, {"title": "Widespread Amazonian dark earth in the Xingu Indigenous Territory", "abstract": "Amazonian dark earth (ADE) is highly nutrient- and carbon-rich soil created by past inhabitants of the Amazon. It would be valuable to know the extent of ADE because of its cultural and environmental importance, but systematic efforts to map its distribution and extent are impractical with traditional field methods. We use remote-sensing imagery and a machine-learning classifier with ground-truthed training data to predict the occurrence of ADE across the 26,000\u2009km2 Territ\u00f3rio Ind\u00edgena do Xingu (TIX) in the southeastern Amazon region of Brazil. We find widespread ADE across the TIX, well beyond previously studied archaeological sites, occupying at least 3\u20134% of the land area. We further estimate that the TIX may sequester 9\u2009Mt of carbon within ADE deposits from past human inputs. Our findings show that ancient inhabitants of the TIX substantially modified their environment, highlighting the importance of \u2026", "year": 2024, "venue": "", "authors": "Samuel L Goldberg and Morgan J Schmidt and Joshua D Himmelstein and Michael Heckenberger and Bruna Franchetto and Helena Lima and Jennifer Watling and Bruno Moraes and Wetherbee B Dorshow and Carlos Fausto and Kumessi Waura and Huke Kuikuro and Taku Wate Kuikuro and Afukaka Kuikuro and J Taylor Perron"}, {"title": "Performance Bounds for Quantum Feedback Control", "abstract": "The limits of quantum feedback control have immediate consequences for quantum information science at large, yet remain largely unexplored. Here, we combine quantum filtering theory and moment-sum-of-squares techniques to construct a hierarchy of convex optimization problems that furnish monotonically improving, computable bounds on the best attainable performance for a broad class of quantum feedback control problems. These bounds may serve as witnesses of fundamental limitations, optimality certificates, or performance targets. We prove convergence of the bounds to the optimal control performance under technical conditions and demonstrate the practical utility of our approach by designing certifiably near-optimal controllers for a qubit in a cavity subjected to photon counting and homodyne detection measurements.", "year": 2024, "venue": "", "authors": "Flemming Holtorf and Frank Sch\u00e4fer and Julian Arnold and Christopher V Rackauckas and Alan Edelman"}, {"title": "Distributed Load Balancing in the Face of Reappearance Dependencies", "abstract": "We consider the problem of load-balancing on distributed databases. We assume that data is divided into chunks and each chunk can be replicated on a constant number d of servers. When a request arrives, it is routed to one of the servers that contains the relevant chunk. Each server may store outstanding requests in a bounded queue and requests may be rejected if the queue is full. The goal is to design strategies for data distribution and request routing that minimize both the rejection rate and the average request latency.  What makes this problem technically difficult is reappearance dependencies: if a chunk x is accessed at multiple different time steps, then the set of d servers that it can be routed to is the same each time it is accessed. This is a substantial departure from classical balls-and-bins settings where each ball arrival introduces fresh randomness into the system.  We show that, with new algorithmic \u2026", "year": 2024, "venue": "", "authors": "Kunal Agrawal and William Kuszmaul and Zhe Wang and Jinhao Zhao"}, {"title": "A Nearly Quadratic Improvement for Memory Reallocation", "abstract": "In the Memory Reallocation Problem a set of items of various sizes must be dynamically assigned to non-overlapping contiguous chunks of memory. It is guaranteed that the sum of the sizes of all items present at any time is at most a (1-\u03b5)-fraction of the total size of memory (i.e., the load-factor is at most 1-\u03b5). The allocator receives insert and delete requests online, and can re-arrange existing items to handle the requests, but at a reallocation cost defined to be the sum of the sizes of items moved divided by the size of the item being inserted/deleted.The folklore algorithm for Memory Reallocation achieves a cost of O(\u03b5-1) per update. In recent work at FOCS'23, Kuszmaul showed that, in the special case where each item is promised to be smaller than an \u03b54-fraction of memory, it is possible to achieve expected update cost O(log\u03b5-1). Kuszmaul conjectures, however, that for larger items the folklore algorithm is optimal.In \u2026", "year": 2024, "venue": "", "authors": "Martin Farach-Colton and William Kuszmaul and Nathan S Sheffield and Alek Westover"}, {"title": "On a perturbation analysis of Higham squared maximum Gaussian elimination growth matrices", "abstract": "Gaussian elimination is the most popular technique for solving a dense linear system. Large errors in this procedure can occur in floating point arithmetic when the matrix's growth factor is large. We study this potential issue and how perturbations can improve the robustness of the Gaussian elimination algorithm. In their 1989 paper, Higham and Higham characterized the complete set of real n by n matrices that achieves the maximum growth factor under partial pivoting. This set of matrices serves as the critical focus of this work. Through theoretical insights and empirical results, we illustrate the high sensitivity of the growth factor of these matrices to perturbations and show how subtle changes can be strategically applied to matrix entries to significantly reduce the growth, thus enhancing computational stability and accuracy.", "year": 2024, "venue": "", "authors": "Alan Edelman and John Urschel and Bowen Zhu"}, {"title": "Helping Faculty Teach Software Performance Engineering", "abstract": "Over the academic year 2022\u201323, we discussed the teaching of software performance engineering with more than a dozen faculty across North America and beyond. Our outreach was centered on research-focused faculty with an existing interest in this course material. These discussions revealed an enthusiasm for making software performance engineering a more prominent part of a curriculum for computer scientists and engineers. Here, we discuss how MIT's longstanding efforts in this area may serve as a launching point for community development of a software performance engineering curriculum, challenges in and solutions for providing the necessary infrastructure to universities, and future directions.", "year": 2024, "venue": "", "authors": "John D Owens and Bruce Hoppe"}, {"title": "Teaching Network Traffic Matrices in an Interactive Game Environment", "abstract": "The Internet has become a critical domain for modern society that requires ongoing efforts for its improvement and protection. Network traffic matrices are a powerful tool for understanding and analyzing networks and are broadly taught in online graph theory educational resources. Network traffic matrix concepts are rarely available in online computer network and cybersecurity educational resources. To fill this gap, an interactive game environment has been developed to teach the foundations of traffic matrices to the computer networking community. The game environment provides a convenient, broadly accessible, delivery mechanism that enables making material available rapidly to a wide audience. The core architecture of the game is a facility to add new network traffic matrix training modules via an easily editable JSON file. Using this facility an initial set of modules were rapidly created covering: basic traffic \u2026", "year": 2024, "venue": "", "authors": "Chasen Milner and Hayden Jananthan and Jeremy Kepner and Vijay Gadepally and Michael Jones and Peter Michaleas and Ritesh Patel and Sandeep Pisharody and Gabriel Wachman and Alex Pentland"}, {"title": "Interpretable Analysis of Production GPU Clusters Monitoring Data via Association Rule Mining", "abstract": "Modern high-performance computing (HPC) and cloud computing systems are integrating powerful GPUs to accelerate increasingly demanding deep learning workloads. To improve cluster efficiency and better understand user behavior and job characteristics, system operators will collect operational data for trace analysis. However, previous efforts on these system logs have lacked the interpretability aspect, and there is no systematic approach that can be widely applied to different datacenter traces and return interpretable results. In this work, we propose a workflow to discover hidden association relation-ships between collected features of system jobs. The outcome of our analysis approach yields useful association rules that can be directly interpreted into operational insights. Using this approach, we have conducted case studies using the traces of three large-scale multi-tenant GPU clusters running production \u2026", "year": 2024, "venue": "", "authors": "Baolin Li and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "On a Novel Application of Wasserstein-Procrustes for Unsupervised Cross-Lingual Alignment of Embeddings", "abstract": "Unsupervised word embeddings, pre-trained on vast monolingual text corpora, have driven the neural revolution in Natural Language Processing (NLP). Initially developed for English, these embeddings soon expanded to other languages, spurring efforts to align embedding spaces for cross-lingual NLP applications. Unsupervised cross-lingual alignment of embeddings (UCAE) is particularly appealing due to its minimal data requirements and competitive performance against supervised and semi-supervised approaches. In this work, we scrutinize prevalent UCAE methods and discover their objectives inherently resemble the Wasserstein-Procrustes problem. Consequently, we propose a direct solution for Wasserstein-Procrustes, enhancing popular UCAE techniques such as iterative closest point (ICP), multilingual unsupervised and supervised embeddings (MUSE), and supervised Procrustes methods. Evaluation on benchmark datasets demonstrates significant improvements over existing approaches. Our reexamination of the Wasserstein-Procrustes problem fosters further research, paving the way for more effective algorithms to align word embeddings across languages.", "year": 2024, "venue": "", "authors": "Guillem Ram\u00edrez and Rumen Dangovski and Preslav Nakov and Marin Solja\u010di\u0107"}, {"title": "TENG: Time-Evolving Natural Gradient for Solving PDEs with Deep Neural Net", "abstract": "", "year": 2024, "venue": "", "authors": "Zhuo Chen and Jacob McCarran and Esteban Vizcaino and Marin Solja\u010di\u0107 and Di Luo"}, {"title": "Backpropagation through Back Substitution with a Backslash", "abstract": "We present a linear algebra formulation of backpropagation which allows the calculation of gradients by using a generically written \u201cbackslash\u201d or Gaussian elimination on triangular systems of equations. Generally, the matrix elements are operators. This paper has three contributions: (i) it is of intellectual value to replace traditional treatments of automatic differentiation with a (left acting) operator theoretic, graph-based approach; (ii) operators can be readily placed in matrices in software in programming languages such as Julia as an implementation option; (iii) we introduce a novel notation, \u201ctranspose dot\u201d operator \u201c\u201d that allows for the reversal of operators. We further demonstrate the elegance of the operators approach in a suitable programming language consisting of generic linear algebra operators such as Julia [Bezanson et al., SIAM Rev., 59 (2017), pp. 65\u201398], and that it is possible to realize this \u2026", "year": 2024, "venue": "", "authors": "Alan Edelman and Ekin Aky\u00fcrek and Yuyang Wang"}, {"title": "Physics-Informed Neural Networks for Satellite State Estimation", "abstract": "The Space Domain Awareness (SDA) community routinely tracks satellites in orbit by fitting an orbital state to observations made by the Space Surveillance Network (SSN). In order to fit such orbits, an accurate model of the forces that are acting on the satellite is required. Over the past several decades, high-quality, physics-based models have been developed for satellite state estimation and propagation. These models have widely varying degrees of fidelity: some only account f or two-body Keplerian motion, while others consider highly accurate Earth gravity models, atmospheric drag, solar radiation pressure (SRP), perturbations from the Sun, Moon, and other celestial bodies, etc. These models are exceedingly good at estimating and propagating orbital states for non-maneuvering satellites; however, there are several classes of anomalous accelerations that a satellite might experience which are not well \u2026", "year": 2024, "venue": "", "authors": "Jacob Varey and Jessica D Ruprecht and Michael Tierney and Ryan Sullenberger"}, {"title": "Making Machine Learning on HPC Systems Cost-Effective and Carbon-Friendly", "abstract": "The rapid adoption of machine learning (ML) has increased the computational and storage demands on high-performance computing (HPC) and cloud datacenters. Unfortunately, deploying ML models on HPC and cloud platforms is challenging because it is often difficult to manage, financially costly, and poses substantial environmental concerns.", "year": 2024, "venue": "", "authors": "Baolin Li"}, {"title": "Load balancing and memory optimizations for expert parallel training of large language models", "abstract": "Large language models (LLMs) are an effective way to solve many text-based machine learning tasks, but require huge amounts of computation to train and evaluate. Mixture of experts models have emerged as a way to reduce the amount of computation required for LLMs without compromising accuracy. It is necessary to distribute these large models across several devices, but this requires substantial communication between devices throughout training. Expert parallel is a promising approach to distributing the model across devices and communicating necessary information during training, especially for small batch sizes or models with large embedding sizes. Unfortunately, expert parallel creates an imbalanced workload across devices, causes errors with existing memory conservation strategies, and has poor overlapping of communication and computation. Some existing works solve the imbalanced workload by dropping excess tokens sent to experts above a capacity, but that may reduce accuracy.  In my thesis I introduce ModuleFormer-PRM, an expert parallel training system that addresses these issues without dropping tokens. I will explain a subtle error that occurs when trying to save memory and a strategy to prevent it. I will analyze the distribution of workload among experts and show two approaches to better balance the workload across devices, leading to more stable memory use and faster runtime. I evaluate ModuleFormerPRM using pretrained MoE models and show my optimizations improved expert parallel\u2019s throughput by 2.1\u00d7.", "year": 2024, "venue": "", "authors": "Daniel Wisdom"}, {"title": "Spacecraft Orbiting and Uncertainty-Planning Surveillance", "abstract": "Scheduling of the Space Surveillance Network (SSN) is a crucial operation for the maintenance of safety and operations in Earth\u2019s orbit. However, the capabilities of the SSN are limited and the number of objects that are being tracked is increasing with every year. This work proposes harnessing Imitation learning (IL) to develop explainable schedules without the development of subjective functions, but instead learning from approved schedules. To that end is proposed a graph structuring of the situation that allows learning from expert solutions. Importantly, this proposed framework also removes fragmentation and discretisation requirements within the time and space domains, requirements that are present in other solutions and lower the asymptotic efficiency that can be achieved. However, the models that were trained in this work did not achieve these goals and showed a very strong competition between the capability to choose the correct pass to observe an object and choosing the correct time within the pass. The trained models also showed a significant maintenance of performance of a trained model on data inputs outside of distribution. Overall, this thesis provides the necessary background to understand the principles of decision making for developing an SSN schedule, shows the set up of a graph structure for the basis of an IL algorithm for scheduling, and presents the results that have been obtained to this point.", "year": 2024, "venue": "", "authors": "Joana N Nikolova"}, {"title": "Fast Partitioning for Distributed Graph Learning using Multi-level Label Propagation", "abstract": "Graph Neural Networks (GNNs) are a popular class of machine learning models that allow scientists to leverage machine learning techniques to perform inference on unstructured data. However, when graphs become too large, partitioning becomes necessary to allow for distributed computation. Standard graph partitioning methods for GNNsinclude Random partitioning and the state-of-the-art METIS. Whereas METIS produces partitions of high-quality, its preprocessing overheads make it impractical for extremely large graphs. Conversely, random partitioning is cheap to compute, but results in poor partition quality that causes GNN training to be bottlenecked by communication. In my thesis, I seek to prove that it is possible to reduce the data preprocessing overhead on small machines for large graph datasets used in ML while maintaining partition quality. In support of this goal, I design and implement a hierarchical label-propagation-based graph partitioning system known as PLaTE (Propagating Labels to Train Efficiently), partially based on the paper \u201cHow to Partition a Billion Node Graph\u201d [18]. PLaTE runs 5.6x faster than METIS on the Open Graph Benchmark\u2019s papers100M dataset, while consuming 4.9x less memory. PLaTE produces partitions that are equally balanced to METIS with comparable communication volumes under certain conditions. In real GNN training experiments, PLaTE has comparable average epoch times to METIS.", "year": 2024, "venue": "", "authors": "Yaseen Alkhafaji"}, {"title": "Mosaic Pages: Big TLB Reach with Small", "abstract": "This article introduces mosaic pages, which increase TLB reach by compressing multiple, discrete translations into one TLB entry. Mosaic leverages virtual contiguity for locality, but does not use physical contiguity. Mosaic relies on recent advances in hashing theory to constrain memory mappings, in order to realize this physical address compression without reducing memory utilization or increasing swapping. Mosaic reduces TLB misses in several workloads by 6\u201381%. Our results show that Mosaic\u2019s constraints on memory mappings do not harm performance, we never see conflicts before memory is 98% full in our experiments\u2014at which point, a traditional design would also likely swap. Timing and area analyses on a commercial 28nm CMOS process indicate that the hashing required on the critical path can run at a maximum frequency of 4 GHz, indicating that a Mosaic TLB is unlikely to affect clock frequency.Data-hungry applications, such as data and graph analytics, are often bottlenecked on the translation lookaside buffer (TLB). A typical", "year": 2024, "venue": "", "authors": "AMD Krishnan Gosakan and William Kuszmaul and Ibrahim N Mubarek and Nirjhar Mukherjee and Karthik Sriram and Evan West and Michael A Bender and Abhishek Bhattacharjee and Mart\u00edn Farach-Colton"}, {"title": "Safe neural control for non-affine control systems with differentiable control barrier functions", "abstract": "This paper addresses the problem of safety-critical control for non-affine control systems. It has been shown that optimizing quadratic costs subject to state and control constraints can be sub-optimally reduced to a sequence of quadratic programs (QPs) by using Control Barrier Functions (CBFs). Our recently proposed High Order CBFs (HOCBFs) can accommodate constraints of arbitrary relative degree. The main challenges in this approach are that it requires affine control dynamics and the solution of the CBF-based QP is sub-optimal since it is solved point-wise. To address these challenges, we incorporate higher-order CBFs into neural ordinary differential equation-based learning models as differentiable CBFs to guarantee safety for non-affine control systems. The differentiable CBFs are trainable in terms of their parameters, and thus, they can address the conservativeness of CBFs such that the system state \u2026", "year": 2023, "venue": "", "authors": "Wei Xiao and Ross Allen and Daniela Rus"}, {"title": "Fuzzy Relational Databases via Associative Arrays", "abstract": "The increasing rise in artificial intelligence has made the use of imprecise language in computer programs like ChatGPT more prominent. Fuzzy logic addresses this form of imprecise language by introducing the concept of fuzzy sets, where elements belong to the set with a certain membership value (called the fuzzy value). This paper combines fuzzy data with relational algebra to provide the mathematical foundation for a fuzzy database querying language, describing various useful operations in the language of linear algebra and multiset operations, in addition to rigorously proving key identities.", "year": 2023, "venue": "", "authors": "Kevin Min and Hayden Jananthan and Jeremy Kepner"}, {"title": "Testing RadiX-Nets: Advances in Viable Sparse Topologies", "abstract": "The exponential growth of data has sparked compu-tational demands on ML research and industry use. Sparsification of hyper-parametrized deep neural networks (DNNs) creates simpler representations of complex data. Past research has shown that some sparse networks achieve similar performance as dense ones, reducing runtime and storage. RadiX-Nets, a subgroup of sparse DNNs, maintain uniformity which counteracts their lack of neural connections. Generation, independent of a dense network, yields faster asymptotic training and removes the need for costly pruning. However, little work has been done on RadiX-Nets, making testing challenging. This paper presents a testing suite for RadiX-Nets in TensorFlow. We test RadiX-Net performance to streamline processing in scalable models, revealing relationships between network topology, initialization, and training behavior. We also encounter \u201cstrange \u2026", "year": 2023, "venue": "", "authors": "Kevin Kwak and Zack West and Hayden Jananthan and Jeremy Kepner"}, {"title": "Algebraic Conditions on One-Step Breadth-First Search", "abstract": "The GraphBLAS community has demonstrated the power of linear algebra-leveraged graph algorithms, such as matrix-vector products for breadth-first search (BFS) traversals. This paper investigates the algebraic conditions needed for such computations when working with directed hypergraphs, represented by incidence arrays with entries from an arbitrary value set with binary addition and multiplication operations. Our results show the one-step BFS traversal is equivalent to requiring specific algebraic properties of those operations. Assuming identity elements 0, 1 for operations, we show that the two operations must be zero-sum-free, zero-divisor-free, and 0 must be an annihilator under multiplication. Additionally, associativity and commutativity are shown to be necessary and sufficient for independence of the one-step BFS computation from several arbitrary conventions. These results aid in application and \u2026", "year": 2023, "venue": "", "authors": "Emma Fu and Hayden Jananthan and Jeremy Kepner"}, {"title": "Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control", "abstract": "Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can outperform their full-rank, fully-connected counterparts in the online setting under distribution shift. This yields memory-efficient and robust agents while opening a new perspective on how we can modulate network dynamics through connectivity.", "year": 2023, "venue": "", "authors": "Neehal Tumma and Mathias Lechner and Noel Loo and Ramin Hasani and Daniela Rus"}, {"title": "Optimizing Compression Schemes for Parallel Sparse Tensor Algebra", "abstract": "This paper studies compression techniques for parallel in-memory sparse tensor algebra. We find that applying simple existing compression schemes can lead to performance loss in some cases. To resolve this issue, we introduce an optimized algorithm for processing compressed inputs that can improve both the space usage as well as the performance compared to uncompressed inputs. We implement the compression techniques on top of a suite of sparse matrix algorithms generated by taco, a compiler for sparse tensor algebra. On a machine with 48 hyperthreads, our empirical evaluation shows that compression reduces the space needed to store the matrices by over 2 x without sacrificing algorithm performance.", "year": 2023, "venue": "", "authors": "Helen Xu and Tao B Schardl and Michael Pellauer and Joel S Emer"}, {"title": "Contextualizing Enhances Gradient Based Meta Learning for Few Shot Image Classification", "abstract": "Meta learning methods have found success when applied to few shot classification problems, in which they quickly adapt to a small number of labeled examples. Prototypical representations, each representing a particular class, have been of particular importance in this setting, as they provide a compact form to convey information learned from the labeled examples. However, these prototypes are just one method of representing this information, and they are narrow in their scope and ability to classify unseen examples. We propose the implementation of contextualizers, which are generalizable prototypes that adapt to given examples and play a larger role in classification for gradient-based models. We demonstrate how to equip meta learning methods with contextualizers and show that their use can significantly boost performance on a range of few shot learning datasets. We also present figures of merit \u2026", "year": 2023, "venue": "", "authors": "Evan Vogelbaum and Rumen Dangovski and Li Jing and Marin Soljacic"}, {"title": "An Analysis of Energy Requirement for Computer Vision Algorithms", "abstract": "The energy requirements of neural network learning are growing at a rapid rate. Increased energy demands have caused a global need to seek ways to improve energy efficiency of neural network learning. This paper aims to establish a baseline on how adjusting basic parameters can affect energy consumption in neural network learning on Computer Vision tasks. In this article, we catalog the effects of various adjustments, from simple batch size adjustments to more complicated hardware settings (e.g., power capping). Based on our characterizations, we have found numerous avenues to adjust computer vision algorithm energy expenditure. For example, switching from a single precision model to mixed precision training can result in energy reductions of nearly 40 %. dditionally, power capping the Graphical Processing Unit (GPU) can reduce energy cost by an additional 10%.", "year": 2023, "venue": "", "authors": "Daniel Edelman and Siddharth Samsi and Joseph McDonald and Adam Michaleas and Vijay Gadepally"}, {"title": "A nearly tight lower bound for the d-dimensional cow-path problem", "abstract": "In the d-dimensional cow-path problem, a cow living in R d must locate a (d\u2212 1)-dimensional hyperplane H whose location is unknown. The only way that the cow can find H is to roam R d until it intersects H. If the cow travels a total distance s to locate a hyperplane H whose distance from the origin was r\u2265 1, then the cow is said to achieve competitive ratio s/r. It is a classic result that, in R 2, the optimal (deterministic) competitive ratio is 9. In R 3, the optimal competitive ratio is known to be at most\u2248 13.811. But in higher dimensions, the asymptotic relationship between d and the optimal competitive ratio remains an open question. The best upper and lower bounds, due to Antoniadis et al., are O (d 3/2) and \u03a9 (d), leaving a gap of roughly d. In this note, we achieve a stronger lower bound of \u03a9\u02dc(d 3/2).", "year": 2023, "venue": "", "authors": "Nikhil Bansal and John Kuszmaul and William Kuszmaul"}, {"title": "Who Provides the Largest Megaphone? The Role of Google News in Promoting Russian State-Affiliated News Sources", "abstract": "The Internet has not only digitized but also democratized information access across the globe. This gradual but path-breaking move to online information propagation has resulted in search engines playing an increasingly prominent role in shaping access to human knowledge. When an Internet user enters a query, the search engine sorts through the hundreds of billions of possible webpages to determine what to show. Google dominates the search engine market, with Google Search surpassing 80% market share globally every year of the last decade. Only in Russia and China do Google competitors claim more market share, with approximately 60% of Internet users in Russia preferring Yandex (compared to 40% in favor of Google) and more than 80% of China's Internet users accessing Baidu as of 2022. Notwithstanding this long-standing regional variation in Internet search providers, there is limited research showing how these providers compare in terms of propagating state-sponsored information. Our study fills this research gap by focusing on Russian cyberspace and examining how Google and Yandex's search algorithms rank content from Russian state-controlled media (hereon, RSM) outlets. This question is timely and of practical interest given widespread reports indicating that RSM outlets have actively engaged in promoting Kremlin propaganda in the lead-up to, and in the aftermath of, the Russian invasion of Ukraine in February 2022.", "year": 2023, "venue": "", "authors": "Keeley Erhardt and Saurabh Khanna"}, {"title": "Investigating Learners Perceptions of Completion and Certification in MOOCs", "abstract": "[EN] Understanding learners\u2019 perceptions of their own learning outcomes is critical for accurately interpreting those outcomes and planning interventions to help improve them. Past research in massive open online courses (MOOCs) shows that many learners enroll in courses they do not finish, and much research has been conducted investigating the patterns and trends driving this. This paper uses a qualitative methodology to understand how learners perceive course completion and certification, and why they do or do not meet their learning goals. Data were analyzed from fifteen interviews with learners who had enrolled in at least one MOOC. The data suggests that learners have a complex understanding of completion that varies depending on their own goals and access to the material. It also shows that they see certification as distinct from completion, and will only be willing to pursue certification under certain circumstances.", "year": 2023, "venue": "", "authors": "Meghan Perdue"}, {"title": "Learning When to Ask for Help: Transferring Human Knowledge through Part-Time Demonstration", "abstract": "Robots operating alongside humans often encounter unfamiliar environments that make autonomous task completion challenging. Though improving models and increasing dataset size can enhance a robot's performance in unseen environments, dataset generation and model refinement may be impractical in every unfamiliar environment. Approaches that utilize human demonstration through manual operation can aid in generalizing to these unfamiliar environments, but often require significant human effort and expertise to achieve satisfactory task performance. To address these challenges, we propose leveraging part-time human interaction for redirection of robots during failed task execution. We train a lightweight help policy that allows robots to learn when to proceed autonomously or request human assistance at times of uncertainty. By incorporating part-time human intervention, robots recover quickly from \u2026", "year": 2023, "venue": "", "authors": "Ifueko Igbinedion and Sertac Karaman"}, {"title": "The conditional DPP approach to random matrix distributions", "abstract": "We present the conditional determinantal point process (DPP) approach to obtain new (mostly Fredholm determinantal) expressions for various eigenvalue statistics in random matrix theory. It is well-known that many (especially ) eigenvalue -point correlation functions are given in terms of  determinants, i.e., they are continuous DPPs. We exploit a derived kernel of the conditional DPP which gives the -point correlation function conditioned on the event of some eigenvalues already existing at fixed locations. Using such kernels we obtain new determinantal expressions for the joint densities of the  largest eigenvalues, probability density functions of the  largest eigenvalue, density of the first eigenvalue spacing, and more. Our formulae are highly amenable to numerical computations and we provide various numerical experiments. Several numerical values that required hours of computing time could now be computed in seconds with our expressions, which proves the effectiveness of our approach. We also demonstrate that our technique can be applied to an efficient sampling of DR paths of the Aztec diamond domino tiling. Further extending the conditional DPP sampling technique, we sample Airy processes from the extended Airy kernel. Additionally we propose a sampling method for non-Hermitian projection DPPs.", "year": 2023, "venue": "", "authors": "Alan Edelman and Sungwoo Jeong"}, {"title": "Applying Human-Centered Design to AI-Enabled Pilot Scheduling", "abstract": "Air Force mission and training scheduling is an immensely complex, time-consuming, and significantly manual process. A scheduling tool known as Puckboard has been developed to help C-17 squadrons transition from moving pucks across large whiteboards to utilizing technology to dynamically plan and deconflict resources in the presence of complex constraints. The overarching goal of incorporating artificial intelligence (AI) into this tool is to empower schedulers to quickly produce more efficient schedules that promote unit readiness, with more pilots completing their training syllabi faster, and with fewer disruptions to missions, training, and aircrew personal life. Our AI efforts focused on refining a neural network approach combining reinforcement learning with linear programming to generate optimal schedules across varying timeframes. The development of this AI-enabled pilot scheduling tool involved applying human-centered design best practices, namely actively involving end-users to inform persona generation, tool functionality, existing and AI-enabled workflows, and wireframe development and iteration.", "year": 2023, "venue": "", "authors": "Amy L Alexander and Audrey Haque and Michael Snyder and Rachael Kusiak and Brice Okubo and Kathie Chung and Eric Robinson"}, {"title": "Algorithmic Interactions With Strategic Users: Incentives, Interplay, and Impact", "abstract": "The societal challenges posed by machine learning algorithms are becoming increasingly important, and to effectively study them, it is crucial to incorporate the incentives and preferences of users into the design of algorithms. In many cases, algorithms are solely designed based on the platform's objectives, without taking into account the potential misalignment between the platform's goals and the interests of users.", "year": 2023, "venue": "", "authors": "Alireza Fallah"}, {"title": "AI For Leadership: Implementation And Evaluation Of An AI Education Program", "abstract": "AI education is rapidly becoming the next frontier when it comes to solving the world's grand challenges; however, ways to introduce AI to large complex organisations are still vastly understudied. To address this gap in 2021, Massachusetts Institute of Technology (MIT) entered into a collaboration with the US Air Force (USAF). The goal of this relationship is to develop, study, and evaluate different learning modalities and online/in-person experiences to introduce AI to the diverse USAF workforce. The USAF is a very complex organisation and its employees vary in terms of educational and cultural backgrounds, as well as in their work-related needs, demands and restrictions. The initial program started in 2021 and a pilot study took place. The pilot evaluated the content, pedagogy, and educational technology used in 3 different learning journeys designed for 6 different learner profiles. Findings from 2021 guided improvements for future iterations. The updated iteration of the learning journey was introduced to the second cohort of the program in 2022. Cohort 2 included 200 USAF leaders, managers and decision makers, and the learning journey consisted of a combination of synchronous and asynchronous online experiences, as well as an in-person active learning component offered on campus to a subgroup of the learners. This research paper will introduce the updated iteration of the program, the evaluation of the learning journey, as well as the overall learner experience.", "year": 2023, "venue": "", "authors": "Aikaterini BAGIATI and Andr\u00e9s F SALAZAR-G\u00d3MEZ and Annalyn BACHMANN and Kathleen D KENNEDY and Cynthia BREAZEAL"}, {"title": "Improving the Performance of Parallel Loops in OpenCilk", "abstract": "For good performance, parallel loop scheduling must achieve low scheduling overheads and multidimensional locality in nested loops. This thesis explores both challenges and contributes an extension to randomized work-stealing for first-class loop support that reduces scheduling overheads.  Randomized work-stealing schedulers traditionally execute parallel-for loops using parallel divide-and-conquer recursion, which is theoretically efficient and scalable but can incur substantial overheads in practice. This thesis extends randomized work-stealing with a custom work-stealing protocol called on-the-fly loop splitting. I introduce loop frames to make work stealing on parallel-for loops more efficient and flexible.  Loop frames make two key changes to work stealing for parallel-for loops. First, loop frames extend work stealing by directly encoding information about intervals of loop iterations in the runtime. Loop frames add first-class support to work stealing for parallel-for loops that composes with classical randomized work stealing. Second, loop frames allow intervals of loop iterations to be split on-the-fly, such that worker threads attempt to steal half of the unexecuted loop iterations rather than a deterministically constructed partition of loop iterations. On-the-fly loop splitting allows for more flexible dynamic load balancing of loop iterations while keeping the work overheads low and maintaining the theoretical efficiency of divide-and-conquer.  I evaluate loop frames in practice by implementing loop frames in the OpenCilk runtime system. In particular, loop frames augment the THE protocol from Cilk to coordinate updates to loop frames. I observe that \u2026", "year": 2023, "venue": "", "authors": "Luka Govedic"}, {"title": "Optimization and Generalization of Minimax Algorithms", "abstract": "This thesis explores minimax formulations of machine learning and multi-agent learning problems, focusing on algorithmic optimization and generalization performance. The first part of the thesis delves into the smooth convex-concave minimax problem, providing a unified analysis of widely used algorithms such as Extra-Gradient (EG) and Optimistic Gradient Descent Ascent (OGDA), whose convergence behavior was not systematically understood. We derive convergence rates for these algorithms in the convex-concave setting. We show that these algorithms work effectively due to their approximation of the Proximal Point (PP) method, which converges to the solution at a fast rate, but is impractical to implement. In the next chapter, we expand our study to nonconvex-nonconcave problems. These problems are generally challenging to solve, as a solution may not be well defined, or even if a solution exists, its \u2026", "year": 2023, "venue": "", "authors": "Sarath Pattathil"}, {"title": "Supercharging Programming through Compiler Technology", "abstract": "The decline of Moore\u2019s law and an increasing reliance on computation has led to an explosion of specialized software packages and hardware architectures. While this diversity enables unprecedented flexibility, it also requires domain-experts to learn how to customize programs to efficiently leverage the latest platform-specific API\u2019s and data structures, instead of working on their intended problem. For example, a researcher hoping to use machine learning on climate code must write a corresponding derivative simulation, understand and implement linear algebra routines, and performance engineer their simulation to run on multiple cores and nodes. Rather than forcing each user to bear this burden, I propose building high-level abstractions within general-purpose compilers that enable fast, portable, and composable programs to be automatically generated.  This thesis will demonstrate this approach through several real-world and composable compilers that I built for a variety of domains including parallelism, automatic differentiation, scheduling, portability, program search, and tensor arithmetic. These domains are critical to both scientific computing and machine learning. Individually, integration of domain knowledge into each of these compilers enable (often asymptotic) performance and usability benefits. Operating on a common compiler representation, however, enables these benefits to compound and provide greater performance than any domain-specific optimization in isolation.  This research in this thesis contains joint work with Charles E. Leiserson, Tao B. Schardl, Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya \u2026", "year": 2023, "venue": "", "authors": "William S Moses"}, {"title": "Decision Transformer-based Traveling Salesman Tour Generation", "abstract": "With the surge of new machine learning methods, research in classic problems like the Traveling Salesman Problem (TSP) is receiving a resurgence of popularity. One of the biggest goals in this renewed interest is to create a model that can not only outperform state-of-the-art heuristic solvers in speed for trivial sizes, but also generalize to larger TSP instances that are currently intractable. In this thesis we approach the TSP with the Decision Transformer, a transformer-based architecture transforming reinforcement learning environments into transformer-compatible sequence-modeling problems. By modeling a TSP instance as an graph-based environment with states and actions, we can input partial tours into the Decision Transformer to infer the next best action in an autoregressive fashion. With the power of the transformer, we take the first step in making headway on the issue of generalization where past models have failed.", "year": 2023, "venue": "", "authors": "Daniel S Liu"}, {"title": "Mission Optimized Weather Nowcasting for Flight Planning", "abstract": "View Video Presentation: https://doi.org/10.2514/6.2023-4095.vidWeather nowcasts are highly efficient, short term forecasts that are widely used for tactical flight planning due to their ability to rapidly ingest and process recent weather data.  Nowcasts are particularly important for uncrewed and remotely piloted aircraft that are required to navigate through hazardous weather conditions in a manner that maximizes the chances of mission success while minimizing risks of damage, fuel loss, or other mission failures.  Recently, deep learning techniques have been widely applied to create highly efficient weather nowcasts that could help assist pilots and mission planners, however many of these neural network models are trained using loss functions based on metrics of image quality that do not necessarily align with operational mission needs.  In this work we present a new paradigm for adapting DL-based weather \u2026", "year": 2023, "venue": "", "authors": "Mark S Veillette and Virginia Goodwin"}, {"title": "Continuation Stealing in Julia", "abstract": "Work stealing schedulers are widely used by parallel programming platforms to distribute tasks across multiple processors. Memory blowup from scheduling a program with work stealing can be bounded by using continuation stealing when new tasks are spawned. Continuation stealing is opposed to child stealing, a different method for spawning tasks that is simpler to implement, but comes at the expense of potentially unbounded memory use. An extension to the Julia programming langauge that adds support for optimizable spawn and sync parallel constructs has been proposed, but it does not currently support continuation stealing. In my thesis, I implement continuation stealing in Julia using two different metaprogramming approaches. One approach uses Julia\u2019s macro system, while the other uses the Julia compiler\u2019s internal representation (IR) of functions. My results show that the IR-based approach uses less memory than the child stealing implementation in the proposed extension to Julia, while having similar speed.", "year": 2023, "venue": "", "authors": "August Trollback"}, {"title": "BP-Tree: Overcoming the Point-Range Operation Tradeoff for In-Memory B\u207a-trees", "abstract": "", "year": 2023, "venue": "", "authors": "Amanda Li"}, {"title": "Finding and Optimizing Certified, Collision-Free Regions in Configuration Space for Robot Manipulators", "abstract": "", "year": 2022, "venue": "", "authors": "Russ Tedrake"}, {"title": "Solving Least Squares Problems on Partially Ordered Sets", "abstract": "We study a general class of least-squares problems structured according to a partially ordered set (poset). This is a fundamental optimization problem underlying the design of structured controllers on directed acyclic graphs or posets. We show that the optimality conditions of this problem yield a structured linear system, with sparsity pattern determined by a derived poset known as the poset of intervals. In general, this system could be relatively dense, and thus standard sparse linear algebra techniques may fail to provide significant reduction in computational complexity. Nonetheless, for a broad class of posets called multitrees identified in [1] we show that performing elimination according to an order defined by the poset intervals progressively decouples variables, reducing the arithmetic complexity of solving the problem.", "year": 2022, "venue": "", "authors": "Alexandre Amice and Pablo A Parrilo"}, {"title": "Image recognition time for humans predicts adversarial vulnerability for models", "abstract": "The success of adversarial attacks and the performance tradeoffs made by adversarial defense methods have both traditionally been evaluated on image test sets constructed from a randomly sampled held out portion of a training set. Mayo 2022 et al.[1] measured the difficulty of the ImageNet and ObjectNet test sets by measuring the minimum viewing time required for an object to be recognized on average by a human, finding that these test sets are heavily skewed towards containing mostly easy, quickly recognized images. While difficult images that require longer viewing times to be recognized are uncommon in test sets, they are both common and critically important to the real world performance of vision models. In this work, we investigated the relationship between adversarial robustness and viewing time difficulty. Measuring the AUC of accuracy vs attack strength (epsilon), we find that easy, quickly recognized, images are more robust to adversarial attacks than difficult images, which require several seconds of viewing time to recognize. Additionally, adversarial defense methods improve models robustness to adversarial attacks on easy images significantly more than on hard images. We propose that the distribution of image difficulties should be carefully considered and controlled for when measuring both the effectiveness of adversarial attacks and when analyzing the clean accuracy vs robustness tradeoff made by adversarial defense methods.", "year": 2022, "venue": "", "authors": "David Mayo and Jesse Cummings and Xinyu Lin and Boris Katz and Andrei Barbu"}, {"title": "Comparison Between Deep Learning and Parametric Methods in Target Imaging from Narrowband Radar Data", "abstract": "Data interpolation techniques via parametric method and deep learning network are investigated to enhance range resolution on images generated from complex radar data collected at narrow bandwidths. A mathematical framework for the parametric method is presented and the very deep super-resolution (VDSR) network developed for real electro-optical data is adapted to deal with complex data. The techniques are applied to data collected on a canonical target at 80 and 120 MHz bandwidths that comprise, respectively, 3 and 4 frequency samples over the range of aspect angles that span from -5 degrees to 95 degrees. Composite images generated from the interpolated data of both techniques show their practicality and usefulness to unravel a target body where images from the original data failed. By comparing results from both interpolated methods, it is demonstrated that the parametric signal processing \u2026", "year": 2022, "venue": "", "authors": "Jean E Piou"}, {"title": "Workshop version: How hard are computer vision datasets? Calibrating dataset difficulty to viewing time", "abstract": "Humans outperform object recognizers despite the fact that models perform well on current datasets. Numerous efforts exist to make more challenging datasets by scaling up on the web, exploring distribution shift, or adding controls for biases. The difficulty of each image in each dataset is not independently evaluated, nor is the concept of dataset difficulty as a whole currently well defined. We develop a new dataset difficulty metric based on how long humans must view an image in order to classify a target object. Images whose objects can be recognized in 17ms are considered to be easier than those which require seconds of viewing time. Using 133,588 judgments on two major datasets, ImageNet and ObjectNet, we determine the distribution of image difficulties in those datasets, which we find varies wildly, but significantly undersamples hard images. Rather than hoping that distribution shift will lead to hard datasets, we should explicitly measure their difficulty. Analyzing model performance guided by image difficulty reveals that models tend to have lower performance and a larger generalization gap on harder images. We release a dataset of difficulty judgments as a complementary metric to raw performance and other behavioral/neural metrics. Such experiments with humans allow us to create a metric for progress in object recognition datasets. This metric can be used to both test the biological validity of models in a novel way, and develop tools to fill out the missing class of hard examples as datasets are being gathered.", "year": 2022, "venue": "", "authors": "David Mayo and Jesse Cummings and Xinyu Lin and Dan Gutfreund and Boris Katz and Andrei Barbu"}, {"title": "Using domain knowledge in coevolution and reinforcement learning to simulate a logistics enterprise", "abstract": "We demonstrate a framework (CoEv-Soar-RL) for a logistics enterprise to improve readiness, sustainment, and reduce operational risk. The CoEv-Soar-RL uses reinforcement learning and coevolutionary algorithms to improve the functions of a logistics enterprise value chain. We address: (1) holistic prediction, optimization, and simulation for the logistics enterprise readiness; (2) the uncertainty and lack of data which require large-scale systematic what-if scenarios to simulate potential new and unknown situations. In this paper, we perform four experiments to investigate how to integrate prediction and simulation to modify a logistics enterprise's demand models and generate synthetic data based. We use general domain knowledge to design simple operators for the coevolutionary search algorithm that provide realistic solutions for the simulation of the logistic enterprise. In addition, to evaluate generated solutions \u2026", "year": 2022, "venue": "", "authors": "Ying Zhao and Erik Hemberg and Nate Derbinsky and Gabino Mata and Una-May O'Reilly"}, {"title": "Efficient Representation of Large-Alphabet Probability Distributions via Arcsinh-Compander", "abstract": "A number of engineering and scientific problems require representing and manipulating probability distributions over large alphabets, which we may think of as long vectors of reals summing to 1. In some cases it is required to represent such a vector with only b bits per entry. A natural choice is to partition the interval [0,1] into 2b uniform bins and quantize entries to each bin independently. We show that a minor modification of this procedure \u2013 applying an entrywise non-linear function (compander) f(x) prior to quantization \u2013 yields an extremely effective quantization method. For example, for b = 8(16) and 105-sized alphabets, the quality of representation improves from a loss (under KL divergence) of 0.5(0.1) bits/entry to 10\u22124(10\u22129) bits/entry. Compared to floating point representations, our compander method improves the loss from 10\u22121(10\u22126) to 10\u22124(10\u22129) bits/entry. These numbers hold for both real-world data \u2026", "year": 2022, "venue": "", "authors": "Aviv Adler and Jennifer Tang and Yury Polyanskiy"}, {"title": "Mashup: Making Serverless Computing Useful for HPC Workflows via Hybrid Execution", "abstract": "This work introduces Mashup, a novel strategy to leverage serverless computing model for executing scientific workflows in a hybrid fashion by taking advantage of both the traditional VM-based cloud computing platform and the emerging serverless platform. Mashup outperforms the state-ofthe-art workflow execution engines by an average of 34% and 43% in terms of execution time reduction and cost reduction, respectively, for widely-used HPC workflows on the Amazon Cloud platform (EC2 and Lambda). CCS Concepts:\u2022 Computer systems organization\u2192 Cloud computing; Heterogeneous (hybrid) systems;\u2022 Computing methodologies\u2192 Distributed computing methodologies; Massively parallel and high-performance simulations.", "year": 2022, "venue": "", "authors": "Rohan Basu Roy and Tirthak Patel and Vijay Gadepally and Devesh Tiwari"}, {"title": "Do temperature and humidity exposures hurt or benefit your SSDs?", "abstract": "SSDs are becoming mainstream data storage de-vices, replacing HDDs in most data centers, consumer goods, and IoT gadgets. In this work, we ask an uncharted research question: What is the environmental conditions' impact on SSD performance? To answer it, we systematically measure, quantify, and characterize the impact of various commonly changing envi-ronmental conditions such as temperature and humidity on the performance of SSDs. Our experiments and analysis uncover that exposure to changes in temperature and humidity can significantly affect SSD performance.", "year": 2022, "venue": "", "authors": "Adnan Maruf and Sashri Brahmakshatriya and Baolin Li and Devesh Tiwari and Gang Quan and Janki Bhimani"}, {"title": "Discovering Conservation Laws via Manifold Learning", "abstract": "Conservation laws are key theoretical and practical tools for understanding, characterizing, and modeling nonlinear dynamical systems. However, for many complex dynamical systems, the corresponding conserved quantities are difficult to identify, making it hard to analyze their dynamics and build efficient, stable predictive models. Many current approaches for discovering conservation laws rely on fine-grained time measurements and dynamical information. We instead reformulate this task as a manifold learning problem and propose a non-parametric approach, combining the Wasserstein metric from optimal transport with diffusion maps, to determine all the conserved quantities that vary across trajectories sampled from a dynamical system. We test this new approach on a variety of physical systems and demonstrate that our manifold learning method is able to both identify the number of conserved quantities and \u2026", "year": 2022, "venue": "", "authors": "Peter Lu and Rumen Dangovski and Marin Solja\u010di\u0107"}, {"title": "Hydra: A real-time spatial perception engine for 3d scene graph construction and optimization", "abstract": "", "year": 2022, "venue": "", "authors": "Nathan Hughes and Yun Chang and Luca Carlone"}, {"title": "Train tracks with gaps: Applying the probabilistic method to trains", "abstract": "We identify a tradeoff curve between the number of wheels on a train car, and the amount of track that must be installed in order to ensure that the train car is supported by the track at all times. The goal is to build an elevated track that covers some large distance \u2113, but that consists primarily of gaps, so that the total amount of feet of train track that is actually installed is only a small fraction of \u2113. In order so that the train track can support the train at all points, the requirement is that as the train drives across the track, at least one set of wheels from the rear quarter and at least one set of wheels from the front quarter of the train must be touching the track at all times. We show that, if a train car has n sets of wheels evenly spaced apart in its rear and n sets of wheels evenly spaced apart in its front, then it is possible to build a train track that supports the train car but uses only \u0398 (\u2113/n) feet of track. We then consider what \u2026", "year": 2022, "venue": "", "authors": "William Kuszmaul"}, {"title": "Using Multi-Instance GPU for Efficient Operation of Multi-Tenant GPU Clusters", "abstract": "GPU technology has been improving at an expedited pace in terms of size and performance, empowering HPC and AI/ML researchers to advance the scientific discovery process. However, this also leads to inefficient resource usage, as most GPU workloads, including complicated AI/ML models, are not able to utilize the GPU resources to their fullest extent -- encouraging support for GPU multi-tenancy. We propose MISO, a technique to exploit the Multi-Instance GPU (MIG) capability on the latest NVIDIA datacenter GPUs (e.g., A100, H100) to dynamically partition GPU resources among co-located jobs. MISO's key insight is to use the lightweight, more flexible Multi-Process Service (MPS) capability to predict the best MIG partition allocation for different jobs, without incurring the overhead of implementing them during exploration. Due to its ability to utilize GPU resources more efficiently, MISO achieves 49% and 16 \u2026", "year": 2022, "venue": "", "authors": "Baolin Li and Tirthak Patel and Siddharth Samsi and Vijay Gadepally and Devesh Tiwari"}, {"title": "Optimizing Parallel Performance with Work and Span in the OpenCilk Compiler", "abstract": "OpenCilk is the modern iteration of Cilk, a multithreaded programming environment designed for high-performance multicore computing. OpenCilk consists of a LLVM fork called Tapir and a runtime scheduler, which, together, allow for OpenCilk\u2019s high performance in practice. However, there are many opportunities to improve on the implementation of OpenCilk. In particular, current grainsize calculations for OpenCilk rely on a notion of work that is limited in a few ways.  In this thesis, I propose an alternate implementation that creates a first-class notion of work and span within the OpenCilk compiler that is then used to inform optimizations within OpenCilk. I then analyze the current formulas for grainsizes. I identify key scenarios where the compiler is unable to determine a suitable grainsize and use this to suggest improvements. Finally, I construct a benchmark of one of these scenarios using a Twitter follower dataset and empirically analyze optimal grainsizes for it.", "year": 2022, "venue": "", "authors": "Nikhil Reddy"}, {"title": "Towards a Deeper Understanding of Neural Language Generation", "abstract": "In recent years, the field of language modelling has witnessed exciting developments. Especially, thanks to large-scale data, powerful model architectures, and high-speed parallel computing devices, researchers are able to train language models which can generate realistic text. However, our understanding of these powerful language models remains shallow. What aspects of the language model are good, and what aspects need to be improved? These will be the key questions behind this thesis.  This thesis includes a set of behavior analyses of language models (LMs) with a focus on generation. We will also propose methods to alleviate some of the identified problems. The four high-level topics are (1) The general sampling behavior of an auto-regressive LM. In particular, we will take a closer look at the popular sampling algorithms. (2) Whether the LM is vulnerable to adversarial attacks, and how to make it more robust. (3) The LM\u2019s ability to remember knowledge learned from data, and relatedly, what\u2019s the best way to expose this learned knowledge. (4) How to get more fine-grained control on the model\u2019s generation.", "year": 2022, "venue": "", "authors": "Tianxing He"}, {"title": "Contention Bounds for Locking Computations", "abstract": "This thesis quantifies lock contention in multithreaded programs by expanding the theoretical model of task-parallel execution traces to account for mutual exclusion locks. While lock profiling and contention detection tools abound in software, empirical measurements of contention suffer from wide fluctuations across different executions of the same code due to scheduling variation and processor availability. In this work we present analytical bounds on the maximum possible contention incurred by a given program over all possible execution schedules, even when running alongside other programs in a busy environment or when scheduled by an adversary. Although we show that computing the exact optimum is NP-hard for general task graphs, in the restricted case of fork-join (series-parallel) computations with \ud835\udc5b strands and a single lock we offer a \u0398(\ud835\udc5b\u00b2) exact algorithm as well as a \u0398(\ud835\udc5b) 2-approximation for worst case contention. In proving these bounds linking maximum contention to the antichain sizes of a program\u2019s parallel trace, we establish graph-based properties of worst case execution schedules that also apply directly to the related single processor scheduling problem of average response time under task precedence constraints. In addition, our analysis of worst case contention offers improved estimates for the completion time of locking computations under the execution trace model.", "year": 2022, "venue": "", "authors": "Wanlin Li"}, {"title": "Learning-based Scheduling", "abstract": "Integer programs provide a powerful abstraction for representing a wide range of real-world scheduling problems. Despite their ability to model general scheduling problems, solving large-scale integer programs (IP) remains a computational challenge in practice. The incorporation of more complex objectives such as robustness to disruptions further exacerbates the computational challenge. With the advent of deep learning in solving various hard problems, this thesis aims to tackle different computationally intensive aspects of scheduling with learning-based methods. First, we apply reinforcement learning (RL) to the Air Force crew-scheduling problem and compare it against IP formulations which explicitly optimize for minimization of overqualification and maximization of training requirements completed. We show that the RL agent is equally effective as its IP counterpart when the reward function is engineered according to the objective we want to optimize. We also show that the RL formulation is able to optimize for multiple objectives with simple modifications to the reward structure, whereas the IP methods require separate formulations for their objective functions. Then we present Neural network IP Coefficient Extraction (NICE), a novel technique that combines reinforcement learning and integer programming to tackle the problem of robust scheduling. More specifically, NICE uses reinforcement learning to approximately represent complex objectives in an integer programming formulation. We use NICE to determine assignments of pilots to a flight crew schedule so as to reduce the impact of disruptions. We compare NICE with (1) a baseline \u2026", "year": 2022, "venue": "", "authors": "Siddharth Nagar Nayak"}, {"title": "Preliminary Investigation of Productivity Tools for Memory Profiling in Parallel Programs", "abstract": "As computing efficiency becomes constrained by hardware scaling limitations, code optimization grows increasingly important as an area of research. The impact of certain optimizations depends on whether a program is compute-bound or memory-bound. Memory-bound computations especially benefit from program transformations that improve their data locality, to better exploit modern memory hierarchies. Reuse distance is a useful measure for analyzing data locality in an architecture-agnostic way, i.e., independent of specific cache sizes. Previous work has researched different ways to calculate reuse distance, ranging from deterministic to probabilistic and using different definitions of reuse distance.  This thesis investigates the use of static compiler instrumentation tools to implement memory analysis tools for parallel programs. I show how the comprehensive static instrumentation (CSI) framework can be used to compute the reuse-distance of memory locations in a sequential execution of a program. For analyzing parallel programs, it is necessary to contextualize the memory access patterns with the logical parallel structure of the code. To this end, I show how reuse distance calculations can be organized according to the logical parallel structure of the program by building a series-parallel tree using CSI. I present several potential algorithms for using this instrumentation to calculate statistics for average and peak memory bandwidth in parallel codes. Although these instrumentation tools remain prototypes, they constitute a compelling proof-of-concept for the use of CSI to perform memory analysis in parallel codes.", "year": 2022, "venue": "", "authors": "Elizabeth Zou"}, {"title": "Learned String Index Structures for In-Memory Databases", "abstract": "Within the field of machine learning for systems, learning-based methods have brought new perspective to indexing by reframing it as a cumulative distribution function (CDF) modeling problem. The burgeoning field, despite its nascence, has brought with it many opportunities and efficiencies. However, most work in this area has focused on efficiently indexing numerical keys, as the additional challenges posed by indexing strings have prevented the effective application of these techniques to string domains. We hypothesize that the machine learning approaches which have, in recent years, made significant strides in scalar indexing applications can also be effectively adapted to string applications. First, we introduce the RadixStringSpline (RSS) learned index structure for efficiently indexing strings. RSS is a tree of learned radix splines each indexing a fixed number of bytes. RSS achieves better performance than other structures by first using the minimal string prefix to sufficiently distinguish the data, followed by a contextual learned model to predict its location. Additionally, the bounded-error nature of RSS accelerates the last mile search and also enables a memory-efficient hash-table lookup accelerator. Second, we benchmark RSS against existing algorithms on several real-world string datasets and study its performance in-depth. RSS approaches or exceeds the performance of traditional string indexes while using up to 300\u00d7 less memory, suggesting this line of research may be promising for future memory-intensive database applications.", "year": 2022, "venue": "", "authors": "Benjamin Spector"}, {"title": "Combining Task Parallelism and Multithreaded Concurrency", "abstract": "In this thesis, I present Multicilk, a threads library based on C11 threads and the OpenCilk runtime for enabling task parallelism within multiple concurrent threads. With Multicilk, a programmer can parallelize threads in a multithreaded application simply by using Cilk independently within each thread. Without Multicilk, doing so violates the semantics that we expect in concurrent thread programming, leading to catastrophic failure of the application. No other existing combination of task-parallel system \u2014 including OpenMP, TBB, and TPL, and the various other implementations of Cilk \u2014 and threads library \u2014 including C11, C++11, Pthreads, and WinAPI threads \u2014 can parallelize multithreaded applications transparently and modularly.  The key insight behind Multicilk recognizes that integrating task-parallel systems with multithreaded concurrency requires two layers of thread abstraction that are conflated in previous systems. Service threads implement the workers in the Cilk runtime system. But the Cilk computation itself, called a cilk, though implemented by many service threads, itself provides the abstraction of a single application thread to other threads within the multithreaded application, regardless of whether they are cilks. Multicilk employs a technique called impersonation to individual workers to act on behalf of the entire cilk, providing the same interface to the outside world as it would it were an ordinary thread. This powerful \u201ctwo-layer-cake\u201d abstraction enables ordinary multithreaded applications to be ported to a Cilk environment and parallelized in a straightforward and modular fashion.  My Multicilk implementation for OpenCilk provides \u2026", "year": 2022, "venue": "", "authors": "Sai Sameer Pusapaty"}, {"title": "Fast Reducer Hyperobjects", "abstract": "This thesis investigates the performance of reducer hyperobjects, a feature of the Cilk task-parallel runtime system that enables concurrent associative updates to nonlocal variables.  Reducers are more performant than more traditional methods of enabling concurrent updates, such as locking and atomic updates.  Unfortunately, existing reducer implementations can suffer a cost of up to 10 times that of a serial update, depending on the benchmark.    This overhead incurred by reducers can be decreased by three approaches: runtime data structures, compiler-runtime integration, and compiler optimization.  When these approaches are used to performance-engineer the OpenCilk runtime system's reducers, the overall performance of a benchmark suite designed to stress-test reducers sees a geometric average improvement of 25.74% and a maximum improvement of 88.32%.  This thesis also investigates ``commutative'' reducers, a proposed reducer optimization premised on restricted semantics, but finds they yield a performance degredation while being linguistically unwieldy.  The examination of commutative reducers leads to an empirical investigation of the scalability of traditional reducers, finding a quadratic upper bound on their performance to be loose.  Parts of this thesis represent joint work with Charles E. Leiserson, Tim Kaler, William S. Moses, Qi Qi, and Tao B. Schardl of MIT and I-Ting Angelina Lee of Washington University in St .Louis.", "year": 2022, "venue": "", "authors": "Matthew Kilgore"}, {"title": "Understanding frank-wolfe adversarial training", "abstract": "", "year": 2022, "venue": "", "authors": "T Tsiligkaridis and J Roberts"}, {"title": "The Pseudo Projection Operator: Applications of Deep Learning to Projection Based Filtering in Non-Trivial Frequency Regimes", "abstract": "Traditional frequency based projection filters, or projection operators (PO), separate signal and noise through a series of transformations which remove frequencies where noise is present. However, this technique relies on a priori knowledge of what frequencies contain signal and noise and that these frequencies do not overlap, which is difficult to achieve in practice. To address these issues, we introduce a PO-neural network hybrid model, the Pseudo Projection Operator (PPO), which leverages a neural network to perform frequency selection. We compare the filtering capabilities of a PPO, PO, and denoising autoencoder (DAE) on the University of Rochester Multi-Modal Music Performance Dataset with a variety of added noise types. In the majority of experiments, the PPO outperforms both the PO and DAE. Based upon these results, we suggest future application of the PPO to filtering problems in the physical and biological sciences.", "year": 2021, "venue": "", "authors": "Matthew L Weiss and Nathan C Frey and Siddharth Samsi and Randy C Paffenroth and Vijay Gadepally"}, {"title": "Equivariant self-supervised learning: Encouraging equivariance in representations", "abstract": "", "year": 2021, "venue": "", "authors": "Rumen Dangovski and Li Jing and Charlotte Loh and Seungwook Han and Akash Srivastava and Brian Cheung and Pulkit Agrawal and Marin Soljacic"}, {"title": "Supercomputing Enabled Deployable Analytics for Disaster Response", "abstract": "First responders and other forward deployed essential workers can benefit from advanced analytics. Limited network access and software security requirements prevent the usage of standard cloud based microservice analytic platforms that are typically used in industry. One solution is to precompute a wide range of analytics as files that can be used with standard preinstalled software that does not require network access or additional software and can run on a wide range of legacy hardware. In response to the COVID-19 pandemic, this approach was tested for providing geo-spatial census data to allow quick analysis of demographic data for better responding to emergencies. These data were processed using the MIT SuperCloud to create several thousand Google Earth and Microsoft Excel files representative of many advanced analytics. The fast mapping of census data using Google Earth and Microsoft Excel \u2026", "year": 2021, "venue": "", "authors": "Kaira Samuel and Jeremy Kepner and Michael Jones and Lauren Milechin and Vijay Gadepally and William Arcand and David Bestor and William Bergeron and Chansup Byun and Matthew Hubbell and Michael Houle and Anna Klein and Victor Lopez and Julie Mullen and Andrew Prout and Albert Reuther and Antonio Rosa and Sid Samsi and Charles Yee and Peter Michaleas"}, {"title": "Air Force Pilots Get an AI-Assist With Scheduling Aircrews", "abstract": "Scheduling C-17 aircraft crews is complicated. It\u2019s a pain point for Airmen of 52 squadrons who operate C-17s, the military cargo aircraft that transport troops and supplies globally. This year, the Air Force marked four million flight hours for its C-17 fleet, which comprises 275 U.S. and allied aircraft. Each flight requires scheduling a crew of six on average.  A team spanning MIT Lincoln Laboratory, the Department of the Air Force and the MIT Department of Aeronautics and Astronautics collaborated with their Air Force sponsor organization to develop an AI\u2013enabled plugin for the existing C-17 scheduling tool that automates C-17 aircrew scheduling and optimizes crew resources.", "year": 2021, "venue": "", "authors": "Kylie Foy"}, {"title": "Brief Announcement: Efficient Access History for Race Detection", "abstract": "While there has been extensive research on race-detection algorithms for task-parallel programs, most of this research has focused on optimizing a particular component, namely, reachability analysis, which checks whether two instructions are logically in parallel. Little attention has been paid to the other important component, the access history, which stores all memory locations previous instructions have accessed. In theory, the access-history component adds no asymptotic overhead; however, in practice, it is often the most expensive component of race detection since it is queried and (possibly) updated at each memory access. We optimize this component based on the observation that, typically, strands within parallel programs access contiguous blocks of memory. Therefore, instead of maintaining the access history at the granularity of individual memory locations, we maintain it at the granularity of these (varying size) intervals. To enable this access history, we propose (1) compiler and runtime mechanisms that allow us to efficiently collect these intervals and (2) a tree-based access-history data structure that allows updates and queries at interval granularity. The resulting tool can race-detect fork-join code with amortized constant overhead, assuming the number of intervals is small compared to the total work of the computation.", "year": 2021, "venue": "", "authors": "Yifan Xu and Anchengcheng Zhou and Grace Yin and Kunal Agrawal and I Lee and Tao Schardl"}, {"title": "Closed-form continuous-time neural models", "abstract": "", "year": 2021, "venue": "", "authors": "Ramin Hasani and Mathias Lechner and Alexander Amini and Lucas Liebenwein and Aaron Ray and Max Tschaikowski and Gerald Teschl and Daniela Rus"}, {"title": "Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients!", "abstract": "Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast \nGradients! Page 1 wmoses@mit.edu June 23, 2021 William S. Moses Valentin Churavy 1 Instead \nof Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients! Page \n2 William S. Moses Valentin Churavy 2 Ludger Paehler Johannes Doerfert Jan H\u00fcckelheim Sri \nHari Krishna Narayanan Michel Schanen Paul Hovland Page 3 Differentiation Is Key To Machine \nLearning And Science \u2022 Computing derivatives is key to many algorithms \u2022 Machine learning \n(back-propagation, Bayesian inference, uncertainty quantification) \u2022 Scientific computing \n(modeling, simulation) \u2022 When working with large codebases or dynamically-generated \nprograms, manually writing derivative functions becomes intractable \u2022 Community has \ndeveloped tools to create derivatives automatically 3 Page 4 Existing AD Approaches \u2022 \u2026", "year": 2021, "venue": "", "authors": "William S Moses Valentin Churavy"}, {"title": "Technical Report on Data Integration and Preparation", "abstract": "", "year": 2021, "venue": "", "authors": "Michael Cafarella El Kindi Rezig and Vijay Gadepally"}, {"title": "DICE: Data Discovery by Example.", "abstract": "In order to conduct analytical tasks, data scientists often need to find relevant data from an avalanche of sources (eg, data lakes, large organizational databases). This effort is typically made in an ad hoc, non-systematic manner, which makes it a daunting endeavour. Current data discovery systems typically require the users to find relevant tables manually, usually by issuing multiple queries (eg, using SQL). However, expressing such queries is nontrivial, as it requires knowledge of the underlying structure (schema) of the data organization in advance. This issue is further exacerbated when data resides in data lakes, where there is no predefined schema that data must conform to. On the other hand, data scientists can often come up with a few example records of interest quickly. Motivated by this observation, we developed DICE\u2014a human-in-the-loop system for Data dIsCovery by Example\u2014that takes user-provided example records as input and returns more records that satisfy the user intent. DICE\u2019s key idea is to synthesize a SQL query that captures the user intent, specified via examples. To this end, DICE follows a three-step process:(1) DICE first discovers a few candidate queries by finding join paths across tables within the data lake.(2) Then DICE consults with the user for validation by presenting a few records to them, and, thus, eliminating spurious queries.(3) Based on the user feedback, DICE refines the search and repeats the process until the user is satisfied with the results. We will demonstrate how DICE can help in data discovery through an interactive, example-based interaction.", "year": 2021, "venue": "", "authors": "El Kindi Rezig and Anshul Bhandari and Anna Fariha and Benjamin Price and Allan Vanterpool and Vijay Gadepally and Michael Stonebraker"}, {"title": "A Machine Learning Approach for Understanding and Discovering Topological Materials", "abstract": "Topological materials are of significant interest for both basic science and next-generation technological applications due to their unconventional electronic properties. The majority of currently-known topological materials have been discovered using methods that involve symmetry-based analysis of the quantum mechanical wavefunction. Here we use machine learning to develop a heuristic chemical rule, which diagnoses whether a material is topological using only its chemical formula. It is based on a notion that we term topogivity, which is a learned numerical value for each element that loosely captures the tendency of an element to form topological materials. Topogivities provide chemical insights for understanding topological materials. We implement a high-throughput procedure for discovering topological materials that are not diagnosable by symmetry indicators. The procedure is based on heuristic rule prediction followed by ab initio validation. The concept of topogivity represents a fundamentally new approach to the study of topological materials, and opens up new directions of research at the intersection of chemistry, machine learning, and band topology.", "year": 2021, "venue": "", "authors": "Andrew Ma"}, {"title": "Performance Engineering of Reactive Molecular Dynamics Simulations", "abstract": "Reactive molecular dynamics is the best-performing option for simulating chemical systems on the order of thousands of atoms, but its high computational cost often limits the temporal scale of simulation. In order to observe scientific phenomena of interest, we need implementations of interatomic potentials which are highly efficient and scalable on modern architectures. Parallel computing is now ubiquitous, and today\u2019s supercomputing clusters often consist of multicore nodes with high on-node parallelism. Current implementations of ReaxFF display good scaling across many distributed nodes, but fall short in taking full advantage of compute available on an individual CPU or GPU.   This thesis presents analysis and performance optimization of the widely used LAMMPS ReaxFF implementations. I analyze the performance characteristics of the USER-REAXC, USER-OMP, and Kokkos implementations in LAMMPS, profiling and describing bottlenecks in each. I then provide optimizations to serial and parallel CPU code which increase the efficiency and parallel thread scaling of USER-OMP. Using an Intel Xeon Platinum 8260, the resulting code obtains a speedup of 1.5-3x and shows scaling with twice as many OpenMP threads on a 1152-atom Hafnium Diboride simulation. I show performance improvements on various simulation sizes up to 44K atoms, and present independently verified correctness on an AMD Ryzen Threadripper 3970X.", "year": 2021, "venue": "", "authors": "Helen He"}, {"title": "Composing Parallel Runtime Systems: A Case Study in How to Compose the Julia and OpenCilk Runtimes", "abstract": "Julia [5] [15] is a high-level computing language used by many developers for its performance and ease of use. Julia operates on tasks that are run concurrently on threads. In its current state, however, Julia is not able to effectively employ fine-grained parallelism. OpenCilk [9] is an open-source implementation of the Cilk concurrency platform designed to utilize fine-grain parallelism. The Cilk runtime system, based on Cheetah [12], offers provably efficient parallel scheduling whose performance is borne out in theory and practice. I propose a combination of the Julia and OpenCilk runtimes through the integration of multiple components. One contribution of this thesis is a novel algorithm for combining C/C++ memory allocations with Julia\u2019s precise garbage collector. Composing the parallelism of OpenCilk and Julia enables programmers to write efficient multithreaded code. Additionally, this work is a case study of combining the high levels of parallelism present in Cilk with a high-level language.", "year": 2021, "venue": "", "authors": "Tim Kralj"}, {"title": "Meta-Learning and Self-Supervised Pretraining for Few-shot Image Translation", "abstract": "Recent advances in machine learning (ML) and deep learning in particular, enabled by hardware advances and big data, have provided impressive results across a wide range of computational problems such as computer vision, natural language, or reinforcement learning. Many of these improvements are however constrained to problems with large-scale curated data-sets which require a lot of human labor to gather. Additionally, these models tend to generalize poorly under both slight distributional shifts and low-data regimes. In recent years, emerging fields such as meta-learning or self-supervised learning have been closing the gap between proof-of-concept results and real-life applications of ML.  We follow this line of work and contribute a novel few-shot multi-task image to image translation problem. We then present several benchmarks for this problem using ideas from both meta-learning and contrastive-learning and improve upon baselines trained using simple supervised learning. Additionally, we contribute to another area of growing interest\u2014applying deep learning to physical problems\u2014and focus our efforts on modeling weather phenomena.  We define an image translation problem between different radar and satellite sensor modalities and leverage spatial and temporal locality to pose it as a multi-task problem. We improve upon naive solutions that ignore this hierarchical dataset structure and demonstrate the effectiveness of meta-learning methods to solving real-world problems. We make our code available here.", "year": 2021, "venue": "", "authors": "Ileana Rugina"}, {"title": "Provably and Practically Efficient Race Detection for Task-Parallel Code", "abstract": "Parallel systems are pervasive nowadays. Specifically, modern computers have embraced multicore architectures due to the difficulties of exploiting higher clock speeds on single-core CPUs. However, parallel programming is challenging. Determinacy race, in particular, is a common pitfall when writing task-parallel code. It can easily lead to non-deterministic behavior of the parallel program and therefore a determinacy race is often considered as a bug. Unfortunately, such bugs are hard to debug because they do not necessarily produce obvious failures in every single execution.", "year": 2021, "venue": "", "authors": "Yifan Xu"}, {"title": "PLEX: towards practical learned indexing", "abstract": "", "year": 2021, "venue": "", "authors": "Mihail Stoian and Andreas Kipf and Ryan Marcus and Tim Kraska"}, {"title": "Prashant Pandey", "abstract": "Many real-world sparse graphs, such as social networks or road networks, change over time. Therefore, systems for storing and processing dynamic (ie streaming) graphs [17, 28, 32, 34, 38, 47, 55] have been designed to process a stream of updates (eg, edge weight update, or edge insertions and deletions) and a stream of queries quickly. That is, both query processing time and the time that it takes for updates to appear in the graph representation must be fast. The ability to quickly apply a batch of updates is critical for efficient streaming graph processing. For example, in incremental triangle counting, insertion (or deletion) time accounted between 25%\u201390% of the overall time [56]. Similarly, on 32 cores, updating the graph takes up to 90% of the overall running time in incremental connected components [58]. In this paper, we focus on data structure design for dynamic graph processing for both efficient updates and queries.In practice, dynamic real-world graphs follow skewed vertex degree distributions as shown in Table 1. For example, real-world graphs, such as those from social networks [33, 62] or computational biology, contain a few very high-degree vertices and many low-degree vertices. This skewness presents unique challenges for efficiently representing dynamic graphs. However, these diverse distributions also present an opportunity to build cache-efficient graph representations via adaptive data structures that take advantage of degree distributions. Existing static graph-processing systems that optimize for skewness demonstrate the potential for improved cache locality. For example, PowerLyra [21] partitions vertices based on \u2026", "year": 2021, "venue": "", "authors": "Helen Xu and Aydin Buluc"}, {"title": "Video action understanding: A tutorial", "abstract": "", "year": 2020, "venue": "", "authors": "Matthew Hutchinson and Vijay Gadepally"}, {"title": "Cache-Efficient Parallel-Partition Algorithms using Exclusive-Read-and-Write Memory", "abstract": "We present an in-place algorithm for the parallel-partition problem with linear work and polylogarithmic span. The algorithm uses only exclusive read/write shared variables and can be implemented using parallel-for-loops without any additional concurrency considerations (ie, the algorithm is EREW). A key feature of the algorithm is that it exhibits provably optimal cache behavior up to small-order factors.We also present a second in-place EREW algorithm with work O (n) and span O (logn\u00b7 log logn), which is within an O (log logn) factor of the optimal span. By using this low-span algorithm as a subroutine within the cache-friendly algorithm, we obtain a single EREW algorithm that combines their theoretical guarantees: the algorithm achieves span O (logn\u00b7 log logn) and exhibits optimal cache behavior. As an immediate consequence, we also get an inplace EREW Quicksort algorithm with work O (n logn) and span O (log2 n\u00b7 log logn).", "year": 2020, "venue": "", "authors": "Alek Westover and William Kuszmaul"}, {"title": "Multicore Paging Algorithms Cannot Be Competitive", "abstract": "Every processor with multiple cores sharing a cache needs to implement a page-replacement algorithm. L\u00f3pez-Ortiz and Salinger [5] demonstrated that competitive ratio of canonical paging algorithms such as Least-Recently-Used (LRU) and Furthest-In-Future (FIF) grows with the length of the input. In this paper, we answer an open question about the existence of competitive multicore paging algorithms in the negative. Specifically, we show that all lazy algorithms, which include all practical algorithms, cannot be competitive against the optimal offline algorithm.", "year": 2020, "venue": "", "authors": "Helen Xu and Shahin Kamali"}, {"title": "Technical Report: An Overview of Data Integration and Preparation", "abstract": "AI application developers typically begin with a dataset of interest and a vision of the end analytic or insight they wish to gain from the data at hand. Although these are two very important components of an AI workflow, one often spends the first few weeks (sometimes months) in the phase we refer to as data conditioning. This step typically includes tasks such as figuring out how to prepare data for analytics, dealing with inconsistencies in the dataset, and determining which algorithm (or set of algorithms) will be best suited for the application. Larger, faster, and messier datasets such as those from Internet of Things sensors, medical devices or autonomous vehicles only amplify these issues. These challenges, often referred to as the three Vs (volume, velocity, variety) of Big Data, require low-level tools for data management, preparation and integration. In most applications, data can come from structured and/or unstructured sources and often includes inconsistencies, formatting differences, and a lack of ground-truth labels.In this report, we highlight a number of tools that can be used to simplify data integration and preparation steps. Specifically, we focus on data integration tools and techniques, a deep dive into an exemplar data integration tool, and a deep-dive in the evolving field of knowledge graphs. Finally, we provide readers with a list of practical steps and considerations that they can use to simplify the data integration challenge. The goal of this report is to provide readers with a view of state-of-the-art as well as practical tips that can be used by data creators that make data integration more seamless.", "year": 2020, "venue": "", "authors": "ElKindi Rezig and Mike Cafarella and Vijay Gadepally"}, {"title": "In-Place Parallel-Partition Algorithms using Exclusive-Read-and-Write Memory", "abstract": "We present an in-place algorithm for the partition problem that has linear work and polylogarithmic span. The algorithm uses only exclusive read/write shared variables, and can be implemented using parallel-for-loops without any additional concurrency considerations (i.e., the algorithm is EREW). A key feature of the algorithm is that it exhibits provably optimal cache behavior, up to small-order factors. We also present a second in-place EREW algorithm for the partition problem that has linear work and span , which is within an  factor of the optimal span. By using this low-span algorithm as a subroutine within the cache-friendly algorithm, we are able to obtain a single EREW algorithm that combines their theoretical guarantees: the algorithm achieves span  and optimal cache behavior. As an immediate consequence, we also get an in-place EREW quicksort algorithm with work , span . Whereas the standard EREW algorithm for parallel partitioning is memory-bandwidth bound on large numbers of cores, our cache-friendly algorithm is able to achieve near-ideal scaling in practice by avoiding the memory-bandwidth bottleneck. The algorithm's performance is comparable to that of the Blocked Strided Algorithm of Francis, Pannan, Frias, and Petit, which is the previous state-of-the art for parallel EREW sorting algorithms, but which lacks theoretical guarantees on its span and cache behavior.", "year": 2020, "venue": "", "authors": "William Kuszmaul and Alek Westover"}, {"title": "10th International Conference on Fun with Algorithms (FUN 2021)", "abstract": "Portal - FUN 2021 FUN 2021 May 30 to June 1, 2021, Favignana Island, Sicily, Italy 10th \nInternational Conference on Fun with Algorithms (FUN 2021) Martin Farach-Colton and \nGiuseppe Prencipe and Ryuhei Uehara (Eds.) ISBN 978-3-95977-145-0, LIPICS Vol. 157 ISSN \n1868-8969 Additional Information License Conference Website Complete volume (PDF, 16 MB) \nSearch Publication Server Authors Adler, Aviv Anagnostopoulos, Aris Ani, Joshua Barbay, \nJ\u00e9r\u00e9my Besa, Juan Jose Biderman, Stella Bil\u00f2, Davide Bosboom, Jeffrey Bramas, Quentin \nBrocken, Thomas Brunner, Josh Calvert, Aiden Chung, Lily Churchill, Alex Clokie, Trevor \nCordasco, Gennaro Crombez, Lo\u00efc da Fonseca, Guilherme D. Demaine, Erik D. Demaine, Martin \nL. Dempsey, Ross Devismes, St\u00e9phane Diomidov, Yenhenii Eppstein, David Farach-Colton, \nMartin Frei, Fabian Frishberg, Daniel Gargano, Luisa Gerard, Yan Gionis, Aristides Gual\u00e0, \u2026", "year": 2020, "venue": "", "authors": "Aviv Adler and Jeffrey Bosboom and Erik D Demaine and Martin L Demaine and Quanquan C Liu and Jayson Lynch and Aris Anagnostopoulos and Aristides Gionis and Nikos Parotsidis and Joshua Ani and Yenhenii Diomidov and Dylan Hendrickson and Juan Jose Besa and Timothy Johnson and Nil Mamano and Martha C Osegueda and Davide Bil\u00f2 and Luciano Gual\u00e0 and Stefano Leucci and Guido Proietti and Giacomo Scornavacca and Quentin Bramas and Pascal Lafourcade and St\u00e9phane Devismes and Josh Brunner and Lily Chung and Adam Hesterberg and Adam Suhl and Avi Zeff and Julian Wellman and Alex Churchill and Stella Biderman and Austin Herrick and Trevor Clokie and Thomas F Lidbetter and Antonio J Molina Lovett and Jeffrey Shallit and Leon Witzman and Lo\u00efc Crombez and Guilherme D da Fonseca and Yan Gerard and Ross Dempsey and Charles Guinn and David Eppstein and Daniel Frishberg and William Maxwell and Fabian Frei and Peter Rossmanith and David Wehner and Thomas Brocken and G Wessel van der Heijden and Irina Kostitsyna and Lloyd E Lo-Wong and Remco JA Surtel and Tomasz Idziaszek and Alexander Koch and Stefan Walzer and Eryk Kopczy\u0144ski and William Kuszmaul and Daiki Miyahara and L\u00e9o Robert and So Takeshige and Takaaki Mizuki and Kazumasa Shinagawa and Atsuki Nagao and Hideaki Sone and Gennaro Cordasco and Luisa Gargano and Adele A Rescigno and Suthee Ruangwises and Toshiya Itoh and J\u00e9r\u00e9my Barbay and Bernardo Subercaseaux and Qian M Zhou and Aiden Calvert and Maxwell Young"}, {"title": "Algorithms and Lower Bounds for the Worker-Task Assignment Problem.", "abstract": "", "year": 2020, "venue": "", "authors": "Aaron Berger and William Kuszmaul and Adam Polak and Jonathan Tidor and Nicole Wein"}, {"title": "AIKIDO: toward straggler mitigation for distributed machine learning training in cloud data centers", "abstract": "As artificial intelligence becomes a critical component of everyday life, the popularity of using cloud data centers for training deep neural networks is relentlessly growing. This poses a significant challenge for data center operators where the network band-width is shared among multiple ML jobs as well as between ML jobs and data center flows. At high loads, the network experiences transient congestion events frequently which in turn delays the parameter updates between ML workers. Consequently, the training convergence suffers as some workers behind congested links straggle to update the model parameters in time, hence delaying all workers. We propose AIKIDO as a first step towards mitigating the impact of transient network-induced stragglers on training workloads caused by the dynamic nature of the data center traffic. AIKIDO exploits the inherent robustness of ML training on occasional loss of gradient updates and implements a Skip-Straggler communication strategy where the updates from straggling workers are simply skipped. In addition, AIKIDO introduces an Active-Backup strategy as an improvement to the Skip method to maintain a high accuracy convergence while using fewer resources than full worker replication. In our experiment, we use Google Cloud Engine environment to train ResNet-50 on ImageNet at various scales and demonstrate that AIKIDO is able to mitigate the effect of stragglers and achieve the time-to-accuracy as if there are no stragglers.", "year": 2020, "venue": "", "authors": "Ayush Sharma"}, {"title": "Towards Data Discovery by Example", "abstract": "", "year": 2020, "venue": "", "authors": "Allan Vanterpool El Kindi Rezig and Vijay Gadepally and Benjamin Price and Michael J Cafarella and Michael Stonebraker"}, {"title": "Train Tracks with Gaps", "abstract": "We identify a tradeoff curve between the number of wheels on a train car, and the amount of track that must be installed in order to ensure that the train car is supported by the track at all times. The goal is to build an elevated track that covers some large distance \ud835\udcc1, but that consists primarily of gaps, so that the total amount of feet of train track that is actually installed is only a small fraction of \ud835\udcc1. In order so that the train track can support the train at all points, the requirement is that as the train drives across the track, at least one set of wheels from the rear quarter and at least one set of wheels from the front quarter of the train must be touching the track at all times.", "year": 2020, "venue": "", "authors": "William Kuszmaul"}, {"title": "35th International Symposium on Computational Geometry (SoCG 2019)", "abstract": "Portal - SoCG 2019 SoCG 2019 June 18-21, 2019, Portland, Oregon, USA 35th \nInternational Symposium on Computational Geometry (SoCG 2019) Gill Barequet and Yusu \nWang (Eds.) ISBN 978-3-95977-104-7, LIPICS Vol. 129 ISSN 1868-8969 Additional \nInformation License Conference Website Complete volume (PDF, 44 MB) Search \nPublication Server Authors Afshani, Peyman Agarwal, Pankaj K. Agrawal, Akanksha Aiger, \nDror Akitaya, Hugo A. Anai, Hirokazu Angelini, Patrizio Aronov, Boris Attali, Dominique \nBarba, Luis Barequet, Gill Bauer, Ulrich Becker, Aaron T. Ben-Shachar, Gil Binucci, Carla \nBoissonnat, Jean-Daniel Bokal, Drago Braverman, Vladimir Bringmann, Karl Buchin, Kevin \nCabello, Sergio Carri\u00e8re, Mathieu Chan, Timothy M. Chang, Hsien-Chih Chaplick, Steven \nCharikar, Moses Chaudhury, Bhaskar Ray Chazal, Fr\u00e9d\u00e9ric Chiang, Yi-Jen Cohen, Ravid \nCohen-Addad, Vincent Colin de Verdi\u00e8re, \u00c9ric \u2026", "year": 2019, "venue": "", "authors": "Sanjoy Dasgupta and Bruce R Donald and Peyman Afshani and Jeff M Phillips and Pankaj K Agarwal and Boris Aronov and Esther Ezra and Joshua Zahl and Hsien-Chih Chang and Allen Xiao and Akanksha Agrawal and Grzegorz Guspiel and Jayakrishnan Madathil and Saket Saurabh and Meirav Zehavi and Dror Aiger and Haim Kaplan and Efi Kokiopoulou and Micha Sharir and Bernhard Zeisl and Hugo A Akitaya and Matias Korman and Mikhail Rudoy and Diane L Souvaine and Csaba D T\u00f3th and Patrizio Angelini and Steven Chaplick and Sabine Cornelsen and Giordano Da Lozzo and Vincenzo Roselli and Dominique Attali and Andr\u00e9 Lieutier and David Salinas and Luis Barba and Carla Binucci and Emilio Di Giacomo and Walter Didimo and Tamara Mchedlidze and Maurizio Patrignani and Drago Bokal and Zdenek Dvor\u00e1k and Petr Hlinen\u00fd and Jes\u00fas Lea\u00f1os and Bojan Mohar and Tilo Wiedera and Andrey Boris Khesin and Aleksandar Nikolov and Dmitry Paramonov and Vladimir Braverman and Moses Charikar and William Kuszmaul and David P Woodruff and Lin F Yang and Karl Bringmann and Marvin K\u00fcnnemann and Andr\u00e9 Nusser and Bhaskar Ray Chaudhury and Kevin Buchin and Sariel Har-Peled and D\u00e1niel Ol\u00e1h and Sergio Cabello and Timothy M Chan and Mathieu Carri\u00e8re and Ulrich Bauer and Jean-Lou De Carufel and Adrian Dumitrescu and Wouter Meulemans and Tim Ophelders and Claire Pennarun and Sander Verdonschot and Marcos Cossarini and Jeff Erickson and Ravid Cohen and Dan Halperin and Wolfgang Mulzer and Vincent Cohen-Addad and \u00c9ric Colin de Verdi\u00e8re and D\u00e1niel Marx and Arnaud de Mesmay and Anne Driemel and Ioannis Psarros and Vida Dujmovic and Pat Morin and Herbert Edelsbrunner and Ziga Virk and Hubert Wagner and David Eppstein and Yipu Wang and S\u00e1ndor P Fekete and Phillip Keldenich and Christian Scheffer and Jacob Fox and J\u00e1nos Pach and Andrew Suk and Ulderico Fugacci and Michael Kerber and Radoslav Fulek and Bernd G\u00e4rtner"}, {"title": "Signed enumeration of upper-right corners in path shuffles", "abstract": "We resolve a conjecture of Albert and Bousquet-M\u00e9lou enumerating quarter-planar walks with fixed horizontal and vertical projections according to their upper-right-corner count modulo 2. In doing this, we introduce a signed upper-right-corner count statistic. We find its distribution over planar walks with any choice of fixed horizontal and vertical projections. Additionally, we prove that the polynomial counting loops with a fixed horizontal and vertical projection according to the absolute value of their signed upper-right-corner count is (x+ 1)-positive. Finally, we conjecture an equivalence between (x+ 1)-positivity of the generating function for upper-right-corner count and signed upper-right-corner count, leading to a reformulation of a conjecture of Albert and Bousquet-M\u00e9lou on which their asymptotic analysis of permutations is sortable by two stacks in parallel relies.", "year": 2017, "venue": "", "authors": "William Kuszmaul"}, {"title": "Efficient GPU-Accelerated Global Optimization for Inverse Problems", "abstract": "This paper introduces a novel hybrid multi-start optimization strategy for solving inverse problems involving nonlinear dynamical systems and machine learning architectures, accelerated by GPU computing on both NVIDIA and AMD GPUs. The method combines Particle Swarm Optimization (PSO) and the Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (L-BFGS) algorithms to address the challenges in parameter estimation for nonlinear dynamical systems. This hybrid strategy aims to leverage the global search capability of PSO and the efficient local convergence of L-BFGS. We experimentally show faster convergence by a factor of up to  in a few non-convex problems with loss landscapes characterized by multiple local minima, which can cause regular optimization approaches to fail.", "year": "", "venue": "", "authors": "Vaibhav Kumar Dixit and Julian Samaroo and Avik Pal and Alan Edelman and Christopher Vincent Rackauckas"}, {"title": "Seventh International Workshop on Exploiting Artificial Intelligence Techniques for Data Management", "abstract": "Quality scientific inquiries depend on access to data distributed over the entire globe. Linked open data (LOD) and FAIRness play major roles in ensuring access to data that scientists need to answer interesting questions. However, a data model and a query language to compute responses to complex scientific inquiries remain outstanding. As the recent emergence of large language models (LLM) reshape how we interact with machines, an intriguing prospect of posing scientific inquiries to smart machines suddenly appears realizable in which a natural language ChatBot is empowered with a LOD knowledgebase as its data source. In this paper, we introduce a model for an LLM interpreter, called ProAb, that aims to answer natural language scientific queries using a structured query language called Needle in which the LOD is viewed as a set of tables. We discuss the contours of ProAb, present its preliminary and \u2026", "year": "", "venue": "", "authors": "Oded Shmueli and Yael Amsterdamer and Donatella Firmani"}, {"title": "Quantifying Rhetoric Alignment using Node Embeddings on Temporal Graphs", "abstract": "In recent years, there has been a growing interest in leveraging node embeddings within graph structures to gain insights into various domains. While text embeddings have been extensively explored for natural language processing tasks, researchers have only recently begun to apply node embeddings in graphs to analyze their underlying structure. This paper utilizes vector representations of a network of Russian and Chinese diplomats\u2019 online activity to map the degree of similarity between the diplomats\u2019 discourse. This approach extends the application of temporal graph learning, which extracts knowledge from evolving networks, to the domain of political communication and international relations. The analysis provides a deeper understanding of diplomatic interactions within the evolving landscape of digital communication.", "year": "", "venue": "", "authors": "Keeley Erhardt and Alex Pentland"}, {"title": "Airline Disruption Management with Delay Ledgers", "abstract": "The impact of disruptions may result in reduced capacities at airports, forcing airlines to revise schedules and delay flights. However, due to myriad factors (eg, passengers who may miss their connections, remaining flights to be performed by an aircraft, high-valued passengers with elite statuses), a delayed flight may be more or less costly to an airline, even when compared to another similarly delayed flight. Currently, identifying optimal slot swaps between airlines requires sharing the airline-specific delay cost of each flight. However, this is not amenable as sharing these private delay costs could reveal sensitive business practices. We propose the use of a procedure called the Delay Ledger (DELED) which enables airlines to identify a set of beneficial slot swaps across a network of airports which guarantees improvements in terms of private delay costs while ensuring that no private flight-specific valuations are shared. DELED is guaranteed to lower airline delay costs, incentivizes truthful airline participation, and supports flexible airline privacy preferences. We evaluate DELED across 30 days with 8 major US airlines, resulting in average reductions in private delay costs of 8-22% per day compared to current approaches.", "year": "", "venue": "", "authors": "Christopher Chin and Hamsa Balakrishnan and Karthik Gopalakrishnan and Max Z Li"}, {"title": "Score-based Source Separation with Applications to Digital Communication Signals", "abstract": "We propose a new method for separating superimposed sources using diffusionbased generative models. Our method relies only on separately trained statistical priors of independent sources to establish a new objective function guided by maximum a posteriori estimation with an \u03b1-posterior, across multiple levels of Gaussian smoothing. Motivated by applications in radio-frequency (RF) systems, we are interested in sources with underlying discrete nature and the recovery of encoded bits from a signal of interest, as measured by the bit error rate (BER). Experimental results with RF mixtures demonstrate that our method results in a BER reduction of 95% over classical and existing learning-based methods. Our analysis demonstrates that our proposed method yields solutions that asymptotically approach the modes of an underlying discrete distribution. Furthermore, our method can be viewed as a multi-source extension to the recently proposed score distillation sampling scheme, shedding additional light on its use beyond conditional sampling. The code is available at https://github. com/tkj516/score_based_source_separation.", "year": "", "venue": "", "authors": "TejasJayashankar GaryC F Lee AlejandroLancho AmirWeiss and Yury Polyanskiy Gregory W Wornell"}, {"title": "Workshop Submission: Using Multimodal DNNs to Study Vision-Language Integration in the Brain", "abstract": "We leverage a large stereoelectroencephalography (SEEG) dataset consisting of neural recordings during movie viewing and a battery of unimodal and multimodal deep neural network models (SBERT, BEIT, SIMCLR, CLIP, SLIP) to identify candidate sites of multimodal integration in the human brain. Our data-driven method involves three steps: first, we parse the neural data into discrete, distinct event-structures, i.e., image-text pairs defined either by word onset times or visual scene cuts. We then use the activity generated by these event-structures in our candidate models to predict the activity generated in the brain. Finally, using contrasts between models with or without multimodal learning signals, we isolate those neural arrays driven more by multimodal representations than by unimodal representations. Using this method, we identify a sizable set of candidate neural sites that our model predictions suggest are shaped by multimodality (from 3\\%-29\\%, depending on increasingly conservative statistical inclusion criteria). We note a meaningful cluster of these multimodal electrodes in and around the temporoparietal junction, long theorized to be a hub of multimodal integration.", "year": "", "venue": "", "authors": "Vighnesh Subramaniam and Colin Conwell and Christopher Wang and Gabriel Kreiman and Boris Katz and Ignacio Cases and Andrei Barbu"}, {"title": "Redeeming intrinsic rewards via constrained policy optimization", "abstract": "", "year": "", "venue": "", "authors": "Eric R Chen and Zhang-Wei Hong and Joni Pajarinen and Pulkit Agrawal"}, {"title": "Deep Learning and Symbolic Regression for Discovering Parametric Equations", "abstract": "Symbolic regression is a machine learning technique that can learn the governing formulas from data and thus has the potential to transform scientific discovery. However, symbolic regression is still limited in the complexity of the systems that it can analyze. Deep learning on the other hand has transformed machine learning in its ability to analyze extremely complex and high-dimensional datasets. Here we develop a method that uses neural networks to extend symbolic regression to parametric systems where some coefficient may vary as a function of time but the underlying governing equation remains constant. We demonstrate our method on various analytic expressions and PDEs with varying coefficients and show that it extrapolate well outside of the training domain. The neural network-based architecture can also integrate with other deep learning architectures so that it can analyze high-dimensional data while being trained end-to-end in a single step. To this end we integrate our architecture with convolutional neural networks and train the system end-to-end to discover various physical quantities from 1D images of spring systems where the spring constant may vary.", "year": "", "venue": "", "authors": "Samuel Kim and Michael Zhang and Peter Y Lu and Marin Soljacic"}, {"title": "GlobalSensitivity. jl: Performant and Parallel Global", "abstract": "DRAFT using QuasiMonteCarlo N= 5000 lb=[1.0, 1.0, 1.0, 1.0] ub=[5.0, 5.0, 5.0, 5.0] sampler= SobolSample () A, B= QuasiMonteCarlo. generate_design_matrices (N, lb, ub, sampler) sobol_sens_desmat= gsa (f1, Sobol (), A, B) f_batch= function (p) prob_func (prob, i, repeat)= remake (prob; p= p [:, i]) ensemble_prob= EnsembleProblem (prob, prob_func= prob_func) sol= solve (ensemble_prob, Tsit5 (), EnsembleThreads (); saveat= t, trajectories= size (p, 2)) out= zeros (2, size (p, 2)) for i in 1: size (p, 2) out [1, i]= mean (sol [i][1,:]) out [2, i]= maximum (sol [i][2,:]) end return out end sobol_sens_batch= gsa (f_batch, Sobol (), A, B, batch= true)@ time gsa (f1, Sobol (), A, B)@ time gsa (f_batch, Sobol (), A, B, batch= true) As mentioned before, you can call the gsa function directly on the differential equation solution 54 and compute sensitivities across the timeseries. This is demonstrated in the example below, 55 the Sobol \u2026", "year": "", "venue": "", "authors": "Vaibhav Kumar Dixit and Christopher Rackauckas"}, {"title": "Examples are All You Need: Iterative Data Discovery by Example in Data Lakes", "abstract": "Data-science development is highly experimental and incremental, ie, it takes dozens of iterations to produce a satisfactory pipeline. At the core of any data-science pipeline, lies the data. As a result, building effective data-science pipelines often hinges on finding the right data to be consumed by the downstream operators. While the database community has successfully developed sophisticated query languages for data retrieval, those only work when data resides in a traditional relational database, conforming to a predefined schema. Even then, the users are required to master the query languages, have a good understanding of the schema, and, finally, they must know what they are looking for (eg, records) and how to find it (eg, which tables to join). However, in the era of data science, data scientists often need to extract data from data lakes, which typically lack meta-data (from which the schema can be inferred) and are too large to sift through manually. As a result, there is a renewed interest in designing novel interfaces to discover relevant data from data lakes effectively and efficiently. Yet, current systems pose a high entry barrier for the users, ie, users must have expertise in query languages and knowledge of the underlying structure of the lake. In the course of our collaboration with the US Air Force, we realize that there is a pressing need to develop a datadiscovery system that (1) liberates the users from the requirement of learning and composing complex queries, and, thus, makes data discovery easy for them;(2) returns effective discovery results that satisfy the user\u2019s intent; and (3) can return results efficiently. To this end, we introduce \u2026", "year": "", "venue": "", "authors": "Anshul Bhandari El Kindi Rezig and Anna Fariha and Benjamin Price and Allan Vanterpool and Andrew Bowne and Lindsey McEvoy and Vijay Gadepally"}, {"title": "Toward Modeling the Emergence of Symbolic Communication", "abstract": "We quantitatively study the emergence of symbolic communication in humans with a communication game that attempts to recapitulate an essential step in the development of human language: the emergence of shared abstract symbols in order to accomplish complex tasks. A teacher must communicate an abstract notion, a formula in first order logic rendered to them in natural language, to a student. Subjects do so through a narrow channel that deprives them of common shared symbols: they cannot see or speak to one another and must only communicate via the motions of cars in a computer game. We observe that subjects spontaneously develop a shared symbolic vocabulary of car motions for task-specific concepts, such as \u201csquare\u201d and \u201cforall\u201d, as well as for task-agnostic concepts such as \u201cI'm confused\u201d. Today, no agent can take part in this task, even though recognizing intentional motions and using those \u2026", "year": "", "venue": "", "authors": "Emily Cheng and Yen-Ling Kuo and Ignacio Cases and Boris Katz and Andrei Barbu"}, {"title": "When Are Learned Models Better Than Hash Functions?(Extended Abstracts)", "abstract": "In this work, we aim to study when learned models are better hash functions, particular for hash-maps. We use lightweight piece-wise linear models to replace the hash functions as they have small inference times and are sufficiently general to capture complex distributions. We analyze the learned models in terms of: the model inference time and the number of collisions. Surprisingly, we found that learned models are not much slower to compute than hash functions if optimized correctly. However, it turns out that learned models can only reduce the number of collisions (ie, the number of times different keys have the same hash value) if the model is able to over-fit to the data; otherwise, it can not be better than a typical hash function. Hence, how much better a learned model is in avoiding collisions highly depends on the data and the ability of the model to overfit. To evaluate the effectiveness of learned models, we used them as hash functions in the bucket chaining and Cuckoo hash tables. For bucket chaining hash table, we found that learned models can achieve 30% smaller sizes and 10% lower probe latency. For Cuckoo hash tables, in some datasets, learned models can achieve a small lookup time benefit. In summary, we found that learned models can indeed outperform hash functions but only for certain data distributions and with a limited margin.", "year": "", "venue": "", "authors": "Ibrahim Sabek and Kapil Vaidya and Dominik Horn and Andreas Kipf and Tim Kraska"}]