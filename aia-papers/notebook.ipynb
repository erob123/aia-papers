{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "import gradio as gr\n",
    "from scholarly import scholarly\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "\n",
    "# === 1. Load SPECTER2 with proximity adapter ===\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61596ff903748eda37bdba871e7e2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "import gradio as gr\n",
    "from scholarly import scholarly\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "\n",
    "# === 1. Load SPECTER2 with proximity adapter ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter2_base\")\n",
    "model = AutoAdapterModel.from_pretrained(\"allenai/specter2_base\")\n",
    "model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"proximity\", set_active=True)\n",
    "model.eval()\n",
    "\n",
    "# === 2. Get papers from Google Scholar ===\n",
    "def get_org_papers(gs_profile_id, max_papers=20):\n",
    "    author = scholarly.search_author_id(gs_profile_id)\n",
    "    author_filled = scholarly.fill(author, sections=[\"publications\"])\n",
    "    paper_list = []\n",
    "    for pub in author_filled[\"publications\"][:max_papers]:\n",
    "        try:\n",
    "            pub_filled = scholarly.fill(pub)\n",
    "            paper_list.append({\n",
    "                \"title\": pub_filled.get(\"bib\", {}).get(\"title\", \"\"),\n",
    "                \"abstract\": pub_filled.get(\"bib\", {}).get(\"abstract\", \"\"),\n",
    "                \"year\": pub_filled.get(\"bib\", {}).get(\"pub_year\", \"\"),\n",
    "                \"venue\": pub_filled.get(\"bib\", {}).get(\"venue\", \"\"),\n",
    "                \"authors\": pub_filled.get(\"bib\", {}).get(\"author\", \"\"),\n",
    "            })\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading pub: {e}\")\n",
    "            continue\n",
    "    return paper_list\n",
    "\n",
    "# === 3. Embed papers with SPECTER2 ===\n",
    "def embed_papers(papers):\n",
    "    texts = [p[\"title\"] + tokenizer.sep_token + p.get(\"abstract\", \"\") for p in papers]\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True,\n",
    "                       return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "\n",
    "# === 4. Build FAISS index ===\n",
    "def build_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# === 5. Save and Load Index ===\n",
    "def save_index_and_data(embeddings, index, papers, dir_path=\"index_data\"):\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    faiss.write_index(index, os.path.join(dir_path, \"index.faiss\"))\n",
    "    np.save(os.path.join(dir_path, \"embeddings.npy\"), embeddings)\n",
    "    with open(os.path.join(dir_path, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(papers, f)\n",
    "\n",
    "def load_index_and_data(dir_path=\"index_data\"):\n",
    "    index = faiss.read_index(os.path.join(dir_path, \"index.faiss\"))\n",
    "    embeddings = np.load(os.path.join(dir_path, \"embeddings.npy\"))\n",
    "    with open(os.path.join(dir_path, \"metadata.json\"), \"r\") as f:\n",
    "        papers = json.load(f)\n",
    "    return papers, embeddings, index\n",
    "\n",
    "# === 6. Compute pairwise similarity table ===\n",
    "def compute_pairwise_similarity_table(papers, embeddings, top_k=100):\n",
    "    sims = cosine_similarity(embeddings)\n",
    "    results = []\n",
    "    for i in range(len(papers)):\n",
    "        for j in range(i + 1, len(papers)):\n",
    "            results.append({\n",
    "                \"paper_A_title\": papers[i][\"title\"],\n",
    "                \"paper_B_title\": papers[j][\"title\"],\n",
    "                \"paper_A_authors\": papers[i].get(\"authors\", \"\"),\n",
    "                \"paper_B_authors\": papers[j].get(\"authors\", \"\"),\n",
    "                \"similarity\": round(sims[i, j], 4)\n",
    "            })\n",
    "    results = sorted(results, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    df = pd.DataFrame(results[:top_k])\n",
    "    df.to_csv(\"pairwise_similarity_top100.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "# === 7. Visualizations ===\n",
    "def build_similarity_graph(pairwise_df, threshold=0.85):\n",
    "    G = nx.Graph()\n",
    "    for _, row in pairwise_df.iterrows():\n",
    "        if row[\"similarity\"] >= threshold:\n",
    "            G.add_edge(row[\"paper_A_title\"], row[\"paper_B_title\"], weight=row[\"similarity\"])\n",
    "    return G\n",
    "\n",
    "def draw_similarity_graph(G, figsize=(12, 8)):\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=figsize)\n",
    "    nx.draw(G, pos, with_labels=True, node_size=700, node_color=\"lightblue\", edge_color=\"gray\")\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels={k: f\"{v:.2f}\" for k, v in labels.items()})\n",
    "    plt.title(\"Semantic Similarity Graph\")\n",
    "    plt.show()\n",
    "\n",
    "def build_interactive_graph(pairwise_df, threshold=0.85, output_file=\"similarity_graph.html\"):\n",
    "    net = Network(height=\"750px\", width=\"100%\", notebook=False)\n",
    "    for _, row in pairwise_df.iterrows():\n",
    "        if row[\"similarity\"] >= threshold:\n",
    "            a, b = row[\"paper_A_title\"], row[\"paper_B_title\"]\n",
    "            net.add_node(a, title=row[\"paper_A_authors\"], label=a)\n",
    "            net.add_node(b, title=row[\"paper_B_authors\"], label=b)\n",
    "            net.add_edge(a, b, value=row[\"similarity\"])\n",
    "    net.show_buttons(filter_=[\"physics\"])\n",
    "    net.show(output_file)\n",
    "\n",
    "# === 8. Gradio UI ===\n",
    "def launch_gradio(papers, index, embeddings):\n",
    "    def recommend(title, abstract, top_k):\n",
    "        text = title + tokenizer.sep_token + abstract\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True,\n",
    "                           return_token_type_ids=False, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "        query_embedding = output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        scores, indices = index.search(query_embedding, top_k)\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            paper = papers[idx]\n",
    "            results.append(f\"ðŸ”¹ **{paper['title']}** ({paper['venue']}, {paper['year']})\\nScore: {score:.3f}\")\n",
    "        return \"\\n\\n\".join(results)\n",
    "\n",
    "    gr.Interface(\n",
    "        fn=recommend,\n",
    "        inputs=[\n",
    "            gr.Textbox(label=\"Title\"),\n",
    "            gr.Textbox(label=\"Abstract\"),\n",
    "            gr.Slider(minimum=1, maximum=10, step=1, label=\"Top K Results\", value=5)\n",
    "        ],\n",
    "        outputs=gr.Markdown(),\n",
    "        title=\"ðŸ“š Citation Recommender\",\n",
    "        description=\"Find semantically similar papers from your Google Scholar corpus.\"\n",
    "    ).launch()\n",
    "\n",
    "# === 9. Run it all ===\n",
    "# google_scholar_id = \"Zid_jw4AAAAJ\"\n",
    "# try:\n",
    "#     papers, embeddings, index = load_index_and_data()\n",
    "#     print(\"âœ… Loaded cached index and papers.\")\n",
    "# except:\n",
    "#     print(\"ðŸš€ Building index from scratch...\")\n",
    "#     papers = get_org_papers(google_scholar_id, max_papers=30)\n",
    "#     embeddings = embed_papers(papers)\n",
    "#     index = build_index(embeddings)\n",
    "#     save_index_and_data(embeddings, index, papers)\n",
    "\n",
    "# pairwise_df = compute_pairwise_similarity_table(papers, embeddings, top_k=100)\n",
    "# G = build_similarity_graph(pairwise_df, threshold=0.88)\n",
    "# draw_similarity_graph(G)\n",
    "# build_interactive_graph(pairwise_df, threshold=0.88)\n",
    "\n",
    "# launch_gradio(papers, index, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aia-papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
